<!doctype html>



  


<html class="theme-next pisces use-motion" lang="zh-Hans">
<head>
  <!-- hexo-inject:begin --><!-- hexo-inject:end --><meta charset="UTF-8"/>
<meta http-equiv="X-UA-Compatible" content="IE=edge" />
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1"/>



<meta http-equiv="Cache-Control" content="no-transform" />
<meta http-equiv="Cache-Control" content="no-siteapp" />



  <meta name="google-site-verification" content="U-X9XD2c_wV6AKCoj83wtV4l3xqUDXk8YHuUj3GiYxg" />













  
  
  <link href="/lib/fancybox/source/jquery.fancybox.css?v=2.1.5" rel="stylesheet" type="text/css" />




  
  
  
  

  
    
    
  

  

  

  

  

  
    
    
    <link href="//fonts.googleapis.com/css?family=Lato:300,300italic,400,400italic,700,700italic&subset=latin,latin-ext" rel="stylesheet" type="text/css">
  






<link href="/lib/font-awesome/css/font-awesome.min.css?v=4.6.2" rel="stylesheet" type="text/css" />

<link href="/css/main.css?v=5.1.0" rel="stylesheet" type="text/css" />


  <meta name="keywords" content="tensorflow,机器翻译,seq2seq,rnn," />








  <link rel="shortcut icon" type="image/x-icon" href="/favicon.ico?v=5.1.0" />






<meta name="description" content="基础神经网络机器翻译在过去，传统的短语翻译系统通过将源句分解成多个块，然后用短语翻译短语来完成它们的任务。这导致不流利的翻译。我们人类这样翻译句子：阅读整个源句子，理解其意义，再进行翻译。神经网络机器翻译模仿这个过程。

Encoder把源句子转换为”含义”向量，再通过decoder产生翻译。
具体来说，NMT系统首先读取源语句，使用编码器来建立一个向量，一个数字序列表示句子的意思。然后，一个解码">
<meta property="og:type" content="article">
<meta property="og:title" content="TensorFlow神经网络机器翻译学习笔记">
<meta property="og:url" content="http://libowei.net/TensorFlow神经网络机器翻译学习笔记.html">
<meta property="og:site_name" content="博伟的博客">
<meta property="og:description" content="基础神经网络机器翻译在过去，传统的短语翻译系统通过将源句分解成多个块，然后用短语翻译短语来完成它们的任务。这导致不流利的翻译。我们人类这样翻译句子：阅读整个源句子，理解其意义，再进行翻译。神经网络机器翻译模仿这个过程。

Encoder把源句子转换为”含义”向量，再通过decoder产生翻译。
具体来说，NMT系统首先读取源语句，使用编码器来建立一个向量，一个数字序列表示句子的意思。然后，一个解码">
<meta property="og:image" content="http://libowei.net/image/1512974276786.png">
<meta property="og:image" content="http://libowei.net/image/1512974935963.png">
<meta property="og:image" content="http://libowei.net/image/1512979222432.png">
<meta property="og:image" content="http://libowei.net/image/1513058152940.png">
<meta property="og:image" content="http://libowei.net/image/1513058623382.png">
<meta property="og:image" content="http://libowei.net/./1513058830172.png">
<meta property="og:updated_time" content="2017-12-13T03:31:09.583Z">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="TensorFlow神经网络机器翻译学习笔记">
<meta name="twitter:description" content="基础神经网络机器翻译在过去，传统的短语翻译系统通过将源句分解成多个块，然后用短语翻译短语来完成它们的任务。这导致不流利的翻译。我们人类这样翻译句子：阅读整个源句子，理解其意义，再进行翻译。神经网络机器翻译模仿这个过程。

Encoder把源句子转换为”含义”向量，再通过decoder产生翻译。
具体来说，NMT系统首先读取源语句，使用编码器来建立一个向量，一个数字序列表示句子的意思。然后，一个解码">
<meta name="twitter:image" content="http://libowei.net/image/1512974276786.png">



<script type="text/javascript" id="hexo.configurations">
  var NexT = window.NexT || {};
  var CONFIG = {
    root: '/',
    scheme: 'Pisces',
    sidebar: {"position":"left","display":"post","offset":12,"offset_float":0,"b2t":true,"scrollpercent":false},
    fancybox: true,
    motion: true,
    duoshuo: {
      userId: '0',
      author: '博主'
    },
    algolia: {
      applicationID: '',
      apiKey: '',
      indexName: '',
      hits: {"per_page":10},
      labels: {"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}
    }
  };
</script>



  <link rel="canonical" href="http://libowei.net/TensorFlow神经网络机器翻译学习笔记.html"/>





  <title> TensorFlow神经网络机器翻译学习笔记 | 博伟的博客 </title><!-- hexo-inject:begin --><!-- hexo-inject:end -->
</head>

<body itemscope itemtype="http://schema.org/WebPage" lang="zh-Hans">

  




<!-- hexo-inject:begin --><!-- hexo-inject:end --><script>
  (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
            (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
          m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
  })(window,document,'script','https://www.google-analytics.com/analytics.js','ga');
  ga('create', 'UA-96532270-1', 'auto');
  ga('send', 'pageview');
</script>


  <script type="text/javascript">
    var _hmt = _hmt || [];
    (function() {
      var hm = document.createElement("script");
      hm.src = "https://hm.baidu.com/hm.js?ff1e2d8efa5365ff70fa3071bee477bb";
      var s = document.getElementsByTagName("script")[0];
      s.parentNode.insertBefore(hm, s);
    })();
  </script>



  <script type="text/javascript">
    (function() {
      var hm = document.createElement("script");
      hm.src = "//tajs.qq.com/stats?sId=61491911";
      var s = document.getElementsByTagName("script")[0];
      s.parentNode.insertBefore(hm, s);
    })();
  </script>








  
  
    
  

  <div class="container sidebar-position-left page-post-detail ">
    <div class="headband"></div>

    <header id="header" class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-wrapper">
  <div class="site-meta ">
    

    <div class="custom-logo-site-title">
      <a href="/"  class="brand" rel="start">
        <span class="logo-line-before"><i></i></span>
        <span class="site-title">博伟的博客</span>
        <span class="logo-line-after"><i></i></span>
      </a>
    </div>
      
        <p class="site-subtitle">Albert's blog</p>
      
  </div>

  <div class="site-nav-toggle">
    <button>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
    </button>
  </div>
</div>

<nav class="site-nav">
  

  
    <ul id="menu" class="menu">
      
        
        <li class="menu-item menu-item-home">
          <a href="/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-home"></i> <br />
            
            首页
          </a>
        </li>
      
        
        <li class="menu-item menu-item-archives">
          <a href="/archives" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-archive"></i> <br />
            
            归档
          </a>
        </li>
      
        
        <li class="menu-item menu-item-tags">
          <a href="/tags" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-tags"></i> <br />
            
            标签
          </a>
        </li>
      

      
        <li class="menu-item menu-item-search">
          
            <a href="javascript:;" class="popup-trigger">
          
            
              <i class="menu-item-icon fa fa-search fa-fw"></i> <br />
            
            搜索
          </a>
        </li>
      
    </ul>
  

  
    <div class="site-search">
      
  <div class="popup search-popup local-search-popup">
  <div class="local-search-header clearfix">
    <span class="search-icon">
      <i class="fa fa-search"></i>
    </span>
    <span class="popup-btn-close">
      <i class="fa fa-times-circle"></i>
    </span>
    <div class="local-search-input-wrapper">
      <input autocapitalize="off" autocomplete="off" autocorrect="off"
             placeholder="搜索..." spellcheck="false"
             type="text" id="local-search-input">
    </div>
  </div>
  <div id="local-search-result"></div>
</div>



    </div>
  
</nav>



 </div>
    </header>

    <main id="main" class="main">
      <div class="main-inner">
        <div class="content-wrap">
          <div id="content" class="content">
            

  <div id="posts" class="posts-expand">
    

  

  
  
  

  <article class="post post-type-normal " itemscope itemtype="http://schema.org/Article">
    <link itemprop="mainEntityOfPage" href="http://libowei.net/TensorFlow神经网络机器翻译学习笔记.html">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="Albert Lee">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/image/学士服照片.jpg">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="博伟的博客">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
            
            
              
                TensorFlow神经网络机器翻译学习笔记
              
            
          </h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="创建于" itemprop="dateCreated datePublished" datetime="2017-12-13T11:02:49+08:00">
                2017-12-13
              </time>
            

            

            
          </span>

          

          
            
          

          
          
             <span id="/TensorFlow神经网络机器翻译学习笔记.html" class="leancloud_visitors" data-flag-title="TensorFlow神经网络机器翻译学习笔记">
               <span class="post-meta-divider">|</span>
               <span class="post-meta-item-icon">
                 <i class="fa fa-eye"></i>
               </span>
               
                 <span class="post-meta-item-text">阅读次数 </span>
               
                 <span class="leancloud-visitors-count"></span>
             </span>
          

          

          

          

        </div>
      </header>
    


    <div class="post-body" itemprop="articleBody">

      
      

      
        <h2 id="基础"><a href="#基础" class="headerlink" title="基础"></a>基础</h2><h3 id="神经网络机器翻译"><a href="#神经网络机器翻译" class="headerlink" title="神经网络机器翻译"></a>神经网络机器翻译</h3><p>在过去，传统的短语翻译系统通过将源句分解成多个块，然后用短语翻译短语来完成它们的任务。这导致不流利的翻译。我们人类这样翻译句子：阅读整个源句子，理解其意义，再进行翻译。神经网络机器翻译模仿这个过程。</p>
<p><img src="/image/1512974276786.png" alt="Alt text"></p>
<p>Encoder把源句子转换为”含义”向量，再通过decoder产生翻译。</p>
<p>具体来说，NMT系统首先读取源语句，使用编码器来建立一个向量，一个数字序列表示句子的意思。然后，一个解码器处理句子向量以生成一个翻译。这通常被称为encoder-decoder结构。这解决了传统短语翻译中的local translation的问题：它可以捕获语言中的长距离依赖关系，如性别、句法结构，并产生更流利的翻译。</p>
<p>NMT具体结构可以变化。绝大多数NMT模型都使用RNN，以应用序列化的数据。在编码器和解码器上都使用RNN 。RNN模型在下面的部分有区别：</p>
<ol>
<li>方向性：单向或双向</li>
<li>深度：单层或多层</li>
<li>类型：普通的RNN，LSTM，或GRU</li>
</ol>
<p>在教程中使用多层、单向的LSTM。把”I am a student”翻译成”Je suis étudiant”的模型如下图：<br><img src="/image/1512974935963.png" alt="Alt text"></p>
<p>NMT模型由两个循环神经网组成：</p>
<ol>
<li>编码器RNN处理输入，不做预测</li>
<li>解码器RNN处理目标句子，同时预测下一个单词</li>
</ol>
<h2 id="建立NMT系统"><a href="#建立NMT系统" class="headerlink" title="建立NMT系统"></a>建立NMT系统</h2><p>在最下面的一层，编码器和解码器RNN接收输入：先是源句，再是边界标记&lt;s&gt;，表示从编码到解码模式的转换，然后是目标句。训练时，输入下面的Tensors，以time-major的格式的单词索引。</p>
<ul>
<li><strong>encoder_inputs</strong> [max_encoder_time , batch_size]  源单词</li>
<li><strong>decoder_inputs</strong> [max_decoder_time, batch_size] 目标输入单词</li>
<li><strong>decoder_outputs</strong> [max_decoder_time, batch_size]  目标输出单词，由decoder_inputs左移一个时间步，和结束标记符组成。</li>
</ul>
<p>为了提高效率，batch_size个句子一起训练。测试稍有不同，稍后讨论。</p>
<h3 id="词向量"><a href="#词向量" class="headerlink" title="词向量"></a>词向量</h3><p>模型需要查找源和目标embeddings来检索得到相应的词的表示。对于这个嵌入层，首先为每个语言选择一个词汇表。通常选定词表大小V，只有最常见的V个词被收录，其他词表示为”unknown”标记，以相同的向量表示。每种语言的词向量通常在训练时学习。</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div></pre></td><td class="code"><pre><div class="line"># embedding变量</div><div class="line">embedding_encoder = variable_scope.get_variable(&quot;embedding_encoder&quot;, [src_vocab_size, embedding_size], ...)</div><div class="line"># embedding查找:</div><div class="line">#   encoder_inputs: [max_time, batch_size]</div><div class="line">#   encoder_emb_inp: [max_time, batch_size, embedding_size]</div><div class="line">encoder_emb_inp = tf.nn.embedding_lookup(embedding_encoder, encoder_inputs)</div></pre></td></tr></table></figure>
<p><code>embedding_decoder</code>和<code>decoder_emb_inp</code>也是相似的。我们可以通过预先训练好的word2vec或Glove来初始化embedding。在一般情况下，给予大量的训练数据，我们可以从零开始学习。</p>
<h3 id="编码器"><a href="#编码器" class="headerlink" title="编码器"></a>编码器</h3><p>检索到词向量后，作为输入传入网络，网络由两个多层RNN组成：源语言编码器和目标语言解码器。这两个RNN原则上可以共享权重，但在实际中，我们通常使用两套不同的RNN参数（拟合大数据集时较好）。编码器RNN使用零向量作为初始状态。</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div></pre></td><td class="code"><pre><div class="line"> #构建RNN单元</div><div class="line">encoder_cell = tf.nn.rnn_cell.BasicLSTMCell(num_units)</div><div class="line"></div><div class="line"> # 动态运行RNN</div><div class="line"> #   encoder_outpus: [max_time, batch_size, num_units]</div><div class="line"> #   encoder_state: [batch_size, num_units]</div><div class="line">encoder_outputs, encoder_state = tf.nn.dynamic_rnn(</div><div class="line">	encoder_cell, encoder_emb_inp, sequence_length=source_sequence_length, time_major=True)</div></pre></td></tr></table></figure>
<p>句子长度不同，为了避免计算资源浪费，我们在<code>dynamic_rnn</code>传入源句子的真实长度<code>source_sequence_length</code>。由于输入格式是time-major，设置<code>time_major=True</code>。这样构建了单层的LSTM。多层LSTMs后面再说。</p>
<h3 id="解码器"><a href="#解码器" class="headerlink" title="解码器"></a>解码器</h3><p>解码器也需要获取源句子信息，简单的方法是用编码器的最终的state:<code>encoder_state</code>来初始化解码器state。</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div></pre></td><td class="code"><pre><div class="line"># Build RNN cell</div><div class="line">decoder_cell = tf.nn.rnn_cell.BasicLSTMCell(num_units)</div><div class="line"># Helper</div><div class="line">helper = tf.contrib.seq2seq.TrainingHelper(decoder_emb_inp, decoder_lengths, time_major=True)</div><div class="line"># Decoder</div><div class="line">decoder = tf.contrib.seq2seq.BasicDecoder(</div><div class="line">    decoder_cell, helper, encoder_state,output_layer=projection_layer)</div><div class="line"># Dynamic decoding</div><div class="line">outputs, _ = tf.contrib.seq2seq.dynamic_decode(decoder, ...)</div><div class="line">logits = outputs.rnn_output</div></pre></td></tr></table></figure>
<p><code>decoder</code>是<code>BasicDecoder</code>对象，接收<code>decoder_cell</code>和<code>helper</code>和之前的<code>encoder_state</code>作为输入。可以使用多种helpers，可以替换成<code>GreedyEmbeddingHelper</code>作贪婪解码。</p>
<p><code>projection_layer</code>是密集矩阵，把hidden state转为V维（词表大小）的结果向量：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">projection_layer = layers_core.Dense(tgt_vocab_size, use_bias=False)</div></pre></td></tr></table></figure>
<h3 id="损失"><a href="#损失" class="headerlink" title="损失"></a>损失</h3><p>给定上面的logits向量，可以计算训练损失：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">crossent = tf.nn.sparse_softmax_cross_entropy_with_logits(labels=decoder_outputs, logits=logits)</div><div class="line">train_loss = (tf.reduce_sum(crossent * target_weights) / batch_size)</div></pre></td></tr></table></figure></p>
<p><code>target_weight</code>是0-1矩阵，和<code>decoder_output</code>大小相同。用0填补超出目标序列长度的位置。</p>
<p>把损失除以了batch_size，这样可使超参数的设置与batch_size无关。有些人用损失除以(batch_size * num_time_steps)，减小在短句上的错误。但我们的超参数不适用于这样，例如使用SGD和学习率1.0，后者使学习率变为1/num_time_steps。</p>
<h3 id="生成翻译"><a href="#生成翻译" class="headerlink" title="生成翻译"></a>生成翻译</h3><p>使用未见过的句子，得到翻译，这个过程称为推断。训练和推断有明显的区别：推断时只有源句。解码的方式有多种：greedy, sampling, beam-search等。下面讨论greedy解码。</p>
<p><img src="/image/1512979222432.png" alt="Alt text"></p>
<ol>
<li>按相同方式输入源句，得到<code>encoder_state</code>，这个state用于初始化解码器。</li>
<li>解码器一收到&lt;s&gt;就开始解码（翻译）过程</li>
<li>在解码器的每个时间步上，RNN的输出为一组数。选择最可能的单词（最大的值）作为输出单词，然后将这个单词作为下一时间步的输入。</li>
<li>这个过程持续到产生&lt;/s&gt;（结束符）</li>
</ol>
<p>第三步是与训练不同的，训练时总是使用正确的目标词作为输入，推断时则使用模型上一步的预测。下面是greedy decoding，和训练decoder很相似。<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div></pre></td><td class="code"><pre><div class="line"># Helper</div><div class="line">helper = tf.contrib.seq2seq.GreedyEmbeddingHelper(</div><div class="line">    embedding_decoder,</div><div class="line">    tf.fill([batch_size], tgt_sos_id), tgt_eos_id)</div><div class="line"></div><div class="line"># Decoder</div><div class="line">decoder = tf.contrib.seq2seq.BasicDecoder(</div><div class="line">    decoder_cell, helper, encoder_state,</div><div class="line">    output_layer=projection_layer)</div><div class="line"># Dynamic decoding</div><div class="line">outputs, _ = tf.contrib.seq2seq.dynamic_decode(</div><div class="line">    decoder, maximum_iterations=maximum_iterations)</div><div class="line">translations = outputs.sample_id</div></pre></td></tr></table></figure></p>
<p>这里使用了<code>GreedyEmbeddingHelper</code>。因为推断时不知道目标序列的长度，所以指定<code>maximum_iterations</code>来限制。一种启发式方法是解码最多两倍的源句子长度。<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">maximum_iterations = tf.round(tf.reduce_max(source_sequence_length) * 2)</div></pre></td></tr></table></figure></p>
<h2 id="技巧"><a href="#技巧" class="headerlink" title="技巧"></a>技巧</h2><h3 id="建立训练、评估、推断图"><a href="#建立训练、评估、推断图" class="headerlink" title="建立训练、评估、推断图"></a>建立训练、评估、推断图</h3><p>构建TensorFlow模型时，最好建立三个独立的图：</p>
<ol>
<li>训练图<ul>
<li>采样、批处理输入</li>
<li>前向、反向运算</li>
<li>optimizer，加入训练op</li>
</ul>
</li>
<li>评估图<ul>
<li>批处理输入</li>
<li>包含训练时的前向op，和额外的评估op</li>
</ul>
</li>
<li>推断图<ul>
<li>可能不需要batch</li>
<li>不需要对输入进行额外处理</li>
<li>从placeholder中读输入数据</li>
<li>包含模型前向op的一部分，和额外的用于存储输入输出状态的调用</li>
</ul>
</li>
</ol>
<p>分别建立这些图有好处：</p>
<ul>
<li>推断图通常与其他两个很不一样，所以单独构造它是有意义的。</li>
<li>评估图更简单，它不包含反向传播op</li>
<li>每个图的数据输入方式可以用不同的方法实现</li>
<li>变量重用更简单，不用到处写reuse参数</li>
<li>分布式训练中，不同的worker进行训练评估和推断。不管怎样，这些都需要建立自己的图。因此，建立这样的系统为你的分布式训练做准备。</li>
</ul>
<p>难点在于如何在一台机器上共享三个图中的变量。可以在每个图是用独立的session。训练session周期性的保存checkpoints，评估session和推断session从checkpoint读取参数。下面是两种方法的区别。</p>
<p><strong> 原先： 三个模型在一个图中，使用一个Session</strong><br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div></pre></td><td class="code"><pre><div class="line">with tf.variable_scope(&apos;root&apos;):</div><div class="line">  train_inputs = tf.placeholder()</div><div class="line">  train_op, loss = BuildTrainModel(train_inputs)</div><div class="line">  initializer = tf.global_variables_initializer()</div><div class="line"></div><div class="line">with tf.variable_scope(&apos;root&apos;, reuse=True):</div><div class="line">  eval_inputs = tf.placeholder()</div><div class="line">  eval_loss = BuildEvalModel(eval_inputs)</div><div class="line"></div><div class="line">with tf.variable_scope(&apos;root&apos;, reuse=True):</div><div class="line">  infer_inputs = tf.placeholder()</div><div class="line">  inference_output = BuildInferenceModel(infer_inputs)</div><div class="line"></div><div class="line">sess = tf.Session()</div><div class="line"></div><div class="line">sess.run(initializer)</div><div class="line"></div><div class="line">for i in itertools.count():</div><div class="line">  train_input_data = ...</div><div class="line">  sess.run([loss, train_op], feed_dict=&#123;train_inputs: train_input_data&#125;)</div><div class="line"></div><div class="line">  if i % EVAL_STEPS == 0:</div><div class="line">    while data_to_eval:</div><div class="line">      eval_input_data = ...</div><div class="line">      sess.run([eval_loss], feed_dict=&#123;eval_inputs: eval_input_data&#125;)</div><div class="line"></div><div class="line">  if i % INFER_STEPS == 0:</div><div class="line">    sess.run(inference_output, feed_dict=&#123;infer_inputs: infer_input_data&#125;)</div></pre></td></tr></table></figure></p>
<p><strong> 现在：三个模型在三个图中，使用三个Session，共享相同的变量</strong><br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div><div class="line">42</div><div class="line">43</div></pre></td><td class="code"><pre><div class="line">train_graph = tf.Graph()</div><div class="line">eval_graph = tf.Graph()</div><div class="line">infer_graph = tf.Graph()</div><div class="line"></div><div class="line">with train_graph.as_default():</div><div class="line">  train_iterator = ...</div><div class="line">  train_model = BuildTrainModel(train_iterator)</div><div class="line">  initializer = tf.global_variables_initializer()</div><div class="line"></div><div class="line">with eval_graph.as_default():</div><div class="line">  eval_iterator = ...</div><div class="line">  eval_model = BuildEvalModel(eval_iterator)</div><div class="line"></div><div class="line">with infer_graph.as_default():</div><div class="line">  infer_iterator, infer_inputs = ...</div><div class="line">  infer_model = BuildInferenceModel(infer_iterator)</div><div class="line"></div><div class="line">checkpoints_path = &quot;/tmp/model/checkpoints&quot;</div><div class="line"></div><div class="line">train_sess = tf.Session(graph=train_graph)</div><div class="line">eval_sess = tf.Session(graph=eval_graph)</div><div class="line">infer_sess = tf.Session(graph=infer_graph)</div><div class="line"></div><div class="line">train_sess.run(initializer)</div><div class="line">train_sess.run(train_iterator.initializer)</div><div class="line"></div><div class="line">for i in itertools.count():</div><div class="line"></div><div class="line">  train_model.train(train_sess)</div><div class="line"></div><div class="line">  if i % EVAL_STEPS == 0:</div><div class="line">    checkpoint_path = train_model.saver.save(train_sess, checkpoints_path, global_step=i)</div><div class="line">    eval_model.saver.restore(eval_sess, checkpoint_path)</div><div class="line">    eval_sess.run(eval_iterator.initializer)</div><div class="line">    while data_to_eval:</div><div class="line">      eval_model.eval(eval_sess)</div><div class="line"></div><div class="line">  if i % INFER_STEPS == 0:</div><div class="line">    checkpoint_path = train_model.saver.save(train_sess, checkpoints_path, global_step=i)</div><div class="line">    infer_model.saver.restore(infer_sess, checkpoint_path)</div><div class="line">    infer_sess.run(infer_iterator.initializer, feed_dict=&#123;infer_inputs: infer_input_data&#125;)</div><div class="line">    while data_to_infer:</div><div class="line">      infer_model.infer(infer_sess)</div></pre></td></tr></table></figure></p>
<h2 id="数据输入"><a href="#数据输入" class="headerlink" title="数据输入"></a>数据输入</h2><p>使用<code>map</code>变换，把句子转化为词：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">dataset = dataset.map(lambda string:tf.string_split([string]).values)</div></pre></td></tr></table></figure></p>
<p>把句子向量转换为包含其长度的元组：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">dataset = dataset.map(lambda words: (words, tf.size(words))</div></pre></td></tr></table></figure></p>
<p>给定词汇查找表，把元组的第一个元素由string转为int<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">dataset = dataset.map(lambda words, size: (table.lookup(words), size))</div></pre></td></tr></table></figure></p>
<p>把两个dataset联结起来：一个dataset为源语言，另一个为目标语言，使用zip生成源、目标语言元组的dataset<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">source_target_dataset = tf.contrib.data.Dataset.zip((source_dataset, target_dataset))</div></pre></td></tr></table></figure></p>
<p>不定长度的句子的批处理：从<code>source_target_dataset</code>返回<code>batch_size</code>个元素，分别把每个向量添加pad，补齐到其中最长元素的长度<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div></pre></td><td class="code"><pre><div class="line">batched_dataset = source_target_dataset.padded_batch(</div><div class="line">  batch_size,</div><div class="line">  padded_shapes=((tf.TensorShape([None]),  # 未知长度的source vectors</div><div class="line">                  tf.TensorShape([])),     # source的长度</div><div class="line">                 (tf.TensorShape([None]),  # 未知长度的target vectors</div><div class="line">                  tf.TensorShape([]))),    # target的长度</div><div class="line">  padding_values=((src_eos_id,  # 向右用src_eos_id补齐</div><div class="line">                   0),          # source的长度 -- 未使用</div><div class="line">                  (tgt_eos_id,  # 向右用tgt_eos_id补齐</div><div class="line">                   0)))         # target的长度 -- 未使用</div></pre></td></tr></table></figure></p>
<p>从dataset读数据需要iterator：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div></pre></td><td class="code"><pre><div class="line">batched_iterator = batched_dataset.make_initializable_iterator()</div><div class="line"></div><div class="line">((source, source_lengths), (target, target_lenghts)) = batched_iterator.get_next()</div><div class="line"></div><div class="line">session.run(batched_iterator.initializer, feed_dict=&#123;...&#125;)</div></pre></td></tr></table></figure></p>
<h2 id="其他细节"><a href="#其他细节" class="headerlink" title="其他细节"></a>其他细节</h2><h3 id="双向RNN"><a href="#双向RNN" class="headerlink" title="双向RNN"></a>双向RNN</h3><p>编码器使用双向RNN可以提高模型，下面是使用单一双向层构建编码器的简化版本：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div></pre></td><td class="code"><pre><div class="line"># 创建向前和向后的cells</div><div class="line">forward_cell = tf.nn.rnn_cell.BasicLSTMCell(num_units)</div><div class="line">backward_cell = tf.nn.rnn_cell.BasicLSTMCell(num_units)</div><div class="line"></div><div class="line">bi_outputs, encoder_state = tf.nn.bidirectional_dynamic_rnn(</div><div class="line">    forward_cell, backward_cell, encoder_emb_inp,</div><div class="line">    sequence_length=source_sequence_length, time_major=True)</div><div class="line">encoder_outputs = tf.concat(bi_outputs, -1)</div></pre></td></tr></table></figure></p>
<p>变量<code>encoder_outputs</code>和<code>encoder_state</code>的用法和前面的一样。</p>
<h3 id="Beam搜索"><a href="#Beam搜索" class="headerlink" title="Beam搜索"></a>Beam搜索</h3><p>beam搜索解码器能进一步提高模型表现。思想是通过在我们翻译时保留一小部分顶级候选结果来更好地探索所有可能的翻译。<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div></pre></td><td class="code"><pre><div class="line"># 复制编码器信息beam_width次</div><div class="line">decoder_initial_state = tf.contrib.seq2seq.tile_batch(</div><div class="line">    encoder_state, multiplier=hparams.beam_width)</div><div class="line"></div><div class="line"># 定义一个 beam-search decoder</div><div class="line">decoder = tf.contrib.seq2seq.BeamSearchDecoder(</div><div class="line">        cell=decoder_cell,</div><div class="line">        embedding=embedding_decoder,</div><div class="line">        start_tokens=start_tokens,</div><div class="line">        end_token=end_token,</div><div class="line">        initial_state=decoder_initial_state,</div><div class="line">        beam_width=beam_width,</div><div class="line">        output_layer=projection_layer,</div><div class="line">        length_penalty_weight=0.0)</div><div class="line"></div><div class="line"># Dynamic decoding</div><div class="line">outputs, _ = tf.contrib.seq2seq.dynamic_decode(decoder, ...)</div></pre></td></tr></table></figure></p>
<p>使用<code>dynamic_decode()</code>API解码，解码后，得到翻译：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div></pre></td><td class="code"><pre><div class="line">translations = outputs.predicted_ids</div><div class="line"># 确保translation的形状为[batch_size, beam_width, time]</div><div class="line">if self.time_major:</div><div class="line">   translations = tf.transpose(translations, perm=[1, 2, 0])</div></pre></td></tr></table></figure></p>
<h3 id="超参数"><a href="#超参数" class="headerlink" title="超参数"></a>超参数</h3><p><em>Attention</em>：Bahdanau需要双向的编码器才表现较好；Luong在不同的配置下都很好；在教程代码中，推荐使用两种变形形式，scaled_luong &amp; normed bahdanau。</p>
<h3 id="多GPU训练"><a href="#多GPU训练" class="headerlink" title="多GPU训练"></a>多GPU训练</h3><p>把不同的RNN层放在不同的GPU上能加速训练。下面是个例子：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div></pre></td><td class="code"><pre><div class="line">cells = []</div><div class="line">for i in range(num_layers):</div><div class="line">  cells.append(tf.contrib.rnn.DeviceWrapper(</div><div class="line">      tf.contrib.rnn.LSTMCell(num_units),</div><div class="line">      &quot;/gpu:%d&quot; % (num_layers % num_gpus)))</div><div class="line">cell = tf.contrib.rnn.MultiRNNCell(cells)</div></pre></td></tr></table></figure></p>
<p>需要在<code>tf.gradients</code>设置<code>colocate_gradients_with_ops=True</code>，并行计算梯度。</p>
<p>会注意到GPU数量增加，速度提升很小。标准的attention结构的一个主要缺点是在每个时间步用最后一层的输出来查询attention，这意味着每个解码step需要等待之前的step完全结束，因此解码过程不能简单的把RNN层放在不同的GPU上来做。</p>
<p> <a href="https://arxiv.org/pdf/1609.08144.pdf" target="_blank" rel="external">NMT attention architecture</a> 并行解码计算通过使用第一层的输出来查询attention。因此，每个解码步骤都可以在前一步的第一层和attention计算完成后开始。</p>
<p><code>GNMTAttentionMultiCell</code>实现了这一结构，是<code>tf.contrib.rnn.MultiRNNCell</code>的子类，例子如下：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div></pre></td><td class="code"><pre><div class="line">cells = []</div><div class="line">for i in range(num_layers):</div><div class="line">  cells.append(tf.contrib.rnn.DeviceWrapper(</div><div class="line">      tf.contrib.rnn.LSTMCell(num_units),</div><div class="line">      &quot;/gpu:%d&quot; % (num_layers % num_gpus)))</div><div class="line">attention_cell = cells.pop(0)</div><div class="line">attention_cell = tf.contrib.seq2seq.AttentionWrapper(</div><div class="line">    attention_cell,</div><div class="line">    attention_mechanism,</div><div class="line">    attention_layer_size=None,  # don&apos;t add an additional dense layer.</div><div class="line">    output_attention=False,)</div><div class="line">cell = GNMTAttentionMultiCell(attention_cell, cells)</div></pre></td></tr></table></figure></p>
<h2 id="注意力机制"><a href="#注意力机制" class="headerlink" title="注意力机制"></a>注意力机制</h2><p>注意机制的核心思想是在翻译过程中，通过对相关源内容的“关注”，在目标和源之间建立直接的短连接。注意机制的一个很好的副产品是源和目标句子之间易于可视化的对齐矩阵。</p>
<p>普通seq2seq模型，编码器的最终隐藏状态传入解码器，这对于短句和中等长度句子非常适用，但对于长句子，一个固定大小的隐藏状态成为信息瓶颈。注意机制提供了一种方法，不丢弃编码器RNN中间的隐藏状态，而允许解码器回看它们（将它们视为源信息的动态存储器）。这样，注意机制提高了长句翻译的性能。现在注意力机制已成了事实上的标准，成功应用在各种任务中（图像标题生成、语音识别和文本摘要）。</p>
<p>现在介绍一个注意力机制的实例（Luong 2015），其已经在一些最先进的系统中使用，包括OpenNMT和TensorFlow API。</p>
<p><img src="/image/1513058152940.png" alt="Alt text"></p>
<p>上图是基于注意力的NMT系统的例子。着重了注意力计算的第一步的细节，没有展示嵌入层和投影层。</p>
<p>注意力计算发生在解码器的每个时间步。由下面几步组成：</p>
<ol>
<li>当前的目标隐藏状态与解码器的每个隐藏状态比较，得到注意力权重</li>
<li>基于注意力权重，计算上下文向量作为源状态的加权平均值。</li>
<li>把当前的目标隐藏状态与上下文向量结合，计算注意力向量。</li>
<li>注意力向量作为下一个时间步的输入。</li>
</ol>
<p>前三步可表示为：<br><img src="/image/1513058623382.png" alt="Alt text"></p>
<p><code>score</code>函数用来比较目标隐藏状态$h_t$和每个源隐藏状态$\overline{h}_s$，结果归一化生成注意力权重（基于源位置的概率分布）。打分函数有多种，常用的评分函数包括乘法和加法形式。<br><img src="./1513058830172.png" alt="Alt text"><br>注意力向量用于得到softmax结果和损失。这与普通的seq2seq模型最上层的隐藏状态相似。</p>
<p>注意力机制的变形取决于评分函数和注意力函数的变换，和之前的状态$h_{t-1}$是否用于评分函数。从经验上来说，我们发现只有某些选项起作用。第一点是注意力机制的基本形式：需要有源和目标之间的直接连接。第二点是在下一时间步传入注意力向量非常重要，用于告知网络以前的注意力决策。最后，评分函数的选择往往会导致不同的性能。</p>
<h3 id="Attention-API"><a href="#Attention-API" class="headerlink" title="Attention API"></a>Attention API</h3><p>在实现上借用了memory network的一些术语。与之不同的是，注意力机制的记忆是只读的。隐藏状态集（Luong的$W\overline{h}_s$，或Bahdaniu的$W_2\overline{h}_s$形式）即为记忆（memory）。在每个时间步，使用当前的隐藏状态作为查询（query），决定读取哪部分记忆。通常，查询需要与对应于每个记忆集的键进行比较。在注意力机制的上述呈现中，我们碰巧使用了一组源隐藏状态作为键（Bahdanau的$W_1h_t$）。可以从这种memory network中获得灵感，得到其他形式的注意力。</p>
<p>Luong 2015形式的注意力机制：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div></pre></td><td class="code"><pre><div class="line"># attention_states: [batch_size, max_time, num_units]</div><div class="line">attention_states = tf.transpose(encoder_outputs, [1, 0, 2])</div><div class="line"></div><div class="line"># Create an attention mechanism</div><div class="line">attention_mechanism = tf.contrib.seq2seq.LuongAttention(</div><div class="line">    num_units, attention_states,</div><div class="line">    memory_sequence_length=source_sequence_length)</div></pre></td></tr></table></figure></p>
<p>在编码器的章节，<code>encoder_outputs</code>是一组最上层源隐藏状态的集合，其形状为[max_time, batch_size, num_units] （time_major=True）。在注意力机制中，要确保memory的传递是batch_major，所以需要转置<code>attention_state</code>。传入<code>cource_sequence_length</code>确保注意力权重在非填补对齐的位置上正确的归一化。</p>
<p>使用<code>AttentionWrapper</code>封装解码器cell<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line">decoder_cell = tf.contrib.seq2seq.AttentionWrapper(</div><div class="line">    decoder_cell, attention_mechanism,</div><div class="line">    attention_layer_size=num_units)</div></pre></td></tr></table></figure></p>

      
    </div>

    <div>
      
        

      
    </div>
	
	
	<div>
      
        
 <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- ad -->
<ins class="adsbygoogle"
     style="display:block"
     data-ad-client="ca-pub-2599972617864441"
     data-ad-slot="6064195846"
     data-ad-format="auto"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>


      
    </div>
	

    <div>
      
        
  <div style="padding: 10px 0; margin: 20px auto; width: 90%; text-align: center;">
    <div>感谢支持！</div>
    <button id="rewardButton" disable="enable" onclick="var qr = document.getElementById('QR'); if (qr.style.display === 'none') {qr.style.display='block';} else {qr.style.display='none'}">
      <span>赏</span>
    </button>
    <div id="QR" style="display: none;">
      
        <div id="wechat" style="display: inline-block">
          <img id="wechat_qr" src="/image/wechat_reward.png" alt="Albert Lee WeChat Pay"/>
          <p>微信打赏</p>
        </div>
      
      
        <div id="alipay" style="display: inline-block">
          <img id="alipay_qr" src="/image/alipay_reward.jpg" alt="Albert Lee Alipay"/>
          <p>支付宝打赏</p>
        </div>
      
    </div>
  </div>


      
    </div>

    <div>
      
        

      
    </div>

    <footer class="post-footer">
      
        <div class="post-tags">
          
            <a href="/tags/tensorflow/" rel="tag"># tensorflow</a>
          
            <a href="/tags/机器翻译/" rel="tag"># 机器翻译</a>
          
            <a href="/tags/seq2seq/" rel="tag"># seq2seq</a>
          
            <a href="/tags/rnn/" rel="tag"># rnn</a>
          
        </div>
      

      
        
      

      
        <div class="post-nav">
          <div class="post-nav-next post-nav-item">
            
              <a href="/CentOS-MySQL数据库迁移.html" rel="next" title="CentOS MySQL数据库迁移">
                <i class="fa fa-chevron-left"></i> CentOS MySQL数据库迁移
              </a>
            
          </div>

          <span class="post-nav-divider"></span>

          <div class="post-nav-prev post-nav-item">
            
              <a href="/使用wordcloud工具生成虎扑新闻词云图.html" rel="prev" title="使用wordcloud工具生成虎扑新闻词云图">
                使用wordcloud工具生成虎扑新闻词云图 <i class="fa fa-chevron-right"></i>
              </a>
            
          </div>
        </div>
      

      
      
    </footer>
  </article>



    <div class="post-spread">
      
        
  <div class="bdsharebuttonbox">
    <a href="#" class="bds_tsina" data-cmd="tsina" title="分享到新浪微博"></a>
    <a href="#" class="bds_douban" data-cmd="douban" title="分享到豆瓣网"></a>
    <a href="#" class="bds_sqq" data-cmd="sqq" title="分享到QQ好友"></a>
    <a href="#" class="bds_qzone" data-cmd="qzone" title="分享到QQ空间"></a>
    <a href="#" class="bds_weixin" data-cmd="weixin" title="分享到微信"></a>
    <a href="#" class="bds_tieba" data-cmd="tieba" title="分享到百度贴吧"></a>
    <a href="#" class="bds_twi" data-cmd="twi" title="分享到Twitter"></a>
    <a href="#" class="bds_fbook" data-cmd="fbook" title="分享到Facebook"></a>
    <a href="#" class="bds_more" data-cmd="more"></a>
    <a class="bds_count" data-cmd="count"></a>
  </div>
  <script>
    window._bd_share_config = {
      "common": {
        "bdText": "",
        "bdMini": "2",
        "bdMiniList": false,
        "bdPic": ""
      },
      "share": {
        "bdSize": "16",
        "bdStyle": "0"
      },
      "image": {
        "viewList": ["tsina", "douban", "sqq", "qzone", "weixin", "twi", "fbook"],
        "viewText": "分享到：",
        "viewSize": "16"
      }
    }
  </script>

<script>
  with(document)0[(getElementsByTagName('head')[0]||body).appendChild(createElement('script')).src='//bdimg.share.baidu.com/static/api/js/share.js?cdnversion='+~(-new Date()/36e5)];
</script>

      
    </div>
  </div>


          </div>
          


          
  <div class="comments" id="comments">
    
  </div>


        </div>
        
          
  
  <div class="sidebar-toggle">
    <div class="sidebar-toggle-line-wrap">
      <span class="sidebar-toggle-line sidebar-toggle-line-first"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-middle"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-last"></span>
    </div>
  </div>

  <aside id="sidebar" class="sidebar">
    <div class="sidebar-inner">

      

      
        <ul class="sidebar-nav motion-element">
          <li class="sidebar-nav-toc sidebar-nav-active" data-target="post-toc-wrap" >
            文章目录
          </li>
          <li class="sidebar-nav-overview" data-target="site-overview">
            站点概览
          </li>
        </ul>
      

      <section class="site-overview sidebar-panel">
        <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
          <img class="site-author-image" itemprop="image"
               src="/image/学士服照片.jpg"
               alt="Albert Lee" />
          <p class="site-author-name" itemprop="name">Albert Lee</p>
           
              <p class="site-description motion-element" itemprop="description">一个没人知道的地方</p>
          
        </div>
        <nav class="site-state motion-element">

          
            <div class="site-state-item site-state-posts">
              <a href="/archives">
                <span class="site-state-item-count">26</span>
                <span class="site-state-item-name">日志</span>
              </a>
            </div>
          

          

          
            
            
            <div class="site-state-item site-state-tags">
              <a href="/tags/index.html">
                <span class="site-state-item-count">30</span>
                <span class="site-state-item-name">标签</span>
              </a>
            </div>
          

        </nav>

        

        <div class="links-of-author motion-element">
          
            
              <span class="links-of-author-item">
                <a href="https://github.com/libowei1213" target="_blank" title="GitHub">
                  
                    <i class="fa fa-fw fa-github"></i>
                  
                  GitHub
                </a>
              </span>
            
              <span class="links-of-author-item">
                <a href="http://weibo.com/libowei1213" target="_blank" title="微博">
                  
                    <i class="fa fa-fw fa-weibo"></i>
                  
                  微博
                </a>
              </span>
            
              <span class="links-of-author-item">
                <a href="https://www.zhihu.com/people/li-bo-wei-72" target="_blank" title="知乎">
                  
                    <i class="fa fa-fw fa-globe"></i>
                  
                  知乎
                </a>
              </span>
            
          
        </div>

        
        

        
        
          <div class="links-of-blogroll motion-element links-of-blogroll-block">
            <div class="links-of-blogroll-title">
              <i class="fa  fa-fw fa-globe"></i>
              友情链接
            </div>
            <ul class="links-of-blogroll-list">
              
                <li class="links-of-blogroll-item">
                  <a href="http://huoche123.top/train/index" title="查火车" target="_blank">查火车</a>
                </li>
              
                <li class="links-of-blogroll-item">
                  <a href="http://huoche123.top/today" title="历史上的今天" target="_blank">历史上的今天</a>
                </li>
              
                <li class="links-of-blogroll-item">
                  <a href="http://duzhihu.cc" title="读知乎" target="_blank">读知乎</a>
                </li>
              
            </ul>
          </div>
        

        

      </section>

      
      <!--noindex-->
        <section class="post-toc-wrap motion-element sidebar-panel sidebar-panel-active">
          <div class="post-toc">

            
              
            

            
              <div class="post-toc-content"><ol class="nav"><li class="nav-item nav-level-2"><a class="nav-link" href="#基础"><span class="nav-number">1.</span> <span class="nav-text">基础</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#神经网络机器翻译"><span class="nav-number">1.1.</span> <span class="nav-text">神经网络机器翻译</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#建立NMT系统"><span class="nav-number">2.</span> <span class="nav-text">建立NMT系统</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#词向量"><span class="nav-number">2.1.</span> <span class="nav-text">词向量</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#编码器"><span class="nav-number">2.2.</span> <span class="nav-text">编码器</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#解码器"><span class="nav-number">2.3.</span> <span class="nav-text">解码器</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#损失"><span class="nav-number">2.4.</span> <span class="nav-text">损失</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#生成翻译"><span class="nav-number">2.5.</span> <span class="nav-text">生成翻译</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#技巧"><span class="nav-number">3.</span> <span class="nav-text">技巧</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#建立训练、评估、推断图"><span class="nav-number">3.1.</span> <span class="nav-text">建立训练、评估、推断图</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#数据输入"><span class="nav-number">4.</span> <span class="nav-text">数据输入</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#其他细节"><span class="nav-number">5.</span> <span class="nav-text">其他细节</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#双向RNN"><span class="nav-number">5.1.</span> <span class="nav-text">双向RNN</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Beam搜索"><span class="nav-number">5.2.</span> <span class="nav-text">Beam搜索</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#超参数"><span class="nav-number">5.3.</span> <span class="nav-text">超参数</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#多GPU训练"><span class="nav-number">5.4.</span> <span class="nav-text">多GPU训练</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#注意力机制"><span class="nav-number">6.</span> <span class="nav-text">注意力机制</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#Attention-API"><span class="nav-number">6.1.</span> <span class="nav-text">Attention API</span></a></li></ol></li></ol></div>
            

          </div>
        </section>
      <!--/noindex-->
      

      
        <div class="back-to-top">
          <i class="fa fa-arrow-up"></i>
          
        </div>
      

    </div>
  </aside>


        
      </div>
    </main>

    <footer id="footer" class="footer">
      <div class="footer-inner">
        <div class="copyright" >
  
  &copy;  2016 - 
  <span itemprop="copyrightYear">2018</span>
  <span class="with-love">
    <i class="fa fa-heart"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">Albert Lee</span>
   
</div>

<div class="theme-info">  
	<span>Hosted by <a href="https://pages.coding.me" style="font-weight: bold">Coding Pages</a>
	&amp; <a href="https://pages.github.com" style="font-weight: bold">GitHub Pages</a>
	</span>
  </div>

<script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<script>
  (adsbygoogle = window.adsbygoogle || []).push({
    google_ad_client: "ca-pub-2599972617864441",
    enable_page_level_ads: true
  });
</script>



        
<div class="busuanzi-count">
  <script async src="https://dn-lbstatics.qbox.me/busuanzi/2.3/busuanzi.pure.mini.js"></script>

  
    <span class="site-uv">
      <i class="fa fa-user"></i>
      <span class="busuanzi-value" id="busuanzi_value_site_uv"></span>
      
    </span>
  

  
    <span class="site-pv">
      <i class="fa fa-eye"></i>
      <span class="busuanzi-value" id="busuanzi_value_site_pv"></span>
      
    </span>
  
</div>


        
      </div>
    </footer>

    

  </div>

  

<script type="text/javascript">
  if (Object.prototype.toString.call(window.Promise) !== '[object Function]') {
    window.Promise = null;
  }
</script>









  


  





  
  <script type="text/javascript" src="/lib/jquery/index.js?v=2.1.3"></script>

  
  <script type="text/javascript" src="/lib/fastclick/lib/fastclick.min.js?v=1.0.6"></script>

  
  <script type="text/javascript" src="/lib/jquery_lazyload/jquery.lazyload.js?v=1.9.7"></script>

  
  <script type="text/javascript" src="/lib/velocity/velocity.min.js?v=1.2.1"></script>

  
  <script type="text/javascript" src="/lib/velocity/velocity.ui.min.js?v=1.2.1"></script>

  
  <script type="text/javascript" src="/lib/fancybox/source/jquery.fancybox.pack.js?v=2.1.5"></script>

  
  <script type="text/javascript" src="/lib/canvas-nest/canvas-nest.min.js"></script>


  


  <script type="text/javascript" src="/js/src/utils.js?v=5.1.0"></script>

  <script type="text/javascript" src="/js/src/motion.js?v=5.1.0"></script>



  
  


  <script type="text/javascript" src="/js/src/affix.js?v=5.1.0"></script>

  <script type="text/javascript" src="/js/src/schemes/pisces.js?v=5.1.0"></script>



  
  <script type="text/javascript" src="/js/src/scrollspy.js?v=5.1.0"></script>
<script type="text/javascript" src="/js/src/post-details.js?v=5.1.0"></script>



  


  <script type="text/javascript" src="/js/src/bootstrap.js?v=5.1.0"></script>



  


  




	





  





  





  






  

  <script type="text/javascript">
    // Popup Window;
    var isfetched = false;
    // Search DB path;
    var search_path = "search.xml";
    if (search_path.length == 0) {
      search_path = "search.xml";
    }
    var path = "/" + search_path;
    // monitor main search box;

    function proceedsearch() {
      $("body")
        .append('<div class="search-popup-overlay local-search-pop-overlay"></div>')
        .css('overflow', 'hidden');
      $('.popup').toggle();
    }
    // search function;
    var searchFunc = function(path, search_id, content_id) {
      'use strict';
      $.ajax({
        url: path,
        dataType: "xml",
        async: true,
        success: function( xmlResponse ) {
          // get the contents from search data
          isfetched = true;
          $('.popup').detach().appendTo('.header-inner');
          var datas = $( "entry", xmlResponse ).map(function() {
            return {
              title: $( "title", this ).text(),
              content: $("content",this).text(),
              url: $( "url" , this).text()
            };
          }).get();
          var $input = document.getElementById(search_id);
          var $resultContent = document.getElementById(content_id);
          $input.addEventListener('input', function(){
            var matchcounts = 0;
            var str='<ul class=\"search-result-list\">';
            var keywords = this.value.trim().toLowerCase().split(/[\s\-]+/);
            $resultContent.innerHTML = "";
            if (this.value.trim().length > 1) {
              // perform local searching
              datas.forEach(function(data) {
                var isMatch = false;
                var content_index = [];
                var data_title = data.title.trim().toLowerCase();
                var data_content = data.content.trim().replace(/<[^>]+>/g,"").toLowerCase();
                var data_url = decodeURIComponent(data.url);
                var index_title = -1;
                var index_content = -1;
                var first_occur = -1;
                // only match artiles with not empty titles and contents
                if(data_title != '') {
                  keywords.forEach(function(keyword, i) {
                    index_title = data_title.indexOf(keyword);
                    index_content = data_content.indexOf(keyword);
                    if( index_title >= 0 || index_content >= 0 ){
                      isMatch = true;
                      if (i == 0) {
                        first_occur = index_content;
                      }
                    }

                  });
                }
                // show search results
                if (isMatch) {
                  matchcounts += 1;
                  str += "<li><a href='"+ data_url +"' class='search-result-title'>"+ data_title +"</a>";
                  var content = data.content.trim().replace(/<[^>]+>/g,"");
                  if (first_occur >= 0) {
                    // cut out 100 characters
                    var start = first_occur - 20;
                    var end = first_occur + 80;
                    if(start < 0){
                      start = 0;
                    }
                    if(start == 0){
                      end = 50;
                    }
                    if(end > content.length){
                      end = content.length;
                    }
                    var match_content = content.substring(start, end);
                    // highlight all keywords
                    keywords.forEach(function(keyword){
                      var regS = new RegExp(keyword, "gi");
                      match_content = match_content.replace(regS, "<b class=\"search-keyword\">"+keyword+"</b>");
                    });

                    str += "<p class=\"search-result\">" + match_content +"...</p>"
                  }
                  str += "</li>";
                }
              })};
            str += "</ul>";
            if (matchcounts == 0) { str = '<div id="no-result"><i class="fa fa-frown-o fa-5x" /></div>' }
            if (keywords == "") { str = '<div id="no-result"><i class="fa fa-search fa-5x" /></div>' }
            $resultContent.innerHTML = str;
          });
          proceedsearch();
        }
      });}

    // handle and trigger popup window;
    $('.popup-trigger').click(function(e) {
      e.stopPropagation();
      if (isfetched == false) {
        searchFunc(path, 'local-search-input', 'local-search-result');
      } else {
        proceedsearch();
      };
    });

    $('.popup-btn-close').click(function(e){
      $('.popup').hide();
      $(".local-search-pop-overlay").remove();
      $('body').css('overflow', '');
    });
    $('.popup').click(function(e){
      e.stopPropagation();
    });
  </script>





  

  
  <script src="https://cdn1.lncld.net/static/js/av-core-mini-0.6.1.js"></script>
  <script>AV.initialize("1HjW8mQ2Mvqrnu7j9nRM17tm-gzGzoHsz", "lnkha8qqp0bLLPJEpucS0V1F");</script>
  <script>
    function showTime(Counter) {
      var query = new AV.Query(Counter);
      var entries = [];
      var $visitors = $(".leancloud_visitors");

      $visitors.each(function () {
        entries.push( $(this).attr("id").trim() );
      });

      query.containedIn('url', entries);
      query.find()
        .done(function (results) {
          var COUNT_CONTAINER_REF = '.leancloud-visitors-count';

          if (results.length === 0) {
            $visitors.find(COUNT_CONTAINER_REF).text(0);
            return;
          }

          for (var i = 0; i < results.length; i++) {
            var item = results[i];
            var url = item.get('url');
            var time = item.get('time');
            var element = document.getElementById(url);

            $(element).find(COUNT_CONTAINER_REF).text(time);
          }
          for(var i = 0; i < entries.length; i++) {
            var url = entries[i];
            var element = document.getElementById(url);
            var countSpan = $(element).find(COUNT_CONTAINER_REF);
            if( countSpan.text() == '') {
              countSpan.text(0);
            }
          }
        })
        .fail(function (object, error) {
          console.log("Error: " + error.code + " " + error.message);
        });
    }

    function addCount(Counter) {
      var $visitors = $(".leancloud_visitors");
      var url = $visitors.attr('id').trim();
      var title = $visitors.attr('data-flag-title').trim();
      var query = new AV.Query(Counter);

      query.equalTo("url", url);
      query.find({
        success: function(results) {
          if (results.length > 0) {
            var counter = results[0];
            counter.fetchWhenSave(true);
            counter.increment("time");
            counter.save(null, {
              success: function(counter) {
                var $element = $(document.getElementById(url));
                $element.find('.leancloud-visitors-count').text(counter.get('time'));
              },
              error: function(counter, error) {
                console.log('Failed to save Visitor num, with error message: ' + error.message);
              }
            });
          } else {
            var newcounter = new Counter();
            /* Set ACL */
            var acl = new AV.ACL();
            acl.setPublicReadAccess(true);
            acl.setPublicWriteAccess(true);
            newcounter.setACL(acl);
            /* End Set ACL */
            newcounter.set("title", title);
            newcounter.set("url", url);
            newcounter.set("time", 1);
            newcounter.save(null, {
              success: function(newcounter) {
                var $element = $(document.getElementById(url));
                $element.find('.leancloud-visitors-count').text(newcounter.get('time'));
              },
              error: function(newcounter, error) {
                console.log('Failed to create');
              }
            });
          }
        },
        error: function(error) {
          console.log('Error:' + error.code + " " + error.message);
        }
      });
    }

    $(function() {
      var Counter = AV.Object.extend("Counter");
      if ($('.leancloud_visitors').length == 1) {
        addCount(Counter);
      } else if ($('.post-title-link').length > 1) {
        showTime(Counter);
      }
    });
  </script>



  
<script>
(function(){
    var bp = document.createElement('script');
    var curProtocol = window.location.protocol.split(':')[0];
    if (curProtocol === 'https') {
        bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';        
    }
    else {
        bp.src = 'http://push.zhanzhang.baidu.com/push.js';
    }
    var s = document.getElementsByTagName("script")[0];
    s.parentNode.insertBefore(bp, s);
})();
</script><!-- hexo-inject:begin --><!-- Begin: Injected MathJax -->
<script type="text/x-mathjax-config">
  MathJax.Hub.Config({"tex2jax":{"inlineMath":[["$","$"],["\\(","\\)"]],"skipTags":["script","noscript","style","textarea","pre","code"],"processEscapes":true},"TeX":{"equationNumbers":{"autoNumber":"AMS"}}});
</script>

<script type="text/x-mathjax-config">
  MathJax.Hub.Queue(function() {
    var all = MathJax.Hub.getAllJax(), i;
    for(i=0; i < all.length; i += 1) {
      all[i].SourceElement().parentNode.className += ' has-jax';
    }
  });
</script>

<script type="text/javascript" src="//cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML">
</script>
<!-- End: Injected MathJax -->
<!-- hexo-inject:end -->


  

  

</body>
</html>
