<!doctype html>



  


<html class="theme-next pisces use-motion" lang="zh-Hans">
<head>
  <!-- hexo-inject:begin --><!-- hexo-inject:end --><meta charset="UTF-8"/>
<meta http-equiv="X-UA-Compatible" content="IE=edge" />
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1"/>



<meta http-equiv="Cache-Control" content="no-transform" />
<meta http-equiv="Cache-Control" content="no-siteapp" />



  <meta name="google-site-verification" content="U-X9XD2c_wV6AKCoj83wtV4l3xqUDXk8YHuUj3GiYxg" />













  
  
  <link href="/lib/fancybox/source/jquery.fancybox.css?v=2.1.5" rel="stylesheet" type="text/css" />




  
  
  
  

  
    
    
  

  

  

  

  

  
    
    
    <link href="//fonts.googleapis.com/css?family=Lato:300,300italic,400,400italic,700,700italic&subset=latin,latin-ext" rel="stylesheet" type="text/css">
  






<link href="/lib/font-awesome/css/font-awesome.min.css?v=4.6.2" rel="stylesheet" type="text/css" />

<link href="/css/main.css?v=5.1.0" rel="stylesheet" type="text/css" />


  <meta name="keywords" content="Hexo, NexT" />








  <link rel="shortcut icon" type="image/x-icon" href="/favicon.ico?v=5.1.0" />






<meta name="description" content="一个没人知道的地方">
<meta property="og:type" content="website">
<meta property="og:title" content="博伟的博客">
<meta property="og:url" content="http://libowei.net/index.html">
<meta property="og:site_name" content="博伟的博客">
<meta property="og:description" content="一个没人知道的地方">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="博伟的博客">
<meta name="twitter:description" content="一个没人知道的地方">



<script type="text/javascript" id="hexo.configurations">
  var NexT = window.NexT || {};
  var CONFIG = {
    root: '/',
    scheme: 'Pisces',
    sidebar: {"position":"left","display":"post","offset":12,"offset_float":0,"b2t":false,"scrollpercent":false},
    fancybox: true,
    motion: true,
    duoshuo: {
      userId: '0',
      author: '博主'
    },
    algolia: {
      applicationID: '',
      apiKey: '',
      indexName: '',
      hits: {"per_page":10},
      labels: {"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}
    }
  };
</script>



  <link rel="canonical" href="http://libowei.net/"/>





  <title> 博伟的博客 </title><!-- hexo-inject:begin --><!-- hexo-inject:end -->
</head>

<body itemscope itemtype="http://schema.org/WebPage" lang="zh-Hans">

  




<!-- hexo-inject:begin --><!-- hexo-inject:end --><script>
  (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
            (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
          m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
  })(window,document,'script','https://www.google-analytics.com/analytics.js','ga');
  ga('create', 'UA-96532270-1', 'auto');
  ga('send', 'pageview');
</script>


  <script type="text/javascript">
    var _hmt = _hmt || [];
    (function() {
      var hm = document.createElement("script");
      hm.src = "https://hm.baidu.com/hm.js?ff1e2d8efa5365ff70fa3071bee477bb";
      var s = document.getElementsByTagName("script")[0];
      s.parentNode.insertBefore(hm, s);
    })();
  </script>



  <script type="text/javascript">
    (function() {
      var hm = document.createElement("script");
      hm.src = "//tajs.qq.com/stats?sId=61491911";
      var s = document.getElementsByTagName("script")[0];
      s.parentNode.insertBefore(hm, s);
    })();
  </script>








  
  
    
  

  <div class="container sidebar-position-left 
   page-home 
 ">
    <div class="headband"></div>

    <header id="header" class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-wrapper">
  <div class="site-meta ">
    

    <div class="custom-logo-site-title">
      <a href="/"  class="brand" rel="start">
        <span class="logo-line-before"><i></i></span>
        <span class="site-title">博伟的博客</span>
        <span class="logo-line-after"><i></i></span>
      </a>
    </div>
      
        <p class="site-subtitle">Albert's blog</p>
      
  </div>

  <div class="site-nav-toggle">
    <button>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
    </button>
  </div>
</div>

<nav class="site-nav">
  

  
    <ul id="menu" class="menu">
      
        
        <li class="menu-item menu-item-home">
          <a href="/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-home"></i> <br />
            
            首页
          </a>
        </li>
      
        
        <li class="menu-item menu-item-archives">
          <a href="/archives" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-archive"></i> <br />
            
            归档
          </a>
        </li>
      
        
        <li class="menu-item menu-item-tags">
          <a href="/tags" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-tags"></i> <br />
            
            标签
          </a>
        </li>
      

      
        <li class="menu-item menu-item-search">
          
            <a href="javascript:;" class="popup-trigger">
          
            
              <i class="menu-item-icon fa fa-search fa-fw"></i> <br />
            
            搜索
          </a>
        </li>
      
    </ul>
  

  
    <div class="site-search">
      
  <div class="popup search-popup local-search-popup">
  <div class="local-search-header clearfix">
    <span class="search-icon">
      <i class="fa fa-search"></i>
    </span>
    <span class="popup-btn-close">
      <i class="fa fa-times-circle"></i>
    </span>
    <div class="local-search-input-wrapper">
      <input autocapitalize="off" autocomplete="off" autocorrect="off"
             placeholder="搜索..." spellcheck="false"
             type="text" id="local-search-input">
    </div>
  </div>
  <div id="local-search-result"></div>
</div>



    </div>
  
</nav>



 </div>
    </header>

    <main id="main" class="main">
      <div class="main-inner">
        <div class="content-wrap">
          <div id="content" class="content">
            
  <section id="posts" class="posts-expand">
    
      

  

  
  
  

  <article class="post post-type-normal " itemscope itemtype="http://schema.org/Article">
    <link itemprop="mainEntityOfPage" href="http://libowei.net/TensorFlow运行多个计算图.html">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="Albert Lee">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/image/学士服照片.jpg">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="博伟的博客">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
            
            
              
                
                <a class="post-title-link" href="/TensorFlow运行多个计算图.html" itemprop="url">
                  TensorFlow运行多个计算图
                </a>
              
            
          </h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="创建于" itemprop="dateCreated datePublished" datetime="2017-11-08T09:24:17+08:00">
                2017-11-08
              </time>
            

            

            
          </span>

          

          
            
          

          
          
             <span id="/TensorFlow运行多个计算图.html" class="leancloud_visitors" data-flag-title="TensorFlow运行多个计算图">
               <span class="post-meta-divider">|</span>
               <span class="post-meta-item-icon">
                 <i class="fa fa-eye"></i>
               </span>
               
                 <span class="post-meta-item-text">阅读次数 </span>
               
                 <span class="leancloud-visitors-count"></span>
             </span>
          

          

          

          

        </div>
      </header>
    


    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <p>有这样的需求：在同一个程序中需要运行两个训练好的TensorFlow模型。<br>如果仅仅是把两个模型放到两个类中，简单的把两个类的代码组合到一起，会出现问题。</p>
<p>这是由于TensorFlow的运行基于计算图(Graph)，在代码中不明确指定当前所用的图的情况下，TensorFlow会自动创建一个图，作为当前的默认图。创建的所有Tensor和计算(op)都会添加到当前默认图中。</p>
<p>而TensorFlow的会话(Session)也是需要基于图来创建，<code>tf.Session()</code>可以通过<code>graph</code>参数来指定Session运行于哪个图上。</p>
<p>当在不明确指定图的情况下，创建两个模型时，两个模型会在同一张图中，可能会出现变量名相同、变量形状不匹配、Session不能启动等问题。</p>
<p>解决方法是在每个模型中自行定义所使用的图，并在向图中添加Tensor和op时将该模型的图作为默认图，在启动Session时指定运行图为当前的图。<br>例如两个模型分别封装在两个类中</p>
<ol>
<li>在创建类时，定义变量<code>self.graph = tf.Graph()</code></li>
<li>在定义网络结构时，在每个Tensor和op前添加<code>with self.graph.as_default():</code></li>
<li>运行Session时，传入当前的图：<code>self.sess = tf.Session(graph=self.graph)</code></li>
</ol>
<p>这样，定义模型结构和运行会话都分别在两个图中进行，不会产生冲突。</p>
<p>参考资料：</p>
<ol>
<li><a href="http://blog.csdn.net/xierhacker/article/details/53860379" target="_blank" rel="external">http://blog.csdn.net/xierhacker/article/details/53860379</a></li>
<li><a href="https://stackoverflow.com/questions/42593771/session-graph-is-empty" target="_blank" rel="external">https://stackoverflow.com/questions/42593771/session-graph-is-empty</a></li>
</ol>

          
        
      
    </div>

    <div>
      
    </div>
	
	
	<div>
      
    </div>
	

    <div>
      
    </div>

    <div>
      
    </div>

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal " itemscope itemtype="http://schema.org/Article">
    <link itemprop="mainEntityOfPage" href="http://libowei.net/TensorFlow-DataSet-API的一个问题.html">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="Albert Lee">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/image/学士服照片.jpg">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="博伟的博客">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
            
            
              
                
                <a class="post-title-link" href="/TensorFlow-DataSet-API的一个问题.html" itemprop="url">
                  TensorFlow DataSet API的一个问题
                </a>
              
            
          </h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="创建于" itemprop="dateCreated datePublished" datetime="2017-11-01T10:06:48+08:00">
                2017-11-01
              </time>
            

            

            
          </span>

          

          
            
          

          
          
             <span id="/TensorFlow-DataSet-API的一个问题.html" class="leancloud_visitors" data-flag-title="TensorFlow DataSet API的一个问题">
               <span class="post-meta-divider">|</span>
               <span class="post-meta-item-icon">
                 <i class="fa fa-eye"></i>
               </span>
               
                 <span class="post-meta-item-text">阅读次数 </span>
               
                 <span class="leancloud-visitors-count"></span>
             </span>
          

          

          

          

        </div>
      </header>
    


    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <p>在使用TensorFlow新的DataSet API时遇到了一个坑。</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line">batched_dataset = dataset.batch(4)</div><div class="line">iterator = batched_dataset.make_one_shot_iterator()</div><div class="line">next_element = iterator.get_next()</div></pre></td></tr></table></figure>
<p><code>DataSet.batch(batch_size)</code>会返回一个第一个维度为<code>batch_size</code>大小的Tensor。但当不断调用<code>interator.get_next()</code>，遍历到样本集的末端时，返回的Tensor的大小可能不为<code>batch_size</code>。</p>
<p>例如样本集中有17个元素，<code>batch_size</code>为4时，第5次调用<code>iterator.get_next()</code>只能返回大小为1的batch。</p>
<p><code>DataSet.repeat()</code>会返回一个无限重复原数据集的DataSet对象，当调用了repeat()时，也会产生上面的问题，有问题代码如下：<br><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div></pre></td><td class="code"><pre><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">read_batch</span><span class="params">(self, batch_size=None, shuffle_buffer=None, repeat=None)</span>:</span></div><div class="line">    dataset = tf.contrib.data.TFRecordDataset(self._files)</div><div class="line">    dataset = dataset.map(self.__parse_function)</div><div class="line">    <span class="keyword">if</span> batch_size:</div><div class="line">        dataset = dataset.batch(batch_size)</div><div class="line">    <span class="keyword">if</span> shuffle_buffer:</div><div class="line">        dataset = dataset.shuffle(buffer_size=shuffle_buffer)</div><div class="line">    <span class="keyword">if</span> repeat != <span class="number">0</span>:</div><div class="line">        dataset = dataset.repeat(repeat)</div><div class="line">    iterator = dataset.make_one_shot_iterator()</div><div class="line">    batch_data = iterator.get_next()</div><div class="line">    <span class="keyword">return</span> self._extract_batch(batch_data)</div></pre></td></tr></table></figure></p>
<p>在遍历数据集的最后一个batch的大小不为<code>batch_size</code>。</p>
<p>在stackoverflow找到了类似的问题：<a href="https://stackoverflow.com/questions/46708822/returned-size-of-tensorflows-dataset-api-is-not-constant" target="_blank" rel="external">returned size of tensorflow’s dataset API is not constant</a></p>
<p>解决方法为：在<code>batch()</code>之前调用<code>repeat()</code>  …… ……</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div></pre></td><td class="code"><pre><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">read_batch</span><span class="params">(self, batch_size=None, shuffle_buffer=None, repeat=None)</span>:</span></div><div class="line">    dataset = tf.contrib.data.TFRecordDataset(self._files)</div><div class="line">    dataset = dataset.map(self.__parse_function)</div><div class="line">    <span class="keyword">if</span> shuffle_buffer:</div><div class="line">        dataset = dataset.shuffle(buffer_size=shuffle_buffer)</div><div class="line">    <span class="keyword">if</span> repeat != <span class="number">0</span>:</div><div class="line">        dataset = dataset.repeat(repeat)</div><div class="line">    <span class="keyword">if</span> batch_size:</div><div class="line">        dataset = dataset.batch(batch_size)</div><div class="line">    iterator = dataset.make_one_shot_iterator()</div><div class="line">    batch_data = iterator.get_next()</div><div class="line">    <span class="keyword">return</span> self._extract_batch(batch_data)</div></pre></td></tr></table></figure>

          
        
      
    </div>

    <div>
      
    </div>
	
	
	<div>
      
    </div>
	

    <div>
      
    </div>

    <div>
      
    </div>

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal " itemscope itemtype="http://schema.org/Article">
    <link itemprop="mainEntityOfPage" href="http://libowei.net/TensorFlow学习笔记：Importing-Data.html">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="Albert Lee">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/image/学士服照片.jpg">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="博伟的博客">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
            
            
              
                
                <a class="post-title-link" href="/TensorFlow学习笔记：Importing-Data.html" itemprop="url">
                  TensorFlow学习笔记：Importing Data
                </a>
              
            
          </h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="创建于" itemprop="dateCreated datePublished" datetime="2017-09-26T14:08:44+08:00">
                2017-09-26
              </time>
            

            

            
          </span>

          

          
            
          

          
          
             <span id="/TensorFlow学习笔记：Importing-Data.html" class="leancloud_visitors" data-flag-title="TensorFlow学习笔记：Importing Data">
               <span class="post-meta-divider">|</span>
               <span class="post-meta-item-icon">
                 <i class="fa fa-eye"></i>
               </span>
               
                 <span class="post-meta-item-text">阅读次数 </span>
               
                 <span class="leancloud-visitors-count"></span>
             </span>
          

          

          

          

        </div>
      </header>
    


    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <p><code>Dataset</code>API可使你从简单、可重用的片段构建复杂的输入管道。例如，一个图像模型的输入管道可能从分布式的系统中聚合数据，对每个图像进行随机扰动，并随机选择一批图像用于训练；文本模型的输入管道可能从原始的文本中提取特征，利用查找表将其转化为embedding identifiers，并将不同长度的序列组合在一起。<code>Dataset</code>API可以很容易的处理大量数据、不同数据格式和复杂的变换。</p>
<p><code>Dataset</code>API引入了两个抽象概念：</p>
<ul>
<li>一个<code>tf.contrib.data.Dataset</code>代表一个元素序列，每个元素包含一个或多个Tensor对象。例如图像管道中，一个元素可能是包含一对表示图像数据和类别标签的Tensor的单一训练样本。有两种创建dataset的方式：<ul>
<li>创建<strong>source</strong>（如<code>Dataset.from_tensor_slices()</code>），由一个或多个<code>tf.Tensor</code>对象构建一个dataset。</li>
<li>应用<strong>transformation</strong>（如<code>Dataset.batch()</code>），从一个或多个<code>tf.contrib.data.Dataset</code>对象中创建新dataset。</li>
</ul>
</li>
<li>一个<code>tf.contrib.data.Iterator</code>提供从dataset提取元素的方法。<code>Iterator.get_next()</code>返回的op执行时产生<code>Dataset</code>的下一个元素，这通常作为输入管道和模型之间的接口。最简单的迭代器是”one-shot 迭代器”，它与一个特定的<code>Dataset</code>相关联，作一次性遍历。<code>Iterator.initializer</code>op能够让你用不同的数据集再初始化和参数化一个迭代器，这样你可以在相同的程序中多次迭代训练集/验证集数据。</li>
</ul>
<h2 id="基本机制"><a href="#基本机制" class="headerlink" title="基本机制"></a>基本机制</h2><p>这一节讲了创建不同种类的<code>Dataset</code>和<code>Iterator</code>对象的基本方法，和如何从其中提取数据。</p>
<p>启动输入管道前，必须定义一个<em>source</em>。例如，从内存中的一些Tensor中创建一个<code>Dataset</code>，可使用<code>tf.contrib.data.Dataset.from_tensors()</code>或<code>tf.contrib.data.Dataset.from_tensor_slices()</code>。如果你的输入数据来自磁盘中的TFRecord，使用<code>tf.contrib.data.TFRecordDataset</code>。</p>
<p>一旦有了<code>Dataset</code>对象，可以<em>transform</em>它，调用链接方法生成新的<code>Dataset</code>。例如，可以应用每元变换（per-element transformations）如<code>Dataset.map()</code>，和多元变换（multi-element transformations）如<code>Dataset.batch()</code>。查看<a href="https://www.tensorflow.org/api_docs/python/tf/contrib/data/Dataset" target="_blank" rel="external"><code>tf.contrib.data.Dataset</code>文档</a>了解所有的变换方法。</p>
<p>从<code>Dataset</code>中取值最常见的方法是通过<strong>iterator</strong>对象每次获取数据集中的一个元素（如调用<code>Dataset.make_one_shot_iterator</code>）。<code>tf.contrib.data.Iterator</code>对象提供两种运算：<code>Iterator.initializer</code>可（再）初始化迭代器的状态；<code>Iterator.get_next()</code>返回下一个元素的Tensor对象。取决于你的应用，可以选择不同类型的迭代器，概述如下。</p>
<h3 id="数据集结构"><a href="#数据集结构" class="headerlink" title="数据集结构"></a>数据集结构</h3><p>一个dataset包含的每个元素都有相同的结构。每个元素包含一个或多个<code>tf.Tensor</code>对象，称为<em>component</em>。每个component的<code>tf.DType</code>代表tensor中元素的类型，<code>tf.TensorShape</code>代表每个元素的固定的形状（可能只指定一部分维度）。<code>Dataset.output_types</code>和<code>Dataset.output_shapes</code>属性使你能对推断出的数据集元素的类型和形状进行检查。这种属性的嵌套结构对应元素的结构，元素可以是单个Tensor，Tensor元组或嵌套的Tensor元组，例如：<br><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div></pre></td><td class="code"><pre><div class="line">dataset1 = tf.contrib.data.Dataset.from_tensor_slices(tf.random_uniform([<span class="number">4</span>, <span class="number">10</span>]))</div><div class="line">print(dataset1.output_types)  <span class="comment"># ==&gt; "tf.float32"</span></div><div class="line">print(dataset1.output_shapes)  <span class="comment"># ==&gt; "(10,)"</span></div><div class="line"></div><div class="line">dataset2 = tf.contrib.data.Dataset.from_tensor_slices(</div><div class="line">   (tf.random_uniform([<span class="number">4</span>]),</div><div class="line">    tf.random_uniform([<span class="number">4</span>, <span class="number">100</span>], maxval=<span class="number">100</span>, dtype=tf.int32)))</div><div class="line">print(dataset2.output_types)  <span class="comment"># ==&gt; "(tf.float32, tf.int32)"</span></div><div class="line">print(dataset2.output_shapes)  <span class="comment"># ==&gt; "((), (100,))"</span></div><div class="line"></div><div class="line">dataset3 = tf.contrib.data.Dataset.zip((dataset1, dataset2))</div><div class="line">print(dataset3.output_types)  <span class="comment"># ==&gt; (tf.float32, (tf.float32, tf.int32))</span></div><div class="line">print(dataset3.output_shapes)  <span class="comment"># ==&gt; "(10, ((), (100,)))"</span></div></pre></td></tr></table></figure></p>
<p>给元素的每个component命名很方便，如果不同的component表示训练样本的不同的特征。除了使用元组之外，还可以用<code>collections.namedtuple</code>或一个{string : tensor}的字典表示数据集的一个元素。<br><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div></pre></td><td class="code"><pre><div class="line">dataset = tf.contrib.data.Dataset.from_tensor_slices(</div><div class="line">   &#123;<span class="string">"a"</span>: tf.random_uniform([<span class="number">4</span>]),</div><div class="line">    <span class="string">"b"</span>: tf.random_uniform([<span class="number">4</span>, <span class="number">100</span>], maxval=<span class="number">100</span>, dtype=tf.int32)&#125;)</div><div class="line">print(dataset.output_types)  <span class="comment"># ==&gt; "&#123;'a': tf.float32, 'b': tf.int32&#125;"</span></div><div class="line">print(dataset.output_shapes)  <span class="comment"># ==&gt; "&#123;'a': (), 'b': (100,)&#125;"</span></div></pre></td></tr></table></figure></p>
<p><code>Dataset</code>变换支持任何结构的数据集。当使用<code>Dataset.map()</code>，<code>Dataset.flat_map()</code>和<code>Dataset.filter()</code>变换时，需要根据元素的结构来定义这些函数的参数：<br><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div></pre></td><td class="code"><pre><div class="line">dataset1 = dataset1.map(<span class="keyword">lambda</span> x: ...)</div><div class="line"></div><div class="line">dataset2 = dataset2.flat_map(<span class="keyword">lambda</span> x, y: ...)</div><div class="line"></div><div class="line"><span class="comment"># Note: Argument destructuring is not available in Python 3.</span></div><div class="line">dataset3 = dataset3.filter(<span class="keyword">lambda</span> x, (y, z): ...)</div></pre></td></tr></table></figure></p>
<h3 id="创建迭代器"><a href="#创建迭代器" class="headerlink" title="创建迭代器"></a>创建迭代器</h3><p>你已经创建了表示输入数据的<code>Dataset</code>，下一步就是创建一个<code>Iterator</code>来获取数据集中的元素。<code>Dataset</code>API目前支持三种迭代器：</p>
<ul>
<li><strong>one-shot</strong></li>
<li><strong>initializable</strong></li>
<li><strong>reinitializable</strong> 和 <strong>feedable</strong></li>
</ul>
<p><strong>one shot</strong> 迭代器是最简单的形式，只支持一次性遍历数据集，不需要显式的初始化操作。当前绝大多数基于队列的输入管道所支持的情况它都能处理，但不支持参数化。<br><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div></pre></td><td class="code"><pre><div class="line">dataset = tf.contrib.data.Dataset.range(<span class="number">100</span>)</div><div class="line">iterator = dataset.make_one_shot_iterator()</div><div class="line">next_element = iterator.get_next()</div><div class="line"></div><div class="line"><span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">100</span>):</div><div class="line">  value = sess.run(next_element)</div><div class="line">  <span class="keyword">assert</span> i == value</div></pre></td></tr></table></figure></p>
<p><strong>initializable</strong> 迭代器需要在使用前显示的定义<code>iterator.initializer</code>。可以根据数据集对迭代器操作进行参数化，可在初始化时使用<code>tf.placeholder()</code>Tensor作为feed。<br><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div></pre></td><td class="code"><pre><div class="line">max_value = tf.placeholder(tf.int64, shape=[])</div><div class="line">dataset = tf.contrib.data.Dataset.range(max_value)</div><div class="line">iterator = dataset.make_initializable_iterator()</div><div class="line">next_element = iterator.get_next()</div><div class="line"></div><div class="line"><span class="comment"># Initialize an iterator over a dataset with 10 elements.</span></div><div class="line">sess.run(iterator.initializer, feed_dict=&#123;max_value: <span class="number">10</span>&#125;)</div><div class="line"><span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">10</span>):</div><div class="line">  value = sess.run(next_element)</div><div class="line">  <span class="keyword">assert</span> i == value</div><div class="line"></div><div class="line"><span class="comment"># Initialize the same iterator over a dataset with 100 elements.</span></div><div class="line">sess.run(iterator.initializer, feed_dict=&#123;max_value: <span class="number">100</span>&#125;)</div><div class="line"><span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">100</span>):</div><div class="line">  value = sess.run(next_element)</div><div class="line">  <span class="keyword">assert</span> i == value</div></pre></td></tr></table></figure></p>
<p><strong>reinitializable</strong> 迭代器可以以多个不同的<code>Dataset</code>对象进行初始化。例如，你可能有一个这样的训练集输入管道：对输入的图像进行随机扰动，和用于在原始图像进行评估的验证集输入管道。这两个管道使用不同的<code>Dataset</code>对象，但是它们的结构是相同的（每个component有相同类型和兼容的形状）。<br><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div></pre></td><td class="code"><pre><div class="line"><span class="comment"># Define training and validation datasets with the same structure.</span></div><div class="line">training_dataset = tf.contrib.data.Dataset.range(<span class="number">100</span>).map(</div><div class="line">    <span class="keyword">lambda</span> x: x + tf.random_uniform([], <span class="number">-10</span>, <span class="number">10</span>, tf.int64))</div><div class="line">validation_dataset = tf.contrib.data.Dataset.range(<span class="number">50</span>)</div><div class="line"></div><div class="line"><span class="comment"># A reinitializable iterator is defined by its structure. We could use the</span></div><div class="line"><span class="comment"># `output_types` and `output_shapes` properties of either `training_dataset`</span></div><div class="line"><span class="comment"># or `validation_dataset` here, because they are compatible.</span></div><div class="line">iterator = Iterator.from_structure(training_dataset.output_types,</div><div class="line">                                   training_dataset.output_shapes)</div><div class="line">next_element = iterator.get_next()</div><div class="line"></div><div class="line">training_init_op = iterator.make_initializer(training_dataset)</div><div class="line">validation_init_op = iterator.make_initializer(validation_dataset)</div><div class="line"></div><div class="line"><span class="comment"># Run 20 epochs in which the training dataset is traversed, followed by the</span></div><div class="line"><span class="comment"># validation dataset.</span></div><div class="line"><span class="keyword">for</span> _ <span class="keyword">in</span> range(<span class="number">20</span>):</div><div class="line">  <span class="comment"># Initialize an iterator over the training dataset.</span></div><div class="line">  sess.run(training_init_op)</div><div class="line">  <span class="keyword">for</span> _ <span class="keyword">in</span> range(<span class="number">100</span>):</div><div class="line">    sess.run(next_element)</div><div class="line"></div><div class="line">  <span class="comment"># Initialize an iterator over the validation dataset.</span></div><div class="line">  sess.run(validation_init_op)</div><div class="line">  <span class="keyword">for</span> _ <span class="keyword">in</span> range(<span class="number">50</span>):</div><div class="line">    sess.run(next_element)</div></pre></td></tr></table></figure></p>
<p><strong>feedable</strong> 迭代器可以在调用<code>tf.Session.run</code>时和<code>tf.placeholder</code>一起使用，通过<code>feed_dict</code>机制，以选择使用哪种<code>Iterator</code>。它与reinitializable迭代器有相同的功能，但在你切换迭代器时不需要从数据集的开始初始化迭代器。举例：使用和上面例子相同的训练和验证样本，可以使用<code>tf.contrib.data.Iterator.from_string_handle</code>来定义一个feedable迭代器，用来在两个数据集间切换。<br><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div></pre></td><td class="code"><pre><div class="line"><span class="comment"># Define training and validation datasets with the same structure.</span></div><div class="line">training_dataset = tf.contrib.data.Dataset.range(<span class="number">100</span>).map(</div><div class="line">    <span class="keyword">lambda</span> x: x + tf.random_uniform([], <span class="number">-10</span>, <span class="number">10</span>, tf.int64)).repeat()</div><div class="line">validation_dataset = tf.contrib.data.Dataset.range(<span class="number">50</span>)</div><div class="line"></div><div class="line"><span class="comment"># A feedable iterator is defined by a handle placeholder and its structure. We</span></div><div class="line"><span class="comment"># could use the `output_types` and `output_shapes` properties of either</span></div><div class="line"><span class="comment"># `training_dataset` or `validation_dataset` here, because they have</span></div><div class="line"><span class="comment"># identical structure.</span></div><div class="line">handle = tf.placeholder(tf.string, shape=[])</div><div class="line">iterator = tf.contrib.data.Iterator.from_string_handle(</div><div class="line">    handle, training_dataset.output_types, training_dataset.output_shapes)</div><div class="line">next_element = iterator.get_next()</div><div class="line"></div><div class="line"><span class="comment"># You can use feedable iterators with a variety of different kinds of iterator</span></div><div class="line"><span class="comment"># (such as one-shot and initializable iterators).</span></div><div class="line">training_iterator = training_dataset.make_one_shot_iterator()</div><div class="line">validation_iterator = validation_dataset.make_initializable_iterator()</div><div class="line"></div><div class="line"><span class="comment"># The `Iterator.string_handle()` method returns a tensor that can be evaluated</span></div><div class="line"><span class="comment"># and used to feed the `handle` placeholder.</span></div><div class="line">training_handle = sess.run(training_iterator.string_handle())</div><div class="line">validation_handle = sess.run(validation_iterator.string_handle())</div><div class="line"></div><div class="line"><span class="comment"># Loop forever, alternating between training and validation.</span></div><div class="line"><span class="keyword">while</span> <span class="keyword">True</span>:</div><div class="line">  <span class="comment"># Run 200 steps using the training dataset. Note that the training dataset is</span></div><div class="line">  <span class="comment"># infinite, and we resume from where we left off in the previous `while` loop</span></div><div class="line">  <span class="comment"># iteration.</span></div><div class="line">  <span class="keyword">for</span> _ <span class="keyword">in</span> range(<span class="number">200</span>):</div><div class="line">    sess.run(next_element, feed_dict=&#123;handle: training_handle&#125;)</div><div class="line"></div><div class="line">  <span class="comment"># Run one pass over the validation dataset.</span></div><div class="line">  sess.run(validation_iterator.initializer)</div><div class="line">  <span class="keyword">for</span> _ <span class="keyword">in</span> range(<span class="number">50</span>):</div><div class="line">    sess.run(next_element, feed_dict=&#123;handle: validation_handle&#125;)</div></pre></td></tr></table></figure></p>
<h3 id="从迭代器中取值"><a href="#从迭代器中取值" class="headerlink" title="从迭代器中取值"></a>从迭代器中取值</h3><p><code>Iterator.get_next()</code>方法返回一个或多个Tensor对象，对应迭代器的下一个元素。每次这些Tensor被求值时，它们从底层的数据集中取下一个元素的值。（注意：同其他具有状态的对象相似，调用<code>Iterator.get_next()</code>不会立即执行，你必须在<code>tf.Session.run</code>中传递其返回的Tensor对象的相应表达式，才会取得下一个元素并推进迭代器。）</p>
<p>如果迭代器到达数据集的末端，执行<code>Iterator.get_next()</code>会产生<code>tf.errors.OutOfRangeError</code>。之后迭代器会变成不可用状态，如果想再次使用必须对其重新初始化。<br><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div></pre></td><td class="code"><pre><div class="line">dataset = tf.contrib.data.Dataset.range(<span class="number">5</span>)</div><div class="line">iterator = dataset.make_initializable_iterator()</div><div class="line">next_element = iterator.get_next()</div><div class="line"></div><div class="line"><span class="comment"># Typically `result` will be the output of a model, or an optimizer's</span></div><div class="line"><span class="comment"># training operation.</span></div><div class="line">result = tf.add(next_element, next_element)</div><div class="line"></div><div class="line">sess.run(iterator.initializer)</div><div class="line">print(sess.run(result))  <span class="comment"># ==&gt; "0"</span></div><div class="line">print(sess.run(result))  <span class="comment"># ==&gt; "2"</span></div><div class="line">print(sess.run(result))  <span class="comment"># ==&gt; "4"</span></div><div class="line">print(sess.run(result))  <span class="comment"># ==&gt; "6"</span></div><div class="line">print(sess.run(result))  <span class="comment"># ==&gt; "8"</span></div><div class="line"><span class="keyword">try</span>:</div><div class="line">  sess.run(result)</div><div class="line"><span class="keyword">except</span> tf.errors.OutOfRangeError:</div><div class="line">  print(<span class="string">"End of dataset"</span>)  <span class="comment"># ==&gt; "End of dataset"</span></div></pre></td></tr></table></figure></p>
<p>常见的形式是在<code>try-except</code>里进行训练循环：<br><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div></pre></td><td class="code"><pre><div class="line">sess.run(iterator.initializer)</div><div class="line"><span class="keyword">while</span> <span class="keyword">True</span>:</div><div class="line">  <span class="keyword">try</span>:</div><div class="line">    sess.run(result)</div><div class="line">  <span class="keyword">except</span> tf.errors.OutOfRangeError:</div><div class="line">    <span class="keyword">break</span></div></pre></td></tr></table></figure></p>
<p>如果数据集中的每个元素都是嵌套结构，<code>Iterator.get_next()</code>返回的一个或多个Tensor也是这种相同的嵌套结构：<br><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div></pre></td><td class="code"><pre><div class="line">dataset1 = tf.contrib.data.Dataset.from_tensor_slices(tf.random_uniform([<span class="number">4</span>, <span class="number">10</span>]))</div><div class="line">dataset2 = tf.contrib.data.Dataset.from_tensor_slices((tf.random_uniform([<span class="number">4</span>]), tf.random_uniform([<span class="number">4</span>, <span class="number">100</span>])))</div><div class="line">dataset3 = tf.contrib.data.Dataset.zip((dataset1, dataset2))</div><div class="line"></div><div class="line">iterator = dataset3.make_initializable_iterator()</div><div class="line"></div><div class="line">sess.run(iterator.initializer)</div><div class="line">next1, (next2, next3) = iterator.get_next()</div></pre></td></tr></table></figure></p>
<p>注意对<code>next1</code>,<code>next2</code>或<code>next3</code>任意一个求值都会推进迭代器，典型的迭代器取值的做法是在一个表达式中包含其所有component。</p>
<h2 id="读输入数据"><a href="#读输入数据" class="headerlink" title="读输入数据"></a>读输入数据</h2><h3 id="Numpy-数组"><a href="#Numpy-数组" class="headerlink" title="Numpy 数组"></a>Numpy 数组</h3><p>如果全部输入数据都在内存中，最简单的方式是由其创建<code>Dataset</code>，转化为<code>tf.Tensor</code>，使用<code>Dataset.from_tensor_slices()</code><br><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div></pre></td><td class="code"><pre><div class="line"><span class="comment"># Load the training data into two NumPy arrays, for example using `np.load()`.</span></div><div class="line"><span class="keyword">with</span> np.load(<span class="string">"/var/data/training_data.npy"</span>) <span class="keyword">as</span> data:</div><div class="line">  features = data[<span class="string">"features"</span>]</div><div class="line">  labels = data[<span class="string">"labels"</span>]</div><div class="line"></div><div class="line"><span class="comment"># Assume that each row of `features` corresponds to the same row as `labels`.</span></div><div class="line"><span class="keyword">assert</span> features.shape[<span class="number">0</span>] == labels.shape[<span class="number">0</span>]</div><div class="line"></div><div class="line">dataset = tf.contrib.data.Dataset.from_tensor_slices((features, labels))</div></pre></td></tr></table></figure></p>
<p>上面的代码会以<code>tf.constant()</code>op在TensorFlow计算图种表示<code>features</code>和<code>labels</code>数组。这适合小的数据集，但是很浪费内存，因为数组的内容会被复制多次，可能会超出<code>tf.GraphDef</code>定义的protocol buffer的2GB大小限制。</p>
<p>作为替代方案，可以以<code>tf.placeholder()</code>Tensor的形式定义<code>Dataset</code>，在迭代器初始化时 <em>feed</em> Numpy 数组。<br><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div></pre></td><td class="code"><pre><div class="line"><span class="comment"># Load the training data into two NumPy arrays, for example using `np.load()`.</span></div><div class="line"><span class="keyword">with</span> np.load(<span class="string">"/var/data/training_data.npy"</span>) <span class="keyword">as</span> data:</div><div class="line">  features = data[<span class="string">"features"</span>]</div><div class="line">  labels = data[<span class="string">"labels"</span>]</div><div class="line"></div><div class="line"><span class="comment"># Assume that each row of `features` corresponds to the same row as `labels`.</span></div><div class="line"><span class="keyword">assert</span> features.shape[<span class="number">0</span>] == labels.shape[<span class="number">0</span>]</div><div class="line"></div><div class="line">features_placeholder = tf.placeholder(features.dtype, features.shape)</div><div class="line">labels_placeholder = tf.placeholder(labels.dtype, labels.shape)</div><div class="line"></div><div class="line">dataset = tf.contrib.data.Dataset.from_tensor_slices((features_placeholder, labels_placeholder))</div><div class="line"><span class="comment"># [Other transformations on `dataset`...]</span></div><div class="line">dataset = ...</div><div class="line">iterator = dataset.make_initializable_iterator()</div><div class="line"></div><div class="line">sess.run(iterator.initializer, feed_dict=&#123;features_placeholder: features,</div><div class="line">                                          labels_placeholder: labels&#125;)</div></pre></td></tr></table></figure></p>
<h3 id="TFRecord-数据"><a href="#TFRecord-数据" class="headerlink" title="TFRecord 数据"></a>TFRecord 数据</h3><p><code>Dataset</code>API支持多种文件格式类型，所以你可以处理不能完整读入内存的大的数据集。以TFRecord为例，TFRecord是一种面向记录的二进制文件，很多TensorFlow应用使用它作为训练数据。<code>tf.contrib.data.TFRecordDataset</code>能够使TFRecord文件作为输入管道的输入流。<br><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line"><span class="comment"># Creates a dataset that reads all of the examples from two files.</span></div><div class="line">filenames = [<span class="string">"/var/data/file1.tfrecord"</span>, <span class="string">"/var/data/file2.tfrecord"</span>]</div><div class="line">dataset = tf.contrib.data.TFRecordDataset(filenames)</div></pre></td></tr></table></figure></p>
<p>传递给<code>TFRecordDataset</code>的参数<code>filenames</code>可以是字符串，字符串列表或<code>tf.Tensor</code>类型的字符串。因此，如果有两组文件分别作为训练和验证，可以使用<code>tf.placeholder(tf.string)</code>来表示文件名，使用适当的文件名来初始化一个迭代器。<br><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div></pre></td><td class="code"><pre><div class="line">filenames = tf.placeholder(tf.string, shape=[<span class="keyword">None</span>])</div><div class="line">dataset = tf.contrib.data.TFRecordDataset(filenames)</div><div class="line">dataset = dataset.map(...)  <span class="comment"># Parse the record into tensors.</span></div><div class="line">dataset = dataset.repeat()  <span class="comment"># Repeat the input indefinitely.</span></div><div class="line">dataset = dataset.batch(<span class="number">32</span>)</div><div class="line">iterator = dataset.make_initializable_iterator()</div><div class="line"></div><div class="line"><span class="comment"># You can feed the initializer with the appropriate filenames for the current</span></div><div class="line"><span class="comment"># phase of execution, e.g. training vs. validation.</span></div><div class="line"></div><div class="line"><span class="comment"># Initialize `iterator` with training data.</span></div><div class="line">training_filenames = [<span class="string">"/var/data/file1.tfrecord"</span>, <span class="string">"/var/data/file2.tfrecord"</span>]</div><div class="line">sess.run(iterator.initializer, feed_dict=&#123;filenames: training_filenames&#125;)</div><div class="line"></div><div class="line"><span class="comment"># Initialize `iterator` with validation data.</span></div><div class="line">validation_filenames = [<span class="string">"/var/data/validation1.tfrecord"</span>, ...]</div><div class="line">sess.run(iterator.initializer, feed_dict=&#123;filenames: validation_filenames&#125;)</div></pre></td></tr></table></figure></p>
<p>###文本数据<br>很多数据集是一个或多个文本文件。<code>tf.contrib.data.TextLineDataset</code>提供了一种简单的方式来提取这些文件的每一行。给定一个或多个文件名，<code>TextLineDataset</code>会对这些文件的每行生成一个值为字符串的元素。<code>TextLineDataset</code>也可以接受<code>tf.Tensor</code>作为<code>filenames</code>，所以你可以传递一个<code>tf.placeholder(tf.string)</code>作为参数。<br><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">filenames = [<span class="string">"/var/data/file1.txt"</span>, <span class="string">"/var/data/file2.txt"</span>]</div><div class="line">dataset = tf.contrib.data.TextLineDataset(filenames)</div></pre></td></tr></table></figure></p>
<p>默认下，<code>TextLineDataset</code>生成每个文件中的每一行，这可能不是你所需要的，例如文件中有标题行，或包含注释。可以使用<code>Dataset.skip()</code>和<code>Dataset.filter()</code>来剔除这些行。为了对每个文件都各自应用这些变换，使用<code>Dataset.flat_map()</code>来对每个文件创建一个嵌套的<code>Dataset</code><br><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div></pre></td><td class="code"><pre><div class="line">filenames = [<span class="string">"/var/data/file1.txt"</span>, <span class="string">"/var/data/file2.txt"</span>]</div><div class="line"></div><div class="line">dataset = tf.contrib.data.Dataset.from_tensor_slices(filenames)</div><div class="line"></div><div class="line"><span class="comment"># Use `Dataset.flat_map()` to transform each file as a separate nested dataset,</span></div><div class="line"><span class="comment"># and then concatenate their contents sequentially into a single "flat" dataset.</span></div><div class="line"><span class="comment"># * Skip the first line (header row).</span></div><div class="line"><span class="comment"># * Filter out lines beginning with "#" (comments).</span></div><div class="line">dataset = dataset.flat_map(</div><div class="line">    <span class="keyword">lambda</span> filename: (</div><div class="line">        tf.contrib.data.TextLineDataset(filename)</div><div class="line">        .skip(<span class="number">1</span>)</div><div class="line">        .filter(<span class="keyword">lambda</span> line: tf.not_equal(tf.substr(line, <span class="number">0</span>, <span class="number">1</span>), <span class="string">"#"</span>))))</div></pre></td></tr></table></figure></p>
<h2 id="使用Dataset-map-进行数据预处理"><a href="#使用Dataset-map-进行数据预处理" class="headerlink" title="使用Dataset.map()进行数据预处理"></a>使用Dataset.map()进行数据预处理</h2><p><code>Dataset.map(f)</code>变换对每个元素应用给定的函数<code>f</code>生成一个新的dataset。这个函数基于函数式编程中通用的<code>map()</code>函数，应用于列表型的数据结构。函数<code>f</code>接受代表单个元素的<code>tf.Tensor</code>作为输入，返回<code>tf.Tensor</code>对象作为新数据集中的元素。在实现上，使用了标准的TensorFlow op将一个元素转换为另一个。</p>
<p>这一节介绍了使用<code>Dataset.map()</code>的例子。</p>
<h3 id="解析tf-Example-protocol-buffer"><a href="#解析tf-Example-protocol-buffer" class="headerlink" title="解析tf.Example protocol buffer"></a>解析tf.Example protocol buffer</h3><p>很多输入管道从TFRecord格式的文件中提取<code>tf.train.Example</code> protocol buffer。每个<code>tf.train.Example</code>记录包含一个或多个features，输入管道把这些featrues转换成Tensor。<br><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div></pre></td><td class="code"><pre><div class="line"><span class="comment"># Transforms a scalar string `example_proto` into a pair of a scalar string and</span></div><div class="line"><span class="comment"># a scalar integer, representing an image and its label, respectively.</span></div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">_parse_function</span><span class="params">(example_proto)</span>:</span></div><div class="line">  features = &#123;<span class="string">"image"</span>: tf.FixedLenFeature((), tf.string, default_value=<span class="string">""</span>),</div><div class="line">              <span class="string">"label"</span>: tf.FixedLenFeature((), tf.int32, default_value=<span class="number">0</span>)&#125;</div><div class="line">  parsed_features = tf.parse_single_example(example_proto, features)</div><div class="line">  <span class="keyword">return</span> parsed_features[<span class="string">"image"</span>], parsed_features[<span class="string">"label"</span>]</div><div class="line"></div><div class="line"><span class="comment"># Creates a dataset that reads all of the examples from two files, and extracts</span></div><div class="line"><span class="comment"># the image and label features.</span></div><div class="line">filenames = [<span class="string">"/var/data/file1.tfrecord"</span>, <span class="string">"/var/data/file2.tfrecord"</span>]</div><div class="line">dataset = tf.contrib.data.TFRecordDataset(filenames)</div><div class="line">dataset = dataset.map(_parse_function)</div></pre></td></tr></table></figure></p>
<h3 id="解码图像、调整大小"><a href="#解码图像、调整大小" class="headerlink" title="解码图像、调整大小"></a>解码图像、调整大小</h3><p>利用真实世界的图像数据训练神经网络时，通常需要把不同大小的图像转换为常见的尺寸，这样可以组织为一批固定大小的数据。<br><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div></pre></td><td class="code"><pre><div class="line"><span class="comment"># Reads an image from a file, decodes it into a dense tensor, and resizes it</span></div><div class="line"><span class="comment"># to a fixed shape.</span></div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">_parse_function</span><span class="params">(filename, label)</span>:</span></div><div class="line">  image_string = tf.read_file(filename)</div><div class="line">  image_decoded = tf.image.decode_image(image_string)</div><div class="line">  image_resized = tf.image.resize_images(image_decoded, [<span class="number">28</span>, <span class="number">28</span>])</div><div class="line">  <span class="keyword">return</span> image_resized, label</div><div class="line"></div><div class="line"><span class="comment"># A vector of filenames.</span></div><div class="line">filenames = tf.constant([<span class="string">"/var/data/image1.jpg"</span>, <span class="string">"/var/data/image2.jpg"</span>, ...])</div><div class="line"></div><div class="line"><span class="comment"># `labels[i]` is the label for the image in `filenames[i].</span></div><div class="line">labels = tf.constant([<span class="number">0</span>, <span class="number">37</span>, ...])</div><div class="line"></div><div class="line">dataset = tf.contrib.data.Dataset.from_tensor_slices((filenames, labels))</div><div class="line">dataset = dataset.map(_parse_function)</div></pre></td></tr></table></figure></p>
<h3 id="使用tf-py-func-应用Python逻辑"><a href="#使用tf-py-func-应用Python逻辑" class="headerlink" title="使用tf.py_func()应用Python逻辑"></a>使用tf.py_func()应用Python逻辑</h3><p>为了提高性能，我们鼓励你使用TensorFlow中的操作进行数据预处理。但有时使用内置的Python库来解析输入数据也有效。<br><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">import</span> cv2</div><div class="line"></div><div class="line"><span class="comment"># Use a custom OpenCV function to read the image, instead of the standard</span></div><div class="line"><span class="comment"># TensorFlow `tf.read_file()` operation.</span></div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">_read_py_function</span><span class="params">(filename, label)</span>:</span></div><div class="line">  image_decoded = cv2.imread(image_string, cv2.IMREAD_GRAYSCALE)</div><div class="line">  <span class="keyword">return</span> image_decoded, label</div><div class="line"></div><div class="line"><span class="comment"># Use standard TensorFlow operations to resize the image to a fixed shape.</span></div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">_resize_function</span><span class="params">(image_decoded, label)</span>:</span></div><div class="line">  image_decoded.set_shape([<span class="keyword">None</span>, <span class="keyword">None</span>, <span class="keyword">None</span>])</div><div class="line">  image_resized = tf.image.resize_images(image_decoded, [<span class="number">28</span>, <span class="number">28</span>])</div><div class="line">  <span class="keyword">return</span> image_resized, label</div><div class="line"></div><div class="line">filenames = [<span class="string">"/var/data/image1.jpg"</span>, <span class="string">"/var/data/image2.jpg"</span>, ...]</div><div class="line">labels = [<span class="number">0</span>, <span class="number">37</span>, <span class="number">29</span>, <span class="number">1</span>, ...]</div><div class="line"></div><div class="line">dataset = tf.contrib.data.Dataset.from_tensor_slices((filenames, labels))</div><div class="line">dataset = dataset.map(</div><div class="line">    <span class="keyword">lambda</span> filename, label: tf.py_func(</div><div class="line">        _read_py_function, [filename, label], [tf.uint8, label.dtype]))</div><div class="line">dataset = dataset.map(_resize_function)</div></pre></td></tr></table></figure></p>
<h2 id="输出Batch"><a href="#输出Batch" class="headerlink" title="输出Batch"></a>输出Batch</h2><h3 id="简单的Batching"><a href="#简单的Batching" class="headerlink" title="简单的Batching"></a>简单的Batching</h3><p>最简单的形式是堆叠n个连续的数据集元素，组织成一个单一的元素。<code>Dataset.batch()</code>变换正是这么做的。和<code>tf.stack()</code>具有相同的限制条件，对每个元素、每个component的Tensor都必须有相同的形状。<br><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div></pre></td><td class="code"><pre><div class="line">inc_dataset = tf.contrib.data.Dataset.range(<span class="number">100</span>)</div><div class="line">dec_dataset = tf.contrib.data.Dataset.range(<span class="number">0</span>, <span class="number">-100</span>, <span class="number">-1</span>)</div><div class="line">dataset = tf.contrib.data.Dataset.zip((inc_dataset, dec_dataset))</div><div class="line">batched_dataset = dataset.batch(<span class="number">4</span>)</div><div class="line"></div><div class="line">iterator = batched_dataset.make_one_shot_iterator()</div><div class="line">next_element = iterator.get_next()</div><div class="line"></div><div class="line">print(sess.run(next_element))  <span class="comment"># ==&gt; ([0, 1, 2,   3],   [ 0, -1,  -2,  -3])</span></div><div class="line">print(sess.run(next_element))  <span class="comment"># ==&gt; ([4, 5, 6,   7],   [-4, -5,  -6,  -7])</span></div><div class="line">print(sess.run(next_element))  <span class="comment"># ==&gt; ([8, 9, 10, 11],   [-8, -9, -10, -11])</span></div></pre></td></tr></table></figure></p>
<h3 id="带边距的Batching-Tensors"><a href="#带边距的Batching-Tensors" class="headerlink" title="带边距的Batching Tensors"></a>带边距的Batching Tensors</h3><p>上面的方法可以处理相同形状的Tensor。但是很多模型的输入数据的大小不一（如长度不同的序列）。为了处理这个情况，<code>Dataset.padded_batch()</code>变换可以对某些维度指定边距，以处理不同形状的Tensor。<br><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div></pre></td><td class="code"><pre><div class="line">dataset = tf.contrib.data.Dataset.range(<span class="number">100</span>)</div><div class="line">dataset = dataset.map(<span class="keyword">lambda</span> x: tf.fill([tf.cast(x, tf.int32)], x))</div><div class="line">dataset = dataset.padded_batch(<span class="number">4</span>, padded_shapes=[<span class="keyword">None</span>])</div><div class="line"></div><div class="line">iterator = dataset.make_one_shot_iterator()</div><div class="line">next_element = iterator.get_next()</div><div class="line"></div><div class="line">print(sess.run(next_element))  <span class="comment"># ==&gt; [[0, 0, 0], [1, 0, 0], [2, 2, 0], [3, 3, 3]]</span></div><div class="line">print(sess.run(next_element))  <span class="comment"># ==&gt; [[4, 4, 4, 4, 0, 0, 0],</span></div><div class="line">                               <span class="comment">#      [5, 5, 5, 5, 5, 0, 0],</span></div><div class="line">                               <span class="comment">#      [6, 6, 6, 6, 6, 6, 0],</span></div><div class="line">                               <span class="comment">#      [7, 7, 7, 7, 7, 7, 7]]</span></div></pre></td></tr></table></figure></p>
<p><code>Dataset.padded_batch()</code>变换允许你对每一个component的每一维度设置不同的边距，边距也可以是可变的长度（设置为<code>None</code>，参考上面的例子），也可以为边距赋值，默认为0。</p>
<h2 id="训练流程"><a href="#训练流程" class="headerlink" title="训练流程"></a>训练流程</h2><h3 id="处理多轮epoch"><a href="#处理多轮epoch" class="headerlink" title="处理多轮epoch"></a>处理多轮epoch</h3><p><code>Dataset</code>API提供了两种处理多轮相同数据的方法。</p>
<p>迭代一个数据集多次最简单的方法是使用<code>Dataset.repeat()</code>变换。下面的例子是创建一个数据集来重复输入10个epochs：<br><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div></pre></td><td class="code"><pre><div class="line">filenames = [<span class="string">"/var/data/file1.tfrecord"</span>, <span class="string">"/var/data/file2.tfrecord"</span>]</div><div class="line">dataset = tf.contrib.data.TFRecordDataset(filenames)</div><div class="line">dataset = dataset.map(...)</div><div class="line">dataset = dataset.repeat(<span class="number">10</span>)</div><div class="line">dataset = dataset.batch(<span class="number">32</span>)</div></pre></td></tr></table></figure></p>
<p>不带参数的<code>Dataset.repeat()</code>会无限的重复输入。<code>Dataset.repeat()</code>变换不会提示一个epoch的结束或新的epoch的开始。</p>
<p>如果想接收epoch起始或结束的信号，可以写一个训练循环，在每个数据集结束时捕获<code>tf.errors.OutOfRangeError</code>，在每轮结束时可以计算一些统计量（如验证集错误率）<br><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div></pre></td><td class="code"><pre><div class="line">filenames = [<span class="string">"/var/data/file1.tfrecord"</span>, <span class="string">"/var/data/file2.tfrecord"</span>]</div><div class="line">dataset = tf.contrib.data.TFRecordDataset(filenames)</div><div class="line">dataset = dataset.map(...)</div><div class="line">dataset = dataset.batch(<span class="number">32</span>)</div><div class="line">iterator = dataset.make_initializable_iterator()</div><div class="line">next_element = iterator.get_next()</div><div class="line"></div><div class="line"><span class="comment"># Compute for 100 epochs.</span></div><div class="line"><span class="keyword">for</span> _ <span class="keyword">in</span> range(<span class="number">100</span>):</div><div class="line">  sess.run(iterator.initializer)</div><div class="line">  <span class="keyword">while</span> <span class="keyword">True</span>:</div><div class="line">    <span class="keyword">try</span>:</div><div class="line">      sess.run(next_element)</div><div class="line">    <span class="keyword">except</span> tf.errors.OutOfRangeError:</div><div class="line">      <span class="keyword">break</span></div><div class="line"></div><div class="line">  <span class="comment"># [Perform end-of-epoch calculations here.]</span></div></pre></td></tr></table></figure></p>
<h3 id="随机打乱数据"><a href="#随机打乱数据" class="headerlink" title="随机打乱数据"></a>随机打乱数据</h3><p><code>Dataset.shuffle()</code>变换使用类似<code>tf.RandomShuffleQueue</code>的算法来随机打乱输入数据：其内部有固定长度的缓冲区，并从该缓冲区中随机的选择下一个元素。<br><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div></pre></td><td class="code"><pre><div class="line">filenames = [<span class="string">"/var/data/file1.tfrecord"</span>, <span class="string">"/var/data/file2.tfrecord"</span>]</div><div class="line">dataset = tf.contrib.data.TFRecordDataset(filenames)</div><div class="line">dataset = dataset.map(...)</div><div class="line">dataset = dataset.shuffle(buffer_size=<span class="number">10000</span>)</div><div class="line">dataset = dataset.batch(<span class="number">32</span>)</div><div class="line">dataset = dataset.repeat()</div></pre></td></tr></table></figure></p>
<h3 id="使用高级API"><a href="#使用高级API" class="headerlink" title="使用高级API"></a>使用高级API</h3><p><code>tf.train.MonitoredTrainingSession</code>API在分布式的设置下在很多方面简化运行TensorFlow。<code>MonitoredTrainSession</code>使用<code>tf.errors.OutOfRangeError</code>来标志训练完成，如果和<code>Dataset</code>API一起用的话，推荐使用<code>Dataset.make_one_shot_iterator()</code>。<br><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div></pre></td><td class="code"><pre><div class="line">filenames = [<span class="string">"/var/data/file1.tfrecord"</span>, <span class="string">"/var/data/file2.tfrecord"</span>]</div><div class="line">dataset = tf.contrib.data.TFRecordDataset(filenames)</div><div class="line">dataset = dataset.map(...)</div><div class="line">dataset = dataset.shuffle(buffer_size=<span class="number">10000</span>)</div><div class="line">dataset = dataset.batch(<span class="number">32</span>)</div><div class="line">dataset = dataset.repeat(num_epochs)</div><div class="line">iterator = dataset.make_one_shot_iterator()</div><div class="line"></div><div class="line">next_example, next_label = iterator.get_next()</div><div class="line">loss = model_function(next_example, next_label)</div><div class="line"></div><div class="line">training_op = tf.train.AdagradOptimizer(...).minimize(loss)</div><div class="line"></div><div class="line"><span class="keyword">with</span> tf.train.MonitoredTrainingSession(...) <span class="keyword">as</span> sess:</div><div class="line">  <span class="keyword">while</span> <span class="keyword">not</span> sess.should_stop():</div><div class="line">    sess.run(training_op)</div></pre></td></tr></table></figure></p>
<p>如果把<code>Dataset</code>作为<code>tf.estimator.Estimator</code>的<code>input_fn</code>来使用，也推荐用<code>Dataset.make_one_shot_iterator</code><br><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div></pre></td><td class="code"><pre><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">dataset_input_fn</span><span class="params">()</span>:</span></div><div class="line">  filenames = [<span class="string">"/var/data/file1.tfrecord"</span>, <span class="string">"/var/data/file2.tfrecord"</span>]</div><div class="line">  dataset = tf.contrib.data.TFRecordDataset(filenames)</div><div class="line"></div><div class="line">  <span class="comment"># Use `tf.parse_single_example()` to extract data from a `tf.Example`</span></div><div class="line">  <span class="comment"># protocol buffer, and perform any additional per-record preprocessing.</span></div><div class="line">  <span class="function"><span class="keyword">def</span> <span class="title">parser</span><span class="params">(record)</span>:</span></div><div class="line">    keys_to_features = &#123;</div><div class="line">        <span class="string">"image_data"</span>: tf.FixedLenFeature((), tf.string, default_value=<span class="string">""</span>),</div><div class="line">        <span class="string">"date_time"</span>: tf.FixedLenFeature((), tf.int64, default_value=<span class="string">""</span>),</div><div class="line">        <span class="string">"label"</span>: tf.FixedLenFeature((), tf.int64,</div><div class="line">                                    default_value=tf.zeros([], dtype=tf.int64)),</div><div class="line">    &#125;</div><div class="line">    parsed = tf.parse_single_example(record, keys_to_features)</div><div class="line"></div><div class="line">    <span class="comment"># Perform additional preprocessing on the parsed data.</span></div><div class="line">    image = tf.decode_jpeg(parsed[<span class="string">"image_data"</span>])</div><div class="line">    image = tf.reshape(image, [<span class="number">299</span>, <span class="number">299</span>, <span class="number">1</span>])</div><div class="line">    label = tf.cast(parsed[<span class="string">"label"</span>], tf.int32)</div><div class="line"></div><div class="line">    <span class="keyword">return</span> &#123;<span class="string">"image_data"</span>: image, <span class="string">"date_time"</span>: parsed[<span class="string">"date_time"</span>]&#125;, label</div><div class="line"></div><div class="line">  <span class="comment"># Use `Dataset.map()` to build a pair of a feature dictionary and a label</span></div><div class="line">  <span class="comment"># tensor for each example.</span></div><div class="line">  dataset = dataset.map(parser)</div><div class="line">  dataset = dataset.shuffle(buffer_size=<span class="number">10000</span>)</div><div class="line">  dataset = dataset.batch(<span class="number">32</span>)</div><div class="line">  dataset = dataset.repeat(num_epochs)</div><div class="line">  iterator = dataset.make_one_shot_iterator()</div><div class="line"></div><div class="line">  <span class="comment"># `features` is a dictionary in which each value is a batch of values for</span></div><div class="line">  <span class="comment"># that feature; `labels` is a batch of labels.</span></div><div class="line">  features, labels = iterator.get_next()</div><div class="line">  <span class="keyword">return</span> features, labels</div></pre></td></tr></table></figure></p>

          
        
      
    </div>

    <div>
      
    </div>
	
	
	<div>
      
    </div>
	

    <div>
      
    </div>

    <div>
      
    </div>

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal " itemscope itemtype="http://schema.org/Article">
    <link itemprop="mainEntityOfPage" href="http://libowei.net/LaTeX数学公式符号.html">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="Albert Lee">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/image/学士服照片.jpg">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="博伟的博客">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
            
            
              
                
                <a class="post-title-link" href="/LaTeX数学公式符号.html" itemprop="url">
                  LaTeX数学公式符号
                </a>
              
            
          </h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="创建于" itemprop="dateCreated datePublished" datetime="2017-08-17T20:03:56+08:00">
                2017-08-17
              </time>
            

            

            
          </span>

          

          
            
          

          
          
             <span id="/LaTeX数学公式符号.html" class="leancloud_visitors" data-flag-title="LaTeX数学公式符号">
               <span class="post-meta-divider">|</span>
               <span class="post-meta-item-icon">
                 <i class="fa fa-eye"></i>
               </span>
               
                 <span class="post-meta-item-text">阅读次数 </span>
               
                 <span class="leancloud-visitors-count"></span>
             </span>
          

          

          

          

        </div>
      </header>
    


    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <h2 id="角标"><a href="#角标" class="headerlink" title="角标"></a>角标</h2><ul>
<li>上标 <code>^{}</code></li>
<li>下标 <code>_{}</code></li>
</ul>
<p>角标为单个字符时，可以不用<code>{}</code>；角标为多字符或多层次，必须使用<code>{}</code></p>
<p><code>x^2</code> $x^2$</p>
<p><code>x^2_1</code> $x^2_1$</p>
<p><code>x^{(n)}_{22}</code> $x^{(n)}_{22}$</p>
<p><code>^{16}O^{2-}_{32}</code>$^{16}O^{2-}_{32}$</p>
<p><code>x^{y_z}</code> $x^{y_z}$</p>
<h2 id="分式"><a href="#分式" class="headerlink" title="分式"></a>分式</h2><ul>
<li><code>\frac{分子}{分母}</code></li>
</ul>
<p><code>\frac{x+y}{y+z}</code> $\frac{x+y}{y+z}$</p>
<p><code>\displaystyle \frac{x+y}{y+z}</code> $\displaystyle \frac{x+y}{y+z}$</p>
<h2 id="根式"><a href="#根式" class="headerlink" title="根式"></a>根式</h2><ul>
<li>二次根式 <code>\sqrt{表达式}</code></li>
<li>n次根式 <code>\sqrt[n]{表达式}</code></li>
<li>根号上没有横线 <code>\surd{表达式}</code></li>
</ul>
<p>被开方表达式字符高度不一致时，为使横线在同一水平线上，插入<code>(\mathstruct)</code></p>
<ul>
<li>$\sqrt{a}+\sqrt{b}+\sqrt{c}$ <code>\sqrt{a}+\sqrt{b}+\sqrt{c}</code> </li>
<li>$\sqrt{\mathstrut a}+\sqrt{\mathstrut b}+\sqrt{\mathstrut c}$ <code>\sqrt{\mathstrut a}+\sqrt{\mathstrut b}+\sqrt{\mathstrut c}</code></li>
</ul>
<h2 id="求和、积分"><a href="#求和、积分" class="headerlink" title="求和、积分"></a>求和、积分</h2><ul>
<li>求和 <code>\sum_{k=1}^n求和项</code></li>
<li>积分 <code>\int_a^b积分项</code></li>
<li>无穷级数 <code>\infty</code></li>
</ul>
<p><code>\sum_{a=1}^n</code> $\sum_{a=1}^nx$</p>
<p><code>\int_1^{10}x</code> $\int_1^{10}x$</p>
<p><code>\sum_{k=1}^\infty \frac{x^n}{n!} =\int_0 ^\infty e^x</code> $\sum_{k=1}^\infty \frac{x^n}{n!} =\int_0 ^\infty e^x$</p>
<p>多行效果：$$\sum_{k=1}^\infty \frac{x^n}{n!} =\int_0 ^\infty e^x$$ </p>
<p>改变上下限位置：</p>
<ul>
<li>强制在上下侧<code>\limits</code></li>
<li>强制在左右侧<code>\nolimits</code></li>
</ul>
<p>行内在上下侧：<code>\sum\limits_{n=0}^\infty x^n</code> $\sum\limits_{n=0}^\infty x^n$</p>
<p>行间在左右侧：<code>$$\sum\nolimits_{n=0}^\infty x^n$$</code> $$\sum\nolimits_{n=0}^\infty x^n$$</p>
<h2 id="上下划线"><a href="#上下划线" class="headerlink" title="上下划线"></a>上下划线</h2><ul>
<li>上划线 <code>\overline</code></li>
<li>下划线 <code>\underline</code></li>
<li>上花括弧 <code>\overbrace</code></li>
<li>下花括弧 <code>\underbrace</code></li>
</ul>
<p><code>$\overline{\overline{a^2}+\underline{ab}+\bar{a}^3}$</code> $\overline{\overline{a^2}+\underline{ab}+\bar{a}^3}$</p>
<p><code>$\underbrace{a+\overbrace{b+\dots+b}^{m\mbox个}+c}_{20\mbox个}$</code> $\underbrace{a+\overbrace{b+\dots+b}^{m\mbox个}+c}_{20\mbox个}$</p>
<h2 id="数学符号"><a href="#数学符号" class="headerlink" title="数学符号"></a>数学符号</h2><ul>
<li><code>\leq</code> $\leq$</li>
<li><code>\geq</code> $\geq$</li>
<li><code>\sim</code> $\sim$</li>
<li><code>\approx</code> $\approx$</li>
<li><code>\in</code> $\in$</li>
<li><code>\notin</code> $\notin$</li>
<li><code>\neq</code> $\neq$</li>
<li><code>\ll</code> $\ll$</li>
<li><code>\gg</code> $\gg$</li>
</ul>
<h2 id="希腊字符"><a href="#希腊字符" class="headerlink" title="希腊字符"></a>希腊字符</h2><ul>
<li>$\alpha$ <code>\alpha</code></li>
<li>$\beta$ <code>\beta</code></li>
<li>$\gamma,\Gamma$ <code>\gamma,\Gamma</code></li>
<li>$\delta,\Delta$ <code>\delta,\Delta</code></li>
<li>$\epsilon,\varepsilon$ <code>\epsilon,\varepsilon</code></li>
<li>$\eta$ <code>\eta</code></li>
<li>$\theta,\Theta,\vartheta$ <code>\theta,\Theta,\vartheta</code></li>
<li>$\iota$ <code>\iota</code></li>
<li>$\lambda$ <code>\lambda</code></li>
<li>$\mu$ <code>\mu</code></li>
<li>$\phi,\Phi,\varphi$ <code>\phi,\Phi,\varphi</code></li>
<li>$\psi,\Psi$ <code>\psi,\Psi</code></li>
<li>$\omega,\Omega$ <code>\omega,\Omega</code></li>
</ul>

          
        
      
    </div>

    <div>
      
    </div>
	
	
	<div>
      
    </div>
	

    <div>
      
    </div>

    <div>
      
    </div>

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal " itemscope itemtype="http://schema.org/Article">
    <link itemprop="mainEntityOfPage" href="http://libowei.net/12306席别代码.html">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="Albert Lee">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/image/学士服照片.jpg">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="博伟的博客">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
            
            
              
                
                <a class="post-title-link" href="/12306席别代码.html" itemprop="url">
                  12306席别代码
                </a>
              
            
          </h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="创建于" itemprop="dateCreated datePublished" datetime="2017-08-14T16:35:39+08:00">
                2017-08-14
              </time>
            

            

            
          </span>

          

          
            
          

          
          
             <span id="/12306席别代码.html" class="leancloud_visitors" data-flag-title="12306席别代码">
               <span class="post-meta-divider">|</span>
               <span class="post-meta-item-icon">
                 <i class="fa fa-eye"></i>
               </span>
               
                 <span class="post-meta-item-text">阅读次数 </span>
               
                 <span class="leancloud-visitors-count"></span>
             </span>
          

          

          

          

        </div>
      </header>
    


    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <p>在12306查询车票时，可从返回的JSON字符串得到车次、始发站、终点站、时间、余票数量等信息。其中有一个字段，其包含的字符串常见的有<code>OM9</code>,<code>1413</code>,<code>OMO</code>等。</p>
<p>这个字段代表该车次发售车票的席别，即硬卧、软卧、二等座、一等座等。</p>
<p>字符串中每一个字符代表一种席别：</p>
<ul>
<li><strong><em>1</em></strong> 硬座</li>
<li><strong><em>2</em></strong> 软座</li>
<li><strong><em>3</em></strong> 硬卧</li>
<li><strong><em>4</em></strong> 软卧</li>
<li><strong><em>6</em></strong> 高级软卧</li>
<li><strong><em>9</em></strong> 商务座</li>
<li><strong><em>F</em></strong> 动卧</li>
<li><strong><em>M</em></strong> 一等座</li>
<li><strong><em>O</em></strong> 二等座</li>
<li><strong><em>P</em></strong> 特等座</li>
</ul>
<p><strong><em>无座</em></strong>的情况：若某个代码重复出现，则代表该席别出售无座票，如</p>
<ul>
<li><code>OMO</code> 表示二等座席位出售无座票</li>
<li><code>1413</code> 表示硬座席位出售无座票</li>
</ul>
<p>12306页面上还有一个席别为<strong><em>其他</em></strong>，可能的席位类型为软卧包厢，单人包厢等，可能的代码为<code>H</code>,<code>F</code>等（具体代表哪种席别我也没弄清楚）</p>

          
        
      
    </div>

    <div>
      
    </div>
	
	
	<div>
      
    </div>
	

    <div>
      
    </div>

    <div>
      
    </div>

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal " itemscope itemtype="http://schema.org/Article">
    <link itemprop="mainEntityOfPage" href="http://libowei.net/TensorBoard说明文档.html">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="Albert Lee">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/image/学士服照片.jpg">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="博伟的博客">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
            
            
              
                
                <a class="post-title-link" href="/TensorBoard说明文档.html" itemprop="url">
                  TensorBoard说明文档
                </a>
              
            
          </h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="创建于" itemprop="dateCreated datePublished" datetime="2017-07-26T20:19:10+08:00">
                2017-07-26
              </time>
            

            

            
          </span>

          

          
            
          

          
          
             <span id="/TensorBoard说明文档.html" class="leancloud_visitors" data-flag-title="TensorBoard说明文档">
               <span class="post-meta-divider">|</span>
               <span class="post-meta-item-icon">
                 <i class="fa fa-eye"></i>
               </span>
               
                 <span class="post-meta-item-text">阅读次数 </span>
               
                 <span class="leancloud-visitors-count"></span>
             </span>
          

          

          

          

        </div>
      </header>
    


    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <h2 id="TensorBoard"><a href="#TensorBoard" class="headerlink" title="TensorBoard"></a>TensorBoard</h2><p>TensorBoard是用来了解TensorFlow如何运行、展示图表的一套Web应用程序。</p>
<p>本文档介绍如何利用TensorBoard还嫌可视化，并给出TensorBoard的一些关键概念。更深入的例子请参见教程<a href="https://www.tensorflow.org/get_started/summaries_and_tensorboard" target="_blank" rel="external"> TensorBoard: Visualizing Learning</a>。要更深入了解图表可视化工具，请看教程<a href="https://www.tensorflow.org/get_started/graph_viz" target="_blank" rel="external">TensorBoard: Graph Visualization</a>。</p>
<p>还可以看这个<a href="https://www.youtube.com/watch?v=eBbEDRsCmv4" target="_blank" rel="external">视频教程</a>，了解如何安装和使用TensorBoard。</p>
<h2 id="使用方法"><a href="#使用方法" class="headerlink" title="使用方法"></a>使用方法</h2><p>在运行TensorBoard之前，确定已经生成并通过<code>summary writer</code>把训练摘要数据保存到文件夹中：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">file_writer = tf.summary.FileWriter(&apos;/path/to/logs&apos;, sess.graph)</div></pre></td></tr></table></figure></p>
<p>查看<a href="https://www.tensorflow.org/get_started/summaries_and_tensorboard" target="_blank" rel="external">TensorBoard tutorial</a>了解更多细节。当有了事件文件，指定其所在文件夹，就可以运行TensorBoard：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">tensorboard --logdir=path/to/logs</div></pre></td></tr></table></figure></p>
<p>执行上述命令后会显示TensorBoard已经启动的信息。现在可以访问<a href="http://localhost:6006" target="_blank" rel="external">http://localhost:6006</a>。</p>
<p>TensorBoard从<code>logdir</code>中读入日志数据。若要了解TensorBoard的配置等信息，运行<code>tensorboard --help</code></p>
<p>TensorBoard可以在Chrome和Firefox浏览器中使用，其他浏览器可能会出现错误或其他性能问题。</p>
<h2 id="关键概念"><a href="#关键概念" class="headerlink" title="关键概念"></a>关键概念</h2><h3 id="摘要运算：TensorBoard如何从TensorFlow中获取数据"><a href="#摘要运算：TensorBoard如何从TensorFlow中获取数据" class="headerlink" title="摘要运算：TensorBoard如何从TensorFlow中获取数据"></a>摘要运算：TensorBoard如何从TensorFlow中获取数据</h3><p>使用TensorBoard的第一步是从运行中的TensorFlow程序中获取数据，这需要摘要运算(summary ops)，摘要操作像<code>tf.matmul</code>或<code>tf.nn.relu</code>等运算操作一样：接收Tensor、产生Tensor，在TensorFlow Graph中求值。但摘要运算有所不同的是：这些Tensor包含serialized protobufs(我理解为可以持久化的一些东西)，可以写入磁盘并在TensorBoard中展示。为了在TensorBoard中对记录的摘要数据可视化，需要对摘要计算求值，得到其结果并将其以摘要的形式写入磁盘。对FileWriter的详细解释和例子在<a href="https://www.tensorflow.org/get_started/summaries_and_tensorboard" target="_blank" rel="external">教程</a>中。</p>
<p>支持的摘要计算包括：</p>
<ul>
<li>tf.summary.scalar</li>
<li>tf.summary.image</li>
<li>tf.summary.audio</li>
<li>tf.summary.text</li>
<li>tf.summary.histogram</li>
</ul>
<h3 id="标签：对数据取名字"><a href="#标签：对数据取名字" class="headerlink" title="标签：对数据取名字"></a>标签：对数据取名字</h3><p>进行摘要计算时，要对其取一个标签<code>tag</code>。这个标签是该计算所记录的数据的名字，而且用于在前端页面展示。Scalar和histogram面板通过标签来标记数据，通过层次化的标签名对数据分组。如果有很多标签的话，我们建议使用<code>/</code>对其分组。</p>
<h3 id="事件文件和日志文件夹：TensorBoard如何读取数据"><a href="#事件文件和日志文件夹：TensorBoard如何读取数据" class="headerlink" title="事件文件和日志文件夹：TensorBoard如何读取数据"></a>事件文件和日志文件夹：TensorBoard如何读取数据</h3><h3 id="Runs：对比模型不同的训练结果"><a href="#Runs：对比模型不同的训练结果" class="headerlink" title="Runs：对比模型不同的训练结果"></a>Runs：对比模型不同的训练结果</h3><h2 id="可视化"><a href="#可视化" class="headerlink" title="可视化"></a>可视化</h2><h3 id="数值面板（Scalar）"><a href="#数值面板（Scalar）" class="headerlink" title="数值面板（Scalar）"></a>数值面板（Scalar）</h3><h3 id="柱状图面板（Histogram）"><a href="#柱状图面板（Histogram）" class="headerlink" title="柱状图面板（Histogram）"></a>柱状图面板（Histogram）</h3><h3 id="统计分布面板（Distribution）"><a href="#统计分布面板（Distribution）" class="headerlink" title="统计分布面板（Distribution）"></a>统计分布面板（Distribution）</h3><h3 id="图像面板（Image）"><a href="#图像面板（Image）" class="headerlink" title="图像面板（Image）"></a>图像面板（Image）</h3><h3 id="声音面板（Audio）"><a href="#声音面板（Audio）" class="headerlink" title="声音面板（Audio）"></a>声音面板（Audio）</h3><h3 id="计算图展示（Graph-Explorer）"><a href="#计算图展示（Graph-Explorer）" class="headerlink" title="计算图展示（Graph Explorer）"></a>计算图展示（Graph Explorer）</h3><h3 id="Embedding-Projector"><a href="#Embedding-Projector" class="headerlink" title="Embedding Projector"></a>Embedding Projector</h3><h3 id="文本面板（Text）"><a href="#文本面板（Text）" class="headerlink" title="文本面板（Text）"></a>文本面板（Text）</h3><h2 id="常见问题"><a href="#常见问题" class="headerlink" title="常见问题"></a>常见问题</h2><h3 id="TensorBoard不显示任何数据"><a href="#TensorBoard不显示任何数据" class="headerlink" title="TensorBoard不显示任何数据"></a>TensorBoard不显示任何数据</h3><h3 id="TensorBoard只显示一部分数据"><a href="#TensorBoard只显示一部分数据" class="headerlink" title="TensorBoard只显示一部分数据"></a>TensorBoard只显示一部分数据</h3><h3 id="TensorBoard支持多线程或分布式的summary-writer吗？"><a href="#TensorBoard支持多线程或分布式的summary-writer吗？" class="headerlink" title="TensorBoard支持多线程或分布式的summary writer吗？"></a>TensorBoard支持多线程或分布式的summary writer吗？</h3><h3 id="数据重叠在一起"><a href="#数据重叠在一起" class="headerlink" title="数据重叠在一起"></a>数据重叠在一起</h3><h3 id="如何处理ensorFlow程序重新启动的问题"><a href="#如何处理ensorFlow程序重新启动的问题" class="headerlink" title="如何处理ensorFlow程序重新启动的问题"></a>如何处理ensorFlow程序重新启动的问题</h3><h3 id="如何从TensorBoard导出数据"><a href="#如何从TensorBoard导出数据" class="headerlink" title="如何从TensorBoard导出数据"></a>如何从TensorBoard导出数据</h3><h3 id="可以重叠多个图表吗"><a href="#可以重叠多个图表吗" class="headerlink" title="可以重叠多个图表吗"></a>可以重叠多个图表吗</h3><h3 id="可以自己生成散点图吗（或其他自定义图表）"><a href="#可以自己生成散点图吗（或其他自定义图表）" class="headerlink" title="可以自己生成散点图吗（或其他自定义图表）"></a>可以自己生成散点图吗（或其他自定义图表）</h3>
          
        
      
    </div>

    <div>
      
    </div>
	
	
	<div>
      
    </div>
	

    <div>
      
    </div>

    <div>
      
    </div>

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal " itemscope itemtype="http://schema.org/Article">
    <link itemprop="mainEntityOfPage" href="http://libowei.net/TensorFlow-CNN-相关API.html">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="Albert Lee">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/image/学士服照片.jpg">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="博伟的博客">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
            
            
              
                
                <a class="post-title-link" href="/TensorFlow-CNN-相关API.html" itemprop="url">
                  TensorFlow CNN 相关API
                </a>
              
            
          </h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="创建于" itemprop="dateCreated datePublished" datetime="2017-07-24T21:03:47+08:00">
                2017-07-24
              </time>
            

            

            
          </span>

          

          
            
          

          
          
             <span id="/TensorFlow-CNN-相关API.html" class="leancloud_visitors" data-flag-title="TensorFlow CNN 相关API">
               <span class="post-meta-divider">|</span>
               <span class="post-meta-item-icon">
                 <i class="fa fa-eye"></i>
               </span>
               
                 <span class="post-meta-item-text">阅读次数 </span>
               
                 <span class="leancloud-visitors-count"></span>
             </span>
          

          

          

          

        </div>
      </header>
    


    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <h2 id="tf-nn-conv2d"><a href="#tf-nn-conv2d" class="headerlink" title="tf.nn.conv2d"></a><code>tf.nn.conv2d</code></h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div></pre></td><td class="code"><pre><div class="line">conv2d(</div><div class="line">    input,</div><div class="line">    filter,</div><div class="line">    strides,</div><div class="line">    padding,</div><div class="line">    use_cudnn_on_gpu=None,</div><div class="line">    data_format=None,</div><div class="line">    name=None</div><div class="line">)</div></pre></td></tr></table></figure>
<p>给定4维的输入和4维的过滤器Tensor，进行2卷积运算。</p>
<p>输入Tensor的形状为[batch,height,width,channels]，过滤器张量的形状为[filter_height, filter_width, in_channels, out_channels]</p>
<ol>
<li>把过滤器转化为2-D，其形状为<code>[filter_height * filter_width * in_channels, output_channels]</code></li>
<li>在图像中提取形状为<code>[batch, out_height, out_width, filter_height * filter_width * in_channels]</code>的Tensor</li>
<li>图像块向量和过滤器矩阵相乘</li>
</ol>
<p>步长参数strides中，必须满足<code>strides[0]=strides[3]=1</code>。大多数情况下水平步长和垂直步长相等。</p>
<p>参数：</p>
<ul>
<li><code>input</code> Tensor。维度的顺序由<code>data-format</code>确定。</li>
<li><code>filter</code> Tensor。与<code>input</code>数据类型相同。形状是<code>[filter_height, filter_width, in_channels, out_channels]</code></li>
<li><code>strides</code> 整数列表（长度为4的1-D Tensor）。每个维度滑动窗口的步长，维度的顺序由<code>data-format</code>确定。</li>
<li><code>padding</code> 字符串：<code>&quot;SAME&quot;</code>或<code>&quot;VALID&quot;</code>。<code>&quot;SAME&quot;</code>是对输入的首尾补0，以满足每个滑动窗口的大小。<code>&quot;VALID&quot;</code>是丢弃末尾的数据。</li>
<li><code>use_cudnn_on_gpu</code> 布尔值，默认为<code>True</code>。</li>
<li><code>data_format</code> 可选<code>&quot;NHWC&quot;</code>或<code>&quot;NCHW&quot;</code>，默认为<code>&quot;NHWC&quot;</code></li>
<li><code>name</code> 为这个操作取一个名字。</li>
</ul>
<h2 id="tf-nn-bias-add"><a href="#tf-nn-bias-add" class="headerlink" title="tf.nn.bias_add"></a><code>tf.nn.bias_add</code></h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div></pre></td><td class="code"><pre><div class="line">bias_add(</div><div class="line">    value,</div><div class="line">    bias,</div><div class="line">    data_format=None,</div><div class="line">    name=None</div><div class="line">)</div></pre></td></tr></table></figure>
<p>这个函数的作用是将偏差项<code>bias</code>加到<code>value</code>上面。</p>
<p>这个操作是<code>tf.add</code>的一个特例。其中<code>bias</code>必须为1维，而<code>value</code>可以为任意维度。与<code>tf.add</code>不同的是，数据被量化的情况下，<code>value</code>和<code>bias</code>的类型可以不同。</p>
<p>参数：</p>
<ul>
<li><code>value</code> Tensor。</li>
<li><code>bias</code> 1维的Tensor。大小与<code>value</code>最后一维相同。类型也须与<code>value</code>相同，除非<code>value</code>被量化。</li>
<li><code>data_format</code> <code>&quot;NHWC&quot;</code>或<code>&quot;NCHW&quot;</code>。</li>
<li><code>name</code> 为这个操作取一个名字。</li>
</ul>
<h2 id="tf-nn-relu"><a href="#tf-nn-relu" class="headerlink" title="tf.nn.relu"></a><code>tf.nn.relu</code></h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div></pre></td><td class="code"><pre><div class="line">relu(</div><div class="line">    features,</div><div class="line">    name=None</div><div class="line">)</div></pre></td></tr></table></figure>
<p>计算非线性激活函数<code>max(features,0)</code></p>
<p>参数:</p>
<ul>
<li><code>features</code> Tensor。</li>
<li><code>name</code> 为这个操作取个名字</li>
</ul>
<h2 id="tf-nn-max-pool"><a href="#tf-nn-max-pool" class="headerlink" title="tf.nn.max_pool"></a><code>tf.nn.max_pool</code></h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div></pre></td><td class="code"><pre><div class="line">    value,</div><div class="line">    ksize,</div><div class="line">    strides,</div><div class="line">    padding,</div><div class="line">    data_format=&apos;NHWC&apos;,</div><div class="line">    name=None</div><div class="line">)</div></pre></td></tr></table></figure>
<p>对输入数据进行最大池化</p>
<p>参数：</p>
<ul>
<li><code>value</code> 4维Tensor，形状为<code>[batch, height, width, channels]</code>，类型为<code>tf.float32</code></li>
<li><code>ksize</code> 长度&gt;=4的整形列表。每个维度的窗口大小。</li>
<li><code>strides</code> 长度&gt;=4的整形列表。每个维度的滑动步长。</li>
<li><code>padding</code> <code>&quot;SAME&quot;</code>或<code>&quot;VALID&quot;</code>。输出形状为：<code>output_height/output_width = (height/width - pool_size) stride + 1</code>，<code>&quot;SAME&quot;</code>为向上取整，<code>&quot;VALID&quot;</code>为向下取整</li>
</ul>
<h2 id="tf-nn-lrn"><a href="#tf-nn-lrn" class="headerlink" title="tf.nn.lrn"></a><code>tf.nn.lrn</code></h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div></pre></td><td class="code"><pre><div class="line">lrn(</div><div class="line">    input, </div><div class="line">    depth_radius=None, </div><div class="line">    bias=None, </div><div class="line">    alpha=None, </div><div class="line">    beta=None,</div><div class="line">    name=None</div><div class="line">)</div></pre></td></tr></table></figure>
<p>局部响应归一化</p>
<p>4维的<code>input</code>Tensor可以看做3维的1维向量（沿着最后一维），对每个向量归一化。<br>计算公式为：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">sqr_sum[a, b, c, d] = sum(input[a, b, c, d - depth_radius : d + depth_radius + 1] ** 2)</div><div class="line">output = input / (bias + alpha * sqr_sum) ** beta</div></pre></td></tr></table></figure></p>
<p>参数:</p>
<ul>
<li><code>input</code> Tensor</li>
<li><code>depth_radius</code> 默认为5</li>
<li><code>bias</code> 默认为1.0</li>
<li><code>alpha</code> 默认为1.0</li>
<li><code>beta</code> 默认为0.5，指数项</li>
<li><code>name</code> 为这个操作取个名字</li>
</ul>
<h2 id="tf-nn-sparse-softmax-cross-entropy-with-logits"><a href="#tf-nn-sparse-softmax-cross-entropy-with-logits" class="headerlink" title="tf.nn.sparse_softmax_cross_entropy_with_logits"></a><code>tf.nn.sparse_softmax_cross_entropy_with_logits</code></h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div></pre></td><td class="code"><pre><div class="line">sparse_softmax_cross_entropy_with_logits(</div><div class="line">    _sentinel=None,</div><div class="line">    labels=None,</div><div class="line">    logits=None,</div><div class="line">    name=None</div><div class="line">)</div></pre></td></tr></table></figure>
<p>计算<code>logists</code>和<code>labels</code>的稀疏softmax交叉熵。</p>
<p>用于度量互斥的离散分类任务（每个样本只属于一类）的概率误差，如CIFAR-10图像，每个图像只属于一类。</p>
<p>注意：这个操作接受未处理的输入，为了提高效率，函数内部对<code>logits</code>做了softmax运算。所以不要输入softmax后的数值，这会产生不正确的结果。</p>
<p>常见的情况，<code>logits</code>的形状为<code>[batch_size, num_classes]</code>，<code>labels</code>为<code>[batch_size]</code>。更高维度的也支持。</p>
<p>为了避免混淆，传递参数时需要带上参数名（如<code>sparse_softmax_cross_entropy_with_logits(logits=logits,labels=labels)</code></p>
<p>参数：</p>
<ul>
<li><code>labels</code> 形状为<code>[d_0, d_1, ..., d_{r-1}]</code>的Tensor（<code>r</code>为<code>label</code>的rank）。<code>labels</code>取值必须为<code>[0, num_classes)</code></li>
<li><code>logits</code> 对数概率(?)，形状为<code>[d_0, d_1, ..., d_{r-1}, num_classes]</code></li>
<li><code>name</code> 为操作取一个名字</li>
</ul>
<h2 id="tf-train-exponential-decay"><a href="#tf-train-exponential-decay" class="headerlink" title="tf.train.exponential_decay"></a><code>tf.train.exponential_decay</code></h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div></pre></td><td class="code"><pre><div class="line">exponential_decay(</div><div class="line">    learning_rate, </div><div class="line">    global_step, </div><div class="line">    decay_steps, </div><div class="line">    decay_rate,</div><div class="line">    staircase=False, </div><div class="line">    name=None</div><div class="line">)</div></pre></td></tr></table></figure>
<p>对学习率应用指数衰减。</p>
<p>训练模型时，随着训练的进行，逐渐减小学习率是好的做法。这个函数对初始的学习率应用指数衰减方法。需要<code>global_step</code>作为参数计算衰减后的学习率。可以传递一个TensorFlow变量，每次训练对其加一。</p>
<p>函数计算方法：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">ecayed_learning_rate = learning_rate * decay_rate ^ (global_step / decay_steps)</div></pre></td></tr></table></figure></p>
<p>若<code>staircase</code>为<code>True</code>，<code>global_step / decay_steps</code>计算结果为整数，此时学习率呈阶梯状下降。</p>
<p>参数:</p>
<ul>
<li><code>learning_rate</code> 标量，初始学习率</li>
<li><code>global_step</code> 标量（整数），不能为负</li>
<li><code>decay_steps</code> 标量（整数），必须为正</li>
<li><code>decay_rate</code> 标量，衰减率</li>
<li><code>staircase</code> 布尔。</li>
</ul>
<h2 id="tf-control-dependencies"><a href="#tf-control-dependencies" class="headerlink" title="tf.control_dependencies"></a><code>tf.control_dependencies</code></h2><p>控制计算顺序</p>
<h2 id="tf-train-ExponentialMovingAverage"><a href="#tf-train-ExponentialMovingAverage" class="headerlink" title="tf.train.ExponentialMovingAverage"></a><code>tf.train.ExponentialMovingAverage</code></h2><p><a href="https://www.tensorflow.org/versions/r1.3/api_docs/python/tf/train/ExponentialMovingAverage" target="_blank" rel="external">tf.train.ExponentialMovingAverage</a></p>

          
        
      
    </div>

    <div>
      
    </div>
	
	
	<div>
      
    </div>
	

    <div>
      
    </div>

    <div>
      
    </div>

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal " itemscope itemtype="http://schema.org/Article">
    <link itemprop="mainEntityOfPage" href="http://libowei.net/CentOS安装TensorFlow.html">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="Albert Lee">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/image/学士服照片.jpg">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="博伟的博客">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
            
            
              
                
                <a class="post-title-link" href="/CentOS安装TensorFlow.html" itemprop="url">
                  CentOS安装TensorFlow
                </a>
              
            
          </h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="创建于" itemprop="dateCreated datePublished" datetime="2017-07-21T10:21:26+08:00">
                2017-07-21
              </time>
            

            

            
          </span>

          

          
            
          

          
          
             <span id="/CentOS安装TensorFlow.html" class="leancloud_visitors" data-flag-title="CentOS安装TensorFlow">
               <span class="post-meta-divider">|</span>
               <span class="post-meta-item-icon">
                 <i class="fa fa-eye"></i>
               </span>
               
                 <span class="post-meta-item-text">阅读次数 </span>
               
                 <span class="leancloud-visitors-count"></span>
             </span>
          

          

          

          

        </div>
      </header>
    


    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <p>在CentOS6上安装TensorFlow1.2后，<code>import tensorflow</code>时出现以下问题：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">ImportError: /lib64/libc.so.6: version `GLIBC_2.17&apos; not found</div></pre></td></tr></table></figure></p>
<p>这个错误的原因是未安装2.17版本的glibc库。</p>
<p>而在CentOS上，使用<code>yum install glibc</code>命令，只能更新到2.12版本。需要手动下载编译安装。</p>
<p>glibc-2.17下载地址：<a href="https://ftp.gnu.org/gnu/glibc/glibc-2.17.tar.gz" target="_blank" rel="external">https://ftp.gnu.org/gnu/glibc/glibc-2.17.tar.gz</a></p>
<p>下载glibc并解压缩<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">wget https://ftp.gnu.org/gnu/glibc/glibc-2.17.tar.gz</div><div class="line">tar -xvf glibc-2.17.tar.gz</div></pre></td></tr></table></figure></p>
<p>编译安装<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div></pre></td><td class="code"><pre><div class="line">cd glibc-2.17</div><div class="line">mkdir build</div><div class="line">cd build</div><div class="line">../configure --prefix=/usr --disable-profile --enable-add-ons --with-headers=/usr/include --with-binutils=/usr/bin</div><div class="line">make &amp;&amp; make install</div></pre></td></tr></table></figure></p>
<p>查看glibc共享库：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">ll /lib64/libc.so.6</div></pre></td></tr></table></figure></p>
<p>现<code>libc.so.6</code>已经软链接到2.17版本<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">lrwxrwxrwx 1 root root 12 7月  21 10:11 /lib64/libc.so.6 -&gt; libc-2.17.so</div></pre></td></tr></table></figure></p>
<p>可以查看系统中可使用的glibc版本<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">strings /lib64/libc.so.6 |grep GLIBC_</div></pre></td></tr></table></figure></p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div></pre></td><td class="code"><pre><div class="line">GLIBC_2.2.5</div><div class="line">GLIBC_2.2.6</div><div class="line">GLIBC_2.3</div><div class="line">GLIBC_2.3.2</div><div class="line">GLIBC_2.3.3</div><div class="line">GLIBC_2.3.4</div><div class="line">GLIBC_2.4</div><div class="line">GLIBC_2.5</div><div class="line">GLIBC_2.6</div><div class="line">GLIBC_2.7</div><div class="line">GLIBC_2.8</div><div class="line">GLIBC_2.9</div><div class="line">GLIBC_2.10</div><div class="line">GLIBC_2.11</div><div class="line">GLIBC_2.12</div><div class="line">GLIBC_2.13</div><div class="line">GLIBC_2.14</div><div class="line">GLIBC_2.15</div><div class="line">GLIBC_2.16</div><div class="line">GLIBC_2.17</div><div class="line">GLIBC_PRIVATE</div></pre></td></tr></table></figure>
<p>现在应该就没问题了。</p>
<p>参考资料：<a href="http://blog.csdn.net/officercat/article/details/39520227" target="_blank" rel="external">Linux/CentOS 升级C基本运行库CLIBC的注意事项</a></p>

          
        
      
    </div>

    <div>
      
    </div>
	
	
	<div>
      
    </div>
	

    <div>
      
    </div>

    <div>
      
    </div>

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal " itemscope itemtype="http://schema.org/Article">
    <link itemprop="mainEntityOfPage" href="http://libowei.net/TensorFlow学习笔记：CNN-CIFAR-10.html">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="Albert Lee">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/image/学士服照片.jpg">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="博伟的博客">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
            
            
              
                
                <a class="post-title-link" href="/TensorFlow学习笔记：CNN-CIFAR-10.html" itemprop="url">
                  TensorFlow学习笔记：CIFAR-10 CNN
                </a>
              
            
          </h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="创建于" itemprop="dateCreated datePublished" datetime="2017-07-19T19:23:41+08:00">
                2017-07-19
              </time>
            

            

            
          </span>

          

          
            
          

          
          
             <span id="/TensorFlow学习笔记：CNN-CIFAR-10.html" class="leancloud_visitors" data-flag-title="TensorFlow学习笔记：CIFAR-10 CNN">
               <span class="post-meta-divider">|</span>
               <span class="post-meta-item-icon">
                 <i class="fa fa-eye"></i>
               </span>
               
                 <span class="post-meta-item-text">阅读次数 </span>
               
                 <span class="leancloud-visitors-count"></span>
             </span>
          

          

          

          

        </div>
      </header>
    


    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <p>官方文档地址：<a href="https://www.tensorflow.org/tutorials/deep_cnn" target="_blank" rel="external">Convolutional Neural Networks</a></p>
<h2 id="概述"><a href="#概述" class="headerlink" title="概述"></a>概述</h2><p>CIFAR-10分类是机器学习中常见的标准问题。CIFAR-10分类目标是把32*32像素的RGB图像分为10类<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">airplane, automobile, bird, cat, deer, dog, frog, horse, ship, and truck.</div></pre></td></tr></table></figure></p>
<h3 id="目标"><a href="#目标" class="headerlink" title="目标"></a>目标</h3><p>建立一个小型的CNN用于图像识别，做到：</p>
<ol>
<li>规范的组织神经网络的结构、训练和评估</li>
<li>为构建更大、更复杂的模型提供模板</li>
</ol>
<p>选用CIFAR-10数据的原因，一方面它足够复杂，能符合TensorFlow处理大模型的能力；另一方面它数据量较小，训练迅速，可以用来做测试和实验。</p>
<h3 id="重点"><a href="#重点" class="headerlink" title="重点"></a>重点</h3><p>CIFAR-10教程示范了一些重要的结构，可以用来在TensorFlow种设计大型的复杂的模型</p>
<ul>
<li>核心计算部分：卷积（convolution）, 修正线性激活（rectified linear activation）, 最大池化（max pooling）, 局部响应归一化（local response normalization）</li>
<li>神经网络训练时行为的可视化，包括图像输入，损失，神经网络行为的分布和梯度等。</li>
<li>计算学到的参数的moving avearage，并在评估中使用以提高预测效果</li>
<li>实现学习率随时间增加而减少</li>
<li>预取队列：将磁盘延迟和高代价的图像预处理与模型分开</li>
</ul>
<p>提供了多GPU训练的版本，实现了：</p>
<ul>
<li>在多个GPU卡间并行训练</li>
<li>在多个GPU间共享变量、更新变量值</li>
</ul>
<h3 id="结构"><a href="#结构" class="headerlink" title="结构"></a>结构</h3><p>该模型是由卷积层和非线性层组成的多层的结构。这些层之后连接全连接层，最后是Softmax分类器。模型结构大致和Alex Krizhevsky提出的模型一致，前面几层略有不同。</p>
<p>这个模型在单个GPU上训练若干小时后，就能达到非常好的效果：86%正确率。模型由1068298个可学习的参数组成，对一个图像分类需要19.5M次乘加操作。</p>
<h2 id="代码组织"><a href="#代码组织" class="headerlink" title="代码组织"></a>代码组织</h2><p>代码在<a href="https://www.tensorflow.org/code/tensorflow_models/tutorials/image/cifar10/" target="_blank" rel="external"><code>models/tutorials/image/cifar10/</code></a></p>
<ul>
<li><code>cifar10_input.py</code> 读原始的二进制CIFAR-10数据</li>
<li><code>cifar10.py</code> 建立模型</li>
<li><code>cifar10_train.py</code> 在CPU或GPU上训练模型</li>
<li><code>cifar10_multi_gpu_train.py</code> 在多GPU环境中训练模型</li>
<li><code>cifar10_eval.py</code> 评估模型</li>
</ul>
<h2 id="CIFAR-10模型"><a href="#CIFAR-10模型" class="headerlink" title="CIFAR-10模型"></a>CIFAR-10模型</h2><p>神经网络模型代码在<code>cifar10.py</code>中。全部的训练图包括765个操作。建立下面的模块，编写重用性高的图结构代码：</p>
<ol>
<li>模型输入：<code>inputs()</code>和<code>disorted_inputs()</code>，为训练和评估读入、预处理图像数据</li>
<li>模型预测：<code>inference()</code> 对提供的图片进行分类</li>
<li>模型训练：<code>loss()</code>和<code>train()</code>，计算损失、梯度、更新变量、结果可视化</li>
</ol>
<h3 id="模型输入"><a href="#模型输入" class="headerlink" title="模型输入"></a>模型输入</h3><p>输入模块由<code>inputs()</code>和<code>distorted_inputs()</code>构成，两个函数读入CIFAR-10二进制数据，文件由固定字节长度的记录组成，所以使用<code>tf.FixedLengthRecordReader</code></p>
<p>图像被处理成：</p>
<ul>
<li>裁切为24*24像素，评估裁剪中间部分，训练时随机</li>
<li>近似白化处理，使模型对图片动态的范围变化不敏感</li>
</ul>
<p>从磁盘中读图片需要相当长的处理时间，为避免其使训练时间变长，使用16个独立的线程读图片来填充TenorFlow队列。</p>
<h3 id="模型预测"><a href="#模型预测" class="headerlink" title="模型预测"></a>模型预测</h3><p>预测部分的代码在<code>inference()</code>中，计算预测的得分（logits），这部分的代码组织如下：</p>
<ul>
<li><code>conv1</code> 卷积、修正线性激活（rectified linear activation）</li>
<li><code>pool1</code> 最大池化</li>
<li><code>norm1</code> 局部响应归一化</li>
<li><code>conv2</code> 卷积、修正线性激活</li>
<li><code>norm2</code> 局部响应归一化</li>
<li><code>pool2</code> 最大池化</li>
<li><code>local3</code> 带有“修正线性激活的”的全连接</li>
<li><code>local4</code> 带有“修正线性激活的”的全连接</li>
<li><code>softmax_linear</code> 线性变换，输出结果</li>
</ul>
<p>下图由TensorBoard生成，展示预测部分的操作</p>
<p><img src="https://www.tensorflow.org/images/cifar_graph.png" alt=""></p>
<blockquote>
<p>练习：<code>inference</code>的输出是未归一化的logits，尝试使用<code>tf.nn.softmax</code>修改网络结构，使其返回归一化的预测结果</p>
<p>练习：<code>inference</code>中的模型结构和<a href="https://code.google.com/p/cuda-convnet/" target="_blank" rel="external">cuda-convnet</a>中的CIFAR-10模型有些许的不同。其中，在Alex的原始模型中，最上几层是局部连接而非全连接。尝试修改网络结构，在最上层形成局部连接的结构。</p>
</blockquote>
<h3 id="模型训练"><a href="#模型训练" class="headerlink" title="模型训练"></a>模型训练</h3><p>训练多分类网络常用的方法是多项式逻辑回归，如softmax regression。Softmax回归对结果应用非线性的softmax，计算归一化的预测结果与one-hot编码标签的交叉熵。为了正则化，使学习的变量的权重逐渐减小。模型的目标函数是交叉熵损失和，和权重衰减项的和，在<code>loss()</code>函数中返回。</p>
<p>应用<code>tf.summary.scalar</code>，在TensorBoard中将这一过程展示：<br><img src="https://www.tensorflow.org/images/cifar_loss.png" alt=""></p>
<p>使用标准的梯度下降算法来训练，学习率随时间变化呈指数衰减：<br><img src="https://www.tensorflow.org/images/cifar_lr_decay.png" alt=""></p>
<p><code>train()</code>中，通过计算梯度、更新学习变量（<code>tf.train.GradientDescentOptimizer</code>），使目标函数最小化。<code>train()</code>返回一个操作，这个操作中执行一批图像的计算，以训练和更新模型。</p>
<h2 id="启动和训练模型"><a href="#启动和训练模型" class="headerlink" title="启动和训练模型"></a>启动和训练模型</h2><p>让训练跑起来：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">python cifar10_train.py</div></pre></td></tr></table></figure></p>
<p>结果如下：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div></pre></td><td class="code"><pre><div class="line">Filling queue with 20000 CIFAR images before starting to train. This will take a few minutes.</div><div class="line">2015-11-04 11:45:45.927302: step 0, loss = 4.68 (2.0 examples/sec; 64.221 sec/batch)</div><div class="line">2015-11-04 11:45:49.133065: step 10, loss = 4.66 (533.8 examples/sec; 0.240 sec/batch)</div><div class="line">2015-11-04 11:45:51.397710: step 20, loss = 4.64 (597.4 examples/sec; 0.214 sec/batch)</div><div class="line">2015-11-04 11:45:54.446850: step 30, loss = 4.62 (391.0 examples/sec; 0.327 sec/batch)</div><div class="line">2015-11-04 11:45:57.152676: step 40, loss = 4.61 (430.2 examples/sec; 0.298 sec/batch)</div><div class="line">2015-11-04 11:46:00.437717: step 50, loss = 4.59 (406.4 examples/sec; 0.315 sec/batch)</div><div class="line">...</div></pre></td></tr></table></figure></p>
<p>每10步显示一次损失、训练速度。下面是一些提示：</p>
<ul>
<li>第一批(batch)数据训练很慢，因为需要预处理线程读取20000张CIFAR图像，打乱顺序加入队列。</li>
<li>显示的损失是最近一批数据的平均损失，这个损失是交叉熵与权重衰减项之和</li>
<li>在Tesla K40c上得到显示的训练速度，如果用CPU训练，速度较慢。</li>
</ul>
<blockquote>
<p>练习：在实验时，第一步训练耗时太长。尝试减少填入队列的图像的数目。在<code>cifar10_input.py</code>中查找<code>min_fraction_of_examples_in_queue</code></p>
</blockquote>
<p><code>cifar10_train.py</code>周期性的以checkpoints files保存模型参数，但不评估模型。Checkpoints会在<code>cifar10_eval.py</code>中使用，用来预测模型性能。</p>
<p>如果读完了前面的步骤，现在可以训练CIFAR-10模型了。Congratulations!</p>
<p><code>cifar10_train.py</code>返回的文字中包含少量的模型训练的信息。我们需要更多的训练时的信息，包括：</p>
<ul>
<li>损失是真实的在减小，还是只是噪声？</li>
<li>为模型提供的图片是否合适？</li>
<li>梯度、激活值、权重是否合理？</li>
<li>学习率是多少</li>
</ul>
<p>TensorBoard提供了这些功能。在<code>cifar10_train.py</code>中，通过<code>tf.summary.FileWriter</code>周期性的显示这些数据。</p>
<p>例如，可以观察激活值的分布和特征的稀疏程度：<br><img src="https://www.tensorflow.org/images/cifar_sparsity.png" alt=""><br><img src="https://www.tensorflow.org/images/cifar_activations.png" alt=""></p>

          
        
      
    </div>

    <div>
      
    </div>
	
	
	<div>
      
    </div>
	

    <div>
      
    </div>

    <div>
      
    </div>

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal " itemscope itemtype="http://schema.org/Article">
    <link itemprop="mainEntityOfPage" href="http://libowei.net/TensorFlow学习笔记：CNN-MNIST.html">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="Albert Lee">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/image/学士服照片.jpg">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="博伟的博客">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
            
            
              
                
                <a class="post-title-link" href="/TensorFlow学习笔记：CNN-MNIST.html" itemprop="url">
                  TensorFlow学习笔记：CNN MNIST
                </a>
              
            
          </h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="创建于" itemprop="dateCreated datePublished" datetime="2017-07-18T19:17:12+08:00">
                2017-07-18
              </time>
            

            

            
          </span>

          

          
            
          

          
          
             <span id="/TensorFlow学习笔记：CNN-MNIST.html" class="leancloud_visitors" data-flag-title="TensorFlow学习笔记：CNN MNIST">
               <span class="post-meta-divider">|</span>
               <span class="post-meta-item-icon">
                 <i class="fa fa-eye"></i>
               </span>
               
                 <span class="post-meta-item-text">阅读次数 </span>
               
                 <span class="leancloud-visitors-count"></span>
             </span>
          

          

          

          

        </div>
      </header>
    


    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <p>官方文档地址：<a href="https://www.tensorflow.org/tutorials/layers" target="_blank" rel="external">A Guide to TF Layers: Building a Convolutional Neural Network</a></p>
<h2 id="卷积神经网络"><a href="#卷积神经网络" class="headerlink" title="卷积神经网络"></a>卷积神经网络</h2><p>卷积神经网络是图片识别任务最先进的模型。</p>
<p>CNN应用一系列filters在原始像素数据上，提取学习高级特征，模型使用这些特征来分类。</p>
<h3 id="CNN组成"><a href="#CNN组成" class="headerlink" title="CNN组成"></a>CNN组成</h3><ol>
<li>卷积层： 指定数量的卷积核。对图像的每个区域进行一系列数学计算，得到单一的值，作为特征。再使用ReLU作为激活函数，以引入非线性特征。</li>
<li>池化层：对卷积得到的特征再次降维以减少处理时间。常用max_pooling，对每个2*2区域保留最大啊值，丢弃其他值。</li>
<li>全连接层：对卷积层提取、池化层采样的特征进行分类。每个节点都和上一层的每个节点相连接。</li>
</ol>
<p>CNN由一堆卷积模块组成，每个模块都有卷积层，后跟一池化层。</p>
<p>最后一个卷积模块包含一个或多个全连接层，用作分类。</p>
<p>最后一个全连接层中，每个类别有一个节点（使模型能预测所有类别），用softmax作为激活函数（所有softmax值和为1），每个softmax值可以解释为图像属于该类的概率。</p>
<p>斯坦福CNN课程资料<br><a href="http://cs231n.github.io/convolutional-networks/" target="_blank" rel="external">http://cs231n.github.io/convolutional-networks/</a></p>
<h2 id="使用CNN构建MNIST分类器"><a href="#使用CNN构建MNIST分类器" class="headerlink" title="使用CNN构建MNIST分类器"></a>使用CNN构建MNIST分类器</h2><h3 id="CNN结构"><a href="#CNN结构" class="headerlink" title="CNN结构"></a>CNN结构</h3><ul>
<li>卷积层-1：32个5*5filter，ReLU激活函数</li>
<li>池化层-1：:2*2max pooling，步长2（使采样区域不重复）</li>
<li>卷积层-2：64个5*5filter, ReLu激活函数</li>
<li>池化层-2：2*2max pooling，步长2</li>
<li>全连接层-1：1024神经元，dropout率0.4（训练中0.4概率给定的元素被丢弃）</li>
<li>全连接层-2：10神经元，每个代表一类</li>
</ul>
<p>使用tf.layers模块构建上述各类型的层</p>
<ul>
<li>conv2d 2维卷积层，给定卷积核数量、大小、padding、激活函数作为参数</li>
<li>max_pooling2d    2维池化层（max_pooling），给定filter大小、步长作为参数</li>
<li>dense    全连接层，参数为神经元数量、激活函数</li>
</ul>
<p>以上方法都接收tensor作为输入，然后输出一个处理后的tensor。可以直接使用返回值作为下一层的输入。</p>
<h3 id="cnn-model-fn"><a href="#cnn-model-fn" class="headerlink" title="cnn_model_fn()"></a>cnn_model_fn()</h3><p><code>cnn_mnist.py</code>接受MNIST特征数据、标签、模型类型（TRAIN、EVAL、INFER）作为输入参数，配置CNN网络，返回预测结果、损失和训练步骤。</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div><div class="line">42</div><div class="line">43</div><div class="line">44</div><div class="line">45</div><div class="line">46</div><div class="line">47</div><div class="line">48</div><div class="line">49</div><div class="line">50</div><div class="line">51</div><div class="line">52</div><div class="line">53</div><div class="line">54</div><div class="line">55</div><div class="line">56</div><div class="line">57</div><div class="line">58</div><div class="line">59</div><div class="line">60</div><div class="line">61</div><div class="line">62</div></pre></td><td class="code"><pre><div class="line">def cnn_model_fn(features, labels, mode):</div><div class="line">  &quot;&quot;&quot;Model function for CNN.&quot;&quot;&quot;</div><div class="line">  # Input Layer</div><div class="line">  input_layer = tf.reshape(features, [-1, 28, 28, 1])</div><div class="line"></div><div class="line">  # Convolutional Layer #1</div><div class="line">  conv1 = tf.layers.conv2d(</div><div class="line">      inputs=input_layer,</div><div class="line">      filters=32,</div><div class="line">      kernel_size=[5, 5],</div><div class="line">      padding=&quot;same&quot;,</div><div class="line">      activation=tf.nn.relu)</div><div class="line"></div><div class="line">  # Pooling Layer #1</div><div class="line">  pool1 = tf.layers.max_pooling2d(inputs=conv1, pool_size=[2, 2], strides=2)</div><div class="line"></div><div class="line">  # Convolutional Layer #2 and Pooling Layer #2</div><div class="line">  conv2 = tf.layers.conv2d(</div><div class="line">      inputs=pool1,</div><div class="line">      filters=64,</div><div class="line">      kernel_size=[5, 5],</div><div class="line">      padding=&quot;same&quot;,</div><div class="line">      activation=tf.nn.relu)</div><div class="line">  pool2 = tf.layers.max_pooling2d(inputs=conv2, pool_size=[2, 2], strides=2)</div><div class="line"></div><div class="line">  # Dense Layer</div><div class="line">  pool2_flat = tf.reshape(pool2, [-1, 7 * 7 * 64])</div><div class="line">  dense = tf.layers.dense(inputs=pool2_flat, units=1024, activation=tf.nn.relu)</div><div class="line">  dropout = tf.layers.dropout(</div><div class="line">      inputs=dense, rate=0.4, training=mode == learn.ModeKeys.TRAIN)</div><div class="line"></div><div class="line">  # Logits Layer</div><div class="line">  logits = tf.layers.dense(inputs=dropout, units=10)</div><div class="line"></div><div class="line">  loss = None</div><div class="line">  train_op = None</div><div class="line"></div><div class="line">  # Calculate Loss (for both TRAIN and EVAL modes)</div><div class="line">  if mode != learn.ModeKeys.INFER:</div><div class="line">    onehot_labels = tf.one_hot(indices=tf.cast(labels, tf.int32), depth=10)</div><div class="line">    loss = tf.losses.softmax_cross_entropy(</div><div class="line">        onehot_labels=onehot_labels, logits=logits)</div><div class="line"></div><div class="line">  # Configure the Training Op (for TRAIN mode)</div><div class="line">  if mode == learn.ModeKeys.TRAIN:</div><div class="line">    train_op = tf.contrib.layers.optimize_loss(</div><div class="line">        loss=loss,</div><div class="line">        global_step=tf.contrib.framework.get_global_step(),</div><div class="line">        learning_rate=0.001,</div><div class="line">        optimizer=&quot;SGD&quot;)</div><div class="line"></div><div class="line">  # Generate Predictions</div><div class="line">  predictions = &#123;</div><div class="line">      &quot;classes&quot;: tf.argmax(</div><div class="line">          input=logits, axis=1),</div><div class="line">      &quot;probabilities&quot;: tf.nn.softmax(</div><div class="line">          logits, name=&quot;softmax_tensor&quot;)</div><div class="line">  &#125;</div><div class="line"></div><div class="line">  # Return a ModelFnOps object</div><div class="line">  return model_fn_lib.ModelFnOps(</div><div class="line">      mode=mode, predictions=predictions, loss=loss, train_op=train_op)</div></pre></td></tr></table></figure>
<p>上述代码使用<code>tf.layers</code>模块构建各个层，计算损失，配置训练步骤，并生成预测。</p>
<h4 id="输入层"><a href="#输入层" class="headerlink" title="输入层"></a>输入层</h4><p>对2维的图像数据构建卷积层和池化层需要输入的tensor的形状为<code>[batch_size,image_width,image_height, channels]</code></p>
<ul>
<li>batch_size 梯度下降训练时子集大小</li>
<li>image_width 输入图像宽度</li>
<li>image_height 输入图像高度</li>
<li>channels 彩色图片为3（红绿蓝）,黑白图片为1（黑）</li>
</ul>
<p>MNIST数据为单色28*28像素，所以输入的shape为<code>[batch_size, 28, 28, 1]</code></p>
<p>把输入转换为该shape，执行<code>reshape</code></p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">input_layer = tf.reshape(features, [-1, 28, 28, 1])</div></pre></td></tr></table></figure>
<p>上面的<code>batch_size</code>为-1表示该维度不确定，由输入的<code>features</code>动态计算。这允许我们把<code>batch_size</code>当做可调整的超参数。例如：batch为5时，<code>features</code>包含3920 (5<em>28</em>28)个值，shape为<code>[5,28,28,1]</code>，batch为100时，输入的值个数为78400</p>
<h4 id="卷积层-1"><a href="#卷积层-1" class="headerlink" title="卷积层-1"></a>卷积层-1</h4><p>对输入层应用32个5*5的filter，并使用ReLU作为激活函数。可以使用<code>conv2d()</code>函数来创建这一层</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div></pre></td><td class="code"><pre><div class="line">conv1 = tf.layers.conv2d(</div><div class="line">    inputs=input_layer,</div><div class="line">    filters=32,</div><div class="line">    kernel_size=[5, 5],</div><div class="line">    padding=&quot;same&quot;,</div><div class="line">    activation=tf.nn.relu)</div></pre></td></tr></table></figure>
<p>输入的tensor的形状必须是<code>[batch_size, image_width,image_height,channels]</code>,这样可以与输入层相连。</p>
<ul>
<li>filters 卷积核数量</li>
<li>kernel_size 卷积核维度</li>
<li>padding 为”valid”或”same”。padding=”same”时，输出的tensor和输入tensor维度相同（边缘处补0）。没有padding的话，28<em>28经过5</em>5卷积，会产生24*24的tensor</li>
<li>activation 指定激活函数，这里使用ReLU（tf.nn.relu）</li>
</ul>
<p>输出tensor的形状为<code>[batch_size,28,28,32]</code>，宽度和高度和原来相同，但经过32个卷积核卷积，现在有32个channels</p>
<h4 id="池化层-1"><a href="#池化层-1" class="headerlink" title="池化层-1"></a>池化层-1</h4><p>现在可以把第一个pooling层和上面创建好的卷积层连接。使用<code>max_pooling2d()</code>构建池化层</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">pool1 = tf.layers.max_pooling2d(inputs=conv1, pool_size=[2, 2], strides=2)</div></pre></td></tr></table></figure>
<ul>
<li>inputs 输入tensor。形状为<code>[batch_size, image_width, image_height, channels]</code>。这里的输入为上一层的输出，形状是<code>[batch_size, 28, 28, 32]</code></li>
<li>pool_size max pool filter的大小（[width, height]），这里为[2,2]</li>
<li>strides 步长的大小。2表示长和宽每2像素划分为一个子区域（2*2的filter使用strides=2，使子区域没有重叠）。也可设置其他大小，如[3,6]</li>
</ul>
<p><code>max_pooling2d()</code>输入的tensor形状为<code>[batch_size, 14, 14, 32]</code>，2*2的filter把高度和宽度减少了50%</p>
<h4 id="卷积层-2和池化层-2"><a href="#卷积层-2和池化层-2" class="headerlink" title="卷积层-2和池化层-2"></a>卷积层-2和池化层-2</h4><p>和前面一样，第二个卷积层和池化层使用<code>conv2d()</code>和<code>max_pooling2d()</code>。在这里，卷积层使用64个5<em>5卷积核，池化层的设置与前面一样（2</em>2的max pool，步长2）</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div></pre></td><td class="code"><pre><div class="line">conv2 = tf.layers.conv2d(</div><div class="line">    inputs=pool1,</div><div class="line">    filters=64,</div><div class="line">    kernel_size=[5, 5],</div><div class="line">    padding=&quot;same&quot;,</div><div class="line">    activation=tf.nn.relu)</div><div class="line"></div><div class="line">pool2 = tf.layers.max_pooling2d(inputs=conv2, pool_size=[2, 2], strides=2)</div></pre></td></tr></table></figure>
<p>第二层卷积层使用第一层池化层的输出<code>pool1</code>作为输入，输出<code>h_conv2</code>tensor，<code>h_conv2</code>的形状为<code>[batch_size,14,14,64]</code>，高和宽与pool1相同，使用64个卷积核，所以channel为64</p>
<p>第二层池化层的形状为<code>[batch_size, 7, 7, 64]</code>（高和宽再次减少50%）</p>
<h4 id="全连接层"><a href="#全连接层" class="headerlink" title="全连接层"></a>全连接层</h4><p>接下来添加一层包含1024神经元，使用ReLU的dense层，对卷积/池化采样的特征进行分类。在连接该层前，需要把特征映射（feature map）变换形状，使其变为2维</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">pool2_flat = tf.reshape(pool2, [-1, 7 * 7 * 64])</div></pre></td></tr></table></figure>
<p>-1表示<code>batch_size</code>维度不定，由输入的样本数量动态计算。每个样本有7<em>7</em>64个特征（长度/宽度/channels）。现在<code>pool2_flat</code>的形状为<code>[batch_size,3136]</code></p>
<p>使用<code>dense()</code>连接该dense层</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">dense = tf.layers.dense(inputs=pool2_flat, units=1024, activation=tf.nn.relu)</div></pre></td></tr></table></figure>
<ul>
<li>input 输入的tensor （展开的特征映射）</li>
<li>units 神经元数量</li>
<li>activation 激活函数</li>
</ul>
<p>为了提高模型效果，需要添加dropout regularization，使用<code>tf.layers.dropout()</code></p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">dropout = tf.layers.dropout(</div><div class="line">    inputs=dense, rate=0.4, training=mode == learn.ModeKeys.TRAIN)</div></pre></td></tr></table></figure>
<ul>
<li>input输入的tensor（<code>dense</code>）</li>
<li>rate dropout率：训练中40%的元素被随机丢弃</li>
<li>training 指定模型是否正在训练中。dropout只有在training=True时被启用。</li>
</ul>
<h4 id="Logits层"><a href="#Logits层" class="headerlink" title="Logits层"></a>Logits层</h4><p>神经网络最后一层是logits层，返回预测的原始值。该层有10个神经元（代表数字0-9）,使用线性激活函数（默认）</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">logits = tf.layers.dense(inputs=dropout, units=10)</div></pre></td></tr></table></figure>
<h4 id="计算损失"><a href="#计算损失" class="headerlink" title="计算损失"></a>计算损失</h4><p>训练和评估模型时，需要定义损失函数来度量模型预测结果和真实值的差异。多分类问题（例如MNIST）中，交叉熵（cross entropy）是典型的损失度量。</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div></pre></td><td class="code"><pre><div class="line">loss = None</div><div class="line">train_op = None</div><div class="line"></div><div class="line"># Calculate loss for both TRAIN and EVAL modes</div><div class="line">if mode != learn.ModeKeys.INFER:</div><div class="line">  onehot_labels = tf.one_hot(indices=tf.cast(labels, tf.int32), depth=10)</div><div class="line">  loss = tf.losses.softmax_cross_entropy(</div><div class="line">      onehot_labels=onehot_labels, logits=logits)</div></pre></td></tr></table></figure>
<p><code>labels</code> tensor包括一组样本的预测分类值，例如[1,9….]。为了计算交叉熵，需要将其变为one-hot编码：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line">[[0, 1, 0, 0, 0, 0, 0, 0, 0, 0],</div><div class="line"> [0, 0, 0, 0, 0, 0, 0, 0, 0, 1],</div><div class="line"> ...]</div></pre></td></tr></table></figure>
<p>使用<code>tf.onehot()</code>进行转换。</p>
<ul>
<li>indices 输入，哪个位置变为one-hot值</li>
<li>depth 类别的数目</li>
</ul>
<p>下面的代码生成one-hot tensor</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">onehot_labels = tf.one_hot(indices=tf.cast(labels, tf.int32), depth=10)</div></pre></td></tr></table></figure>
<p><code>labels</code> 中为0-9的数值（[1,9…]），将其转换为int类型。<code>depth</code>为10，因为有10个可能的目标类别。</p>
<p>使用<code>tf.losses.softmax_cross_entropy()</code>对logits层进行softmax，计算交叉熵，结果为标量tensor</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">loss = tf.losses.softmax_cross_entropy(</div><div class="line">        onehot_labels=onehot_labels, logits=logits)</div></pre></td></tr></table></figure>
<h4 id="设置训练步骤"><a href="#设置训练步骤" class="headerlink" title="设置训练步骤"></a>设置训练步骤</h4><p>前面定义了CNN结构，现在设置训练步骤，以优化训练损失。使用<code>tf.contrib.layers.optimize_loss()</code>, 这里使用0.001学习率，并使用随机梯度下降（stochastic gradient descent）作为优化算法</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div></pre></td><td class="code"><pre><div class="line"># Configure the Training Op (for TRAIN mode)</div><div class="line">if mode == learn.ModeKeys.TRAIN:</div><div class="line">    train_op = tf.contrib.layers.optimize_loss(</div><div class="line">        loss=loss,</div><div class="line">        global_step=tf.contrib.framework.get_global_step(),</div><div class="line">        learning_rate=0.001,</div><div class="line">        optimizer=&quot;SGD&quot;)</div></pre></td></tr></table></figure>
<h4 id="预测"><a href="#预测" class="headerlink" title="预测"></a>预测</h4><p>logits层返回[batch_size, 10]的tensor，现将这些原始值转换为两种形式</p>
<ol>
<li>预测的类别</li>
<li>各类别的概率<br>例子中，预测的类别是返回的tensor中每行最大值对应的列，使用<code>tf.argmax</code>找到其索引</li>
</ol>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">tf.argmax(input=logits, axis=1)</div></pre></td></tr></table></figure>
<ul>
<li>input 从该tensor中提取最大值</li>
<li>axis  从哪个维度提取（logits形状为[batch_size, 10]）</li>
</ul>
<p>通过<code>tf.nn.softmax()</code>得到概率</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">tf.nn.softmax(logits, name=&quot;softmax_tensor&quot;)</div></pre></td></tr></table></figure>
<p>将输出整理为字典形式：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div></pre></td><td class="code"><pre><div class="line">predictions = &#123;</div><div class="line">    &quot;classes&quot;: tf.argmax(</div><div class="line">        input=logits, axis=1),</div><div class="line">    &quot;probabilities&quot;: tf.nn.softmax(</div><div class="line">        logits, name=&quot;softmax_tensor&quot;)</div><div class="line">&#125;</div></pre></td></tr></table></figure>
<p>现在，得到了<code>predictions</code>,<code>loss</code>,<code>train_op</code>，连同<code>mode</code>参数可以把它们返回。</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line"># Return a ModelFnOps object</div><div class="line">return model_fn_lib.ModelFnOps(</div><div class="line">    mode=mode, predictions=predictions, loss=loss, train_op=train_op)</div></pre></td></tr></table></figure>
<h2 id="训练和评估CNN-MNIST模型"><a href="#训练和评估CNN-MNIST模型" class="headerlink" title="训练和评估CNN MNIST模型"></a>训练和评估CNN MNIST模型</h2><h3 id="加载训练集-测试集"><a href="#加载训练集-测试集" class="headerlink" title="加载训练集/测试集"></a>加载训练集/测试集</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div></pre></td><td class="code"><pre><div class="line">def main(unused_argv):</div><div class="line">  # Load training and eval data</div><div class="line">  mnist = learn.datasets.load_dataset(&quot;mnist&quot;)</div><div class="line">  train_data = mnist.train.images # Returns np.array</div><div class="line">  train_labels = np.asarray(mnist.train.labels, dtype=np.int32)</div><div class="line">  eval_data = mnist.test.images # Returns np.array</div><div class="line">  eval_labels = np.asarray(mnist.test.labels, dtype=np.int32)</div></pre></td></tr></table></figure>
<p>把训练集数据（55000图像的原始像素）和标签（0-9）以numpy arrays保存在<code>train_data</code>和<code>train_lable</code>中。<br>相似的，保存10000数据和标签作为测试集。</p>
<h3 id="创建Estimator"><a href="#创建Estimator" class="headerlink" title="创建Estimator"></a>创建Estimator</h3><p>Estimator是TensorFlow用来进行高级模型训练、评价和预测的类。</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line"># Create the Estimator</div><div class="line">mnist_classifier = learn.Estimator(</div><div class="line">      model_fn=cnn_model_fn, model_dir=&quot;/tmp/mnist_convnet_model&quot;)</div></pre></td></tr></table></figure>
<ul>
<li>model_fn 指定训练/评估/预测的模型</li>
<li>model_dir 保存checkpoints的路径</li>
</ul>
<h3 id="Logging"><a href="#Logging" class="headerlink" title="Logging"></a>Logging</h3><p>创建日志以跟踪训练过程。创建一个<code>tf.train.LoggingTensorHook</code>，会记录Softmax层计算出的概率</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div></pre></td><td class="code"><pre><div class="line"># Set up logging for predictions</div><div class="line">  tensors_to_log = &#123;&quot;probabilities&quot;: &quot;softmax_tensor&quot;&#125;</div><div class="line">  logging_hook = tf.train.LoggingTensorHook(</div><div class="line">      tensors=tensors_to_log, every_n_iter=50)</div></pre></td></tr></table></figure>
<ul>
<li>将需要记录的tensors存入tensor_to_log字典，key作为日志中的标签，相应的值是指定tensor的名称（name属性）</li>
<li>every_n_iter 每训练50步记录一次</li>
</ul>
<h3 id="训练模型"><a href="#训练模型" class="headerlink" title="训练模型"></a>训练模型</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div></pre></td><td class="code"><pre><div class="line"># Train the model</div><div class="line">mnist_classifier.fit(</div><div class="line">    x=train_data,</div><div class="line">    y=train_labels,</div><div class="line">    batch_size=100,</div><div class="line">    steps=20000,</div><div class="line">    monitors=[logging_hook])</div></pre></td></tr></table></figure>
<ul>
<li>x 训练数据</li>
<li>y 训练集标签</li>
<li>batch 每次训练样本数</li>
<li>steps 训练次数</li>
<li>monitors logging_hook</li>
</ul>
<h3 id="评估模型"><a href="#评估模型" class="headerlink" title="评估模型"></a>评估模型</h3><p>在MNIST测试集上评价训练好的模型。使用<code>tf.contrib.learn.MetricSpec</code>创建metrics字典来计算准确率</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div></pre></td><td class="code"><pre><div class="line"># Configure the accuracy metric for evaluation</div><div class="line">metrics = &#123;</div><div class="line">    &quot;accuracy&quot;:</div><div class="line">        learn.MetricSpec(</div><div class="line">            metric_fn=tf.metrics.accuracy, prediction_key=&quot;classes&quot;),</div><div class="line">&#125;</div></pre></td></tr></table></figure>
<ul>
<li>metric_fn 计算metric的函数，这里使用预定义的<code>tf.metrics.accuracy</code></li>
<li>prediction_key 包含预测值的tensor的名称（前面定义过）</li>
</ul>
<p>通过下面代码打印评估结果：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div></pre></td><td class="code"><pre><div class="line"># Evaluate the model and print results</div><div class="line">eval_results = mnist_classifier.evaluate(</div><div class="line">    x=eval_data, y=eval_labels, metrics=metrics)</div><div class="line">print(eval_results)</div></pre></td></tr></table></figure>
<h3 id="运行模型"><a href="#运行模型" class="headerlink" title="运行模型"></a>运行模型</h3><p>结果显示如下</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div></pre></td><td class="code"><pre><div class="line">INFO:tensorflow:loss = 2.36026, step = 1</div><div class="line">INFO:tensorflow:probabilities = [[ 0.07722801  0.08618255  0.09256398, ...]]</div><div class="line">...</div><div class="line">INFO:tensorflow:loss = 2.13119, step = 101</div><div class="line">INFO:tensorflow:global_step/sec: 5.44132</div><div class="line">...</div><div class="line">INFO:tensorflow:Loss for final step: 0.553216.</div><div class="line"></div><div class="line">INFO:tensorflow:Restored model from /tmp/mnist_convnet_model</div><div class="line">INFO:tensorflow:Eval steps [0,inf) for training step 20000.</div><div class="line">INFO:tensorflow:Input iterator is exhausted.</div><div class="line">INFO:tensorflow:Saving evaluation summary for step 20000: accuracy = 0.9733, loss = 0.0902271</div><div class="line">&#123;&apos;loss&apos;: 0.090227105, &apos;global_step&apos;: 20000, &apos;accuracy&apos;: 0.97329998&#125;</div></pre></td></tr></table></figure>

          
        
      
    </div>

    <div>
      
    </div>
	
	
	<div>
      
    </div>
	

    <div>
      
    </div>

    <div>
      
    </div>

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </article>


    
  </section>

  
  <nav class="pagination">
    <span class="page-number current">1</span><a class="page-number" href="/page/2/">2</a><a class="page-number" href="/page/3/">3</a><a class="extend next" rel="next" href="/page/2/"><i class="fa fa-angle-right"></i></a>
  </nav>



          </div>
          


          

        </div>
        
          
  
  <div class="sidebar-toggle">
    <div class="sidebar-toggle-line-wrap">
      <span class="sidebar-toggle-line sidebar-toggle-line-first"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-middle"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-last"></span>
    </div>
  </div>

  <aside id="sidebar" class="sidebar">
    <div class="sidebar-inner">

      

      

      <section class="site-overview sidebar-panel sidebar-panel-active">
        <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
          <img class="site-author-image" itemprop="image"
               src="/image/学士服照片.jpg"
               alt="Albert Lee" />
          <p class="site-author-name" itemprop="name">Albert Lee</p>
           
              <p class="site-description motion-element" itemprop="description">一个没人知道的地方</p>
          
        </div>
        <nav class="site-state motion-element">

          
            <div class="site-state-item site-state-posts">
              <a href="/archives">
                <span class="site-state-item-count">21</span>
                <span class="site-state-item-name">日志</span>
              </a>
            </div>
          

          

          
            
            
            <div class="site-state-item site-state-tags">
              <a href="/tags/index.html">
                <span class="site-state-item-count">26</span>
                <span class="site-state-item-name">标签</span>
              </a>
            </div>
          

        </nav>

        

        <div class="links-of-author motion-element">
          
            
              <span class="links-of-author-item">
                <a href="https://github.com/libowei1213" target="_blank" title="GitHub">
                  
                    <i class="fa fa-fw fa-github"></i>
                  
                  GitHub
                </a>
              </span>
            
              <span class="links-of-author-item">
                <a href="http://weibo.com/libowei1213" target="_blank" title="微博">
                  
                    <i class="fa fa-fw fa-weibo"></i>
                  
                  微博
                </a>
              </span>
            
              <span class="links-of-author-item">
                <a href="https://www.zhihu.com/people/li-bo-wei-72" target="_blank" title="知乎">
                  
                    <i class="fa fa-fw fa-globe"></i>
                  
                  知乎
                </a>
              </span>
            
          
        </div>

        
        

        
        
          <div class="links-of-blogroll motion-element links-of-blogroll-block">
            <div class="links-of-blogroll-title">
              <i class="fa  fa-fw fa-globe"></i>
              友情链接
            </div>
            <ul class="links-of-blogroll-list">
              
                <li class="links-of-blogroll-item">
                  <a href="http://huoche123.top/train/index" title="查火车" target="_blank">查火车</a>
                </li>
              
                <li class="links-of-blogroll-item">
                  <a href="http://huoche123.top/today" title="历史上的今天" target="_blank">历史上的今天</a>
                </li>
              
                <li class="links-of-blogroll-item">
                  <a href="http://duzhihu.cc" title="读知乎" target="_blank">读知乎</a>
                </li>
              
            </ul>
          </div>
        

        

      </section>

      

      

    </div>
  </aside>


        
      </div>
    </main>

    <footer id="footer" class="footer">
      <div class="footer-inner">
        <div class="copyright" >
  
  &copy;  2016 - 
  <span itemprop="copyrightYear">2017</span>
  <span class="with-love">
    <i class="fa fa-heart"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">Albert Lee</span>
</div>

<script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<script>
  (adsbygoogle = window.adsbygoogle || []).push({
    google_ad_client: "ca-pub-2599972617864441",
    enable_page_level_ads: true
  });
</script>


<div class="powered-by">
  由 <a class="theme-link" href="https://hexo.io">Hexo</a> 强力驱动
</div>

<div class="theme-info">
  主题 -
  <a class="theme-link" href="https://github.com/iissnan/hexo-theme-next">
    NexT.Pisces
  </a>
</div>


        
<div class="busuanzi-count">
  <script async src="https://dn-lbstatics.qbox.me/busuanzi/2.3/busuanzi.pure.mini.js"></script>

  
    <span class="site-uv">
      <i class="fa fa-user"></i>
      <span class="busuanzi-value" id="busuanzi_value_site_uv"></span>
      
    </span>
  

  
    <span class="site-pv">
      <i class="fa fa-eye"></i>
      <span class="busuanzi-value" id="busuanzi_value_site_pv"></span>
      
    </span>
  
</div>


        
      </div>
    </footer>

    
      <div class="back-to-top">
        <i class="fa fa-arrow-up"></i>
        
      </div>
    

  </div>

  

<script type="text/javascript">
  if (Object.prototype.toString.call(window.Promise) !== '[object Function]') {
    window.Promise = null;
  }
</script>









  


  





  
  <script type="text/javascript" src="/lib/jquery/index.js?v=2.1.3"></script>

  
  <script type="text/javascript" src="/lib/fastclick/lib/fastclick.min.js?v=1.0.6"></script>

  
  <script type="text/javascript" src="/lib/jquery_lazyload/jquery.lazyload.js?v=1.9.7"></script>

  
  <script type="text/javascript" src="/lib/velocity/velocity.min.js?v=1.2.1"></script>

  
  <script type="text/javascript" src="/lib/velocity/velocity.ui.min.js?v=1.2.1"></script>

  
  <script type="text/javascript" src="/lib/fancybox/source/jquery.fancybox.pack.js?v=2.1.5"></script>

  
  <script type="text/javascript" src="/lib/canvas-nest/canvas-nest.min.js"></script>


  


  <script type="text/javascript" src="/js/src/utils.js?v=5.1.0"></script>

  <script type="text/javascript" src="/js/src/motion.js?v=5.1.0"></script>



  
  


  <script type="text/javascript" src="/js/src/affix.js?v=5.1.0"></script>

  <script type="text/javascript" src="/js/src/schemes/pisces.js?v=5.1.0"></script>



  

  


  <script type="text/javascript" src="/js/src/bootstrap.js?v=5.1.0"></script>



  


  




	





  
    
    <script>
      var cloudTieConfig = {
        url: document.location.href, 
        sourceId: "",
        productKey: "cf430319302742b9a50fd82c00ca3171",
        target: "cloud-tie-wrapper"
      };
    </script>
    <script src="https://img1.ws.126.net/f2e/tie/yun/sdk/loader.js"></script>
  










  

  <script type="text/javascript">
    // Popup Window;
    var isfetched = false;
    // Search DB path;
    var search_path = "search.xml";
    if (search_path.length == 0) {
      search_path = "search.xml";
    }
    var path = "/" + search_path;
    // monitor main search box;

    function proceedsearch() {
      $("body")
        .append('<div class="search-popup-overlay local-search-pop-overlay"></div>')
        .css('overflow', 'hidden');
      $('.popup').toggle();
    }
    // search function;
    var searchFunc = function(path, search_id, content_id) {
      'use strict';
      $.ajax({
        url: path,
        dataType: "xml",
        async: true,
        success: function( xmlResponse ) {
          // get the contents from search data
          isfetched = true;
          $('.popup').detach().appendTo('.header-inner');
          var datas = $( "entry", xmlResponse ).map(function() {
            return {
              title: $( "title", this ).text(),
              content: $("content",this).text(),
              url: $( "url" , this).text()
            };
          }).get();
          var $input = document.getElementById(search_id);
          var $resultContent = document.getElementById(content_id);
          $input.addEventListener('input', function(){
            var matchcounts = 0;
            var str='<ul class=\"search-result-list\">';
            var keywords = this.value.trim().toLowerCase().split(/[\s\-]+/);
            $resultContent.innerHTML = "";
            if (this.value.trim().length > 1) {
              // perform local searching
              datas.forEach(function(data) {
                var isMatch = false;
                var content_index = [];
                var data_title = data.title.trim().toLowerCase();
                var data_content = data.content.trim().replace(/<[^>]+>/g,"").toLowerCase();
                var data_url = decodeURIComponent(data.url);
                var index_title = -1;
                var index_content = -1;
                var first_occur = -1;
                // only match artiles with not empty titles and contents
                if(data_title != '') {
                  keywords.forEach(function(keyword, i) {
                    index_title = data_title.indexOf(keyword);
                    index_content = data_content.indexOf(keyword);
                    if( index_title >= 0 || index_content >= 0 ){
                      isMatch = true;
                      if (i == 0) {
                        first_occur = index_content;
                      }
                    }

                  });
                }
                // show search results
                if (isMatch) {
                  matchcounts += 1;
                  str += "<li><a href='"+ data_url +"' class='search-result-title'>"+ data_title +"</a>";
                  var content = data.content.trim().replace(/<[^>]+>/g,"");
                  if (first_occur >= 0) {
                    // cut out 100 characters
                    var start = first_occur - 20;
                    var end = first_occur + 80;
                    if(start < 0){
                      start = 0;
                    }
                    if(start == 0){
                      end = 50;
                    }
                    if(end > content.length){
                      end = content.length;
                    }
                    var match_content = content.substring(start, end);
                    // highlight all keywords
                    keywords.forEach(function(keyword){
                      var regS = new RegExp(keyword, "gi");
                      match_content = match_content.replace(regS, "<b class=\"search-keyword\">"+keyword+"</b>");
                    });

                    str += "<p class=\"search-result\">" + match_content +"...</p>"
                  }
                  str += "</li>";
                }
              })};
            str += "</ul>";
            if (matchcounts == 0) { str = '<div id="no-result"><i class="fa fa-frown-o fa-5x" /></div>' }
            if (keywords == "") { str = '<div id="no-result"><i class="fa fa-search fa-5x" /></div>' }
            $resultContent.innerHTML = str;
          });
          proceedsearch();
        }
      });}

    // handle and trigger popup window;
    $('.popup-trigger').click(function(e) {
      e.stopPropagation();
      if (isfetched == false) {
        searchFunc(path, 'local-search-input', 'local-search-result');
      } else {
        proceedsearch();
      };
    });

    $('.popup-btn-close').click(function(e){
      $('.popup').hide();
      $(".local-search-pop-overlay").remove();
      $('body').css('overflow', '');
    });
    $('.popup').click(function(e){
      e.stopPropagation();
    });
  </script>





  

  
  <script src="https://cdn1.lncld.net/static/js/av-core-mini-0.6.1.js"></script>
  <script>AV.initialize("1HjW8mQ2Mvqrnu7j9nRM17tm-gzGzoHsz", "lnkha8qqp0bLLPJEpucS0V1F");</script>
  <script>
    function showTime(Counter) {
      var query = new AV.Query(Counter);
      var entries = [];
      var $visitors = $(".leancloud_visitors");

      $visitors.each(function () {
        entries.push( $(this).attr("id").trim() );
      });

      query.containedIn('url', entries);
      query.find()
        .done(function (results) {
          var COUNT_CONTAINER_REF = '.leancloud-visitors-count';

          if (results.length === 0) {
            $visitors.find(COUNT_CONTAINER_REF).text(0);
            return;
          }

          for (var i = 0; i < results.length; i++) {
            var item = results[i];
            var url = item.get('url');
            var time = item.get('time');
            var element = document.getElementById(url);

            $(element).find(COUNT_CONTAINER_REF).text(time);
          }
          for(var i = 0; i < entries.length; i++) {
            var url = entries[i];
            var element = document.getElementById(url);
            var countSpan = $(element).find(COUNT_CONTAINER_REF);
            if( countSpan.text() == '') {
              countSpan.text(0);
            }
          }
        })
        .fail(function (object, error) {
          console.log("Error: " + error.code + " " + error.message);
        });
    }

    function addCount(Counter) {
      var $visitors = $(".leancloud_visitors");
      var url = $visitors.attr('id').trim();
      var title = $visitors.attr('data-flag-title').trim();
      var query = new AV.Query(Counter);

      query.equalTo("url", url);
      query.find({
        success: function(results) {
          if (results.length > 0) {
            var counter = results[0];
            counter.fetchWhenSave(true);
            counter.increment("time");
            counter.save(null, {
              success: function(counter) {
                var $element = $(document.getElementById(url));
                $element.find('.leancloud-visitors-count').text(counter.get('time'));
              },
              error: function(counter, error) {
                console.log('Failed to save Visitor num, with error message: ' + error.message);
              }
            });
          } else {
            var newcounter = new Counter();
            /* Set ACL */
            var acl = new AV.ACL();
            acl.setPublicReadAccess(true);
            acl.setPublicWriteAccess(true);
            newcounter.setACL(acl);
            /* End Set ACL */
            newcounter.set("title", title);
            newcounter.set("url", url);
            newcounter.set("time", 1);
            newcounter.save(null, {
              success: function(newcounter) {
                var $element = $(document.getElementById(url));
                $element.find('.leancloud-visitors-count').text(newcounter.get('time'));
              },
              error: function(newcounter, error) {
                console.log('Failed to create');
              }
            });
          }
        },
        error: function(error) {
          console.log('Error:' + error.code + " " + error.message);
        }
      });
    }

    $(function() {
      var Counter = AV.Object.extend("Counter");
      if ($('.leancloud_visitors').length == 1) {
        addCount(Counter);
      } else if ($('.post-title-link').length > 1) {
        showTime(Counter);
      }
    });
  </script><!-- hexo-inject:begin --><!-- Begin: Injected MathJax -->
<script type="text/x-mathjax-config">
  MathJax.Hub.Config({"tex2jax":{"inlineMath":[["$","$"],["\\(","\\)"]],"skipTags":["script","noscript","style","textarea","pre","code"],"processEscapes":true},"TeX":{"equationNumbers":{"autoNumber":"AMS"}}});
</script>

<script type="text/x-mathjax-config">
  MathJax.Hub.Queue(function() {
    var all = MathJax.Hub.getAllJax(), i;
    for(i=0; i < all.length; i += 1) {
      all[i].SourceElement().parentNode.className += ' has-jax';
    }
  });
</script>

<script type="text/javascript" src="//cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML">
</script>
<!-- End: Injected MathJax -->
<!-- hexo-inject:end -->



  

  

  

</body>
</html>
