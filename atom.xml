<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>博伟的博客</title>
  
  <subtitle>Albert&#39;s blog</subtitle>
  <link href="/atom.xml" rel="self"/>
  
  <link href="http://libowei.net/"/>
  <updated>2017-11-24T04:54:31.527Z</updated>
  <id>http://libowei.net/</id>
  
  <author>
    <name>Albert Lee</name>
    
  </author>
  
  <generator uri="http://hexo.io/">Hexo</generator>
  
  <entry>
    <title>ImportError: /usr/lib64/libstdc++.so.6: version CXXABI_1.3.7’ not found</title>
    <link href="http://libowei.net/ImportError-usr-lib64-libstdc-so-6-version-CXXABI-1-3-7%E2%80%99-not-found.html"/>
    <id>http://libowei.net/ImportError-usr-lib64-libstdc-so-6-version-CXXABI-1-3-7’-not-found.html</id>
    <published>2017-11-24T04:53:26.000Z</published>
    <updated>2017-11-24T04:54:31.527Z</updated>
    
    <content type="html"><![CDATA[<p>今天在CentOS上升级TensorFlow1.4版本后出现了一个问题</p><p>Python程序中<code>import tensorflow as tf</code>会出现下面的错误：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line">ImportError: /usr/lib64/libstdc++.so.6: version `CXXABI_1.3.7&apos; not found (required by /root/anaconda3/lib/python3.6/site-nternal.so)</div><div class="line"></div><div class="line">Failed to load the native TensorFlow runtime.</div></pre></td></tr></table></figure></p><p>出现这个问题的原因是没有链接到<code>CXXABI</code>库的最新的版本。通过下面的命令查看<code>/usr/lib64/</code>下的动态库版本<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">strings /usr/lib64/libstdc++.so.6 | grep &apos;CXXABI&apos;</div></pre></td></tr></table></figure></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div></pre></td><td class="code"><pre><div class="line">CXXABI_1.3</div><div class="line">CXXABI_1.3.1</div><div class="line">CXXABI_1.3.2</div><div class="line">CXXABI_1.3.3</div></pre></td></tr></table></figure><p>发现只有CXXABI的1.3.3版本，而TensorFlow环境需要1.3.7的版本。</p><p>这个动态库是gcc的库，升级gcc后系统中应该存在最新版的libstdc++.so.6链接库文件。<br>用下面命令查找动态库文件：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">find / -name &quot;libstdc++.so.*&quot;</div></pre></td></tr></table></figure></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div></pre></td><td class="code"><pre><div class="line">/usr/lib/libstdc++.so.5.0.7</div><div class="line">/usr/lib/libstdc++.so.6</div><div class="line">/usr/lib/libstdc++.so.6.0.13</div><div class="line">/usr/lib/libstdc++.so.5</div><div class="line">/usr/lib64/libstdc++.so.5.0.7</div><div class="line">/usr/lib64/libstdc++.so.6</div><div class="line">/usr/lib64/libstdc++.so.6.0.13</div><div class="line">/usr/lib64/libstdc++.so.5</div><div class="line">/root/anaconda3/lib/libstdc++.so.6</div><div class="line">/root/anaconda3/lib/libstdc++.so.6.0.19</div><div class="line">/root/anaconda3/pkgs/libgcc-4.8.5-2/lib/libstdc++.so.6</div><div class="line">/root/anaconda3/pkgs/libgcc-4.8.5-2/lib/libstdc++.so.6.0.19</div></pre></td></tr></table></figure><p><code>/root/anaconda3/lib/libstdc++.so.6.0.19</code>为最新的动态库文件。把<code>libstdc++.so.6.0.19</code>链接到<code>libstdc++.so.6</code>上：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div></pre></td><td class="code"><pre><div class="line">cp /root/anaconda3/lib/libstdc++.so.6.0.19 /usr/lib64/</div><div class="line">cd /usr/lib64/</div><div class="line">rm -rf libstdc++.so.6</div><div class="line">ls -n libstdc++.so.6.0.19 libstdc++.so.6</div></pre></td></tr></table></figure></p><p>默认动态库升级完成。重新运行以下命令检查动态库：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">strings /usr/lib64/libstdc++.so.6 | grep &apos;CXXABI&apos;</div></pre></td></tr></table></figure></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div></pre></td><td class="code"><pre><div class="line">CXXABI_1.3</div><div class="line">CXXABI_1.3.1</div><div class="line">CXXABI_1.3.2</div><div class="line">CXXABI_1.3.3</div><div class="line">CXXABI_1.3.4</div><div class="line">CXXABI_1.3.5</div><div class="line">CXXABI_1.3.6</div><div class="line">CXXABI_1.3.7</div><div class="line">CXXABI_TM_1</div></pre></td></tr></table></figure><p>现在<code>CXXABI</code>动态库版本更新到了1.3.7。运行TensorFlow不会报相应的错误了。</p><p>参考：<a href="http://blog.csdn.net/zx714311728/article/details/69628836" target="_blank" rel="external">http://blog.csdn.net/zx714311728/article/details/69628836</a></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;今天在CentOS上升级TensorFlow1.4版本后出现了一个问题&lt;/p&gt;
&lt;p&gt;Python程序中&lt;code&gt;import tensorflow as tf&lt;/code&gt;会出现下面的错误：&lt;br&gt;&lt;figure class=&quot;highlight plain&quot;&gt;&lt;tab
      
    
    </summary>
    
    
      <category term="centos" scheme="http://libowei.net/tags/centos/"/>
    
      <category term="tensorflow" scheme="http://libowei.net/tags/tensorflow/"/>
    
  </entry>
  
  <entry>
    <title>Distributed Tensorflow</title>
    <link href="http://libowei.net/Distributed-Tensorflow.html"/>
    <id>http://libowei.net/Distributed-Tensorflow.html</id>
    <published>2017-11-16T11:29:43.000Z</published>
    <updated>2017-11-23T04:11:59.260Z</updated>
    
    <content type="html"><![CDATA[<p>原文地址: <a href="https://tensorflow.google.cn/deploy/distributed" target="_blank" rel="external">Distributed TensorFlow</a></p><p>执行简单的TensorFlow集群，执行下面语句：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div></pre></td><td class="code"><pre><div class="line"># Start a TensorFlow server as a single-process &quot;cluster&quot;.</div><div class="line">$ python</div><div class="line">&gt;&gt;&gt; import tensorflow as tf</div><div class="line">&gt;&gt;&gt; c = tf.constant(&quot;Hello, distributed TensorFlow!&quot;)</div><div class="line">&gt;&gt;&gt; server = tf.train.Server.create_local_server()</div><div class="line">&gt;&gt;&gt; sess = tf.Session(server.target)  # Create a session on the server.</div><div class="line">&gt;&gt;&gt; sess.run(c)</div><div class="line">&apos;Hello, distributed TensorFlow!&apos;</div></pre></td></tr></table></figure></p><p><code>tf.train.Server.create_local_server</code>方法在服务器中创建了一个单一进程的集群。</p><h2 id="创建集群"><a href="#创建集群" class="headerlink" title="创建集群"></a>创建集群</h2><p>TensorFlow集群(cluster)是一组参与TensorFlow分布式图计算的”tasks”。每个task与TensorFlow “server”关联，”server”包含一个用来创建session的master，和一个执行图运算的worker。一个集群可以被分为一个或多个jobs，每个job包含一个或多个tasks。</p><p>创建集群需要在每个task中启动TensorFlow server。通常每个task运行在不同的机器上，但也可以在同一台机器上启动多个tasks（如控制不同的GPU设备）。在每个task中，需要：</p><ol><li>创建<code>tf.train.ClusterSpec</code>，用来描述集群中的tasks。对于每个task都是相同的。</li><li>创建<code>tf.train.Server</code>，在构造函数中传入<code>tf.train.ClusterSpec</code>，用job名和task序号来指定本机的task。</li></ol><h3 id="创建tf-train-ClusterSpec描述集群"><a href="#创建tf-train-ClusterSpec描述集群" class="headerlink" title="创建tf.train.ClusterSpec描述集群"></a>创建<code>tf.train.ClusterSpec</code>描述集群</h3><p>集群用字典来定义：jobs名称对应网络地址列表。把这个字典传入<code>tf.train.ClusterSpec</code>构造器。<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">tf.train.ClusterSpec(&#123;&quot;local&quot;: [&quot;localhost:2222&quot;, &quot;localhost:2223&quot;]&#125;)</div></pre></td></tr></table></figure></p><ul><li><strong>/job:local/task:0</strong></li><li><strong>/job:local/task:1</strong></li></ul><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div></pre></td><td class="code"><pre><div class="line">tf.train.ClusterSpec(&#123;</div><div class="line">    &quot;worker&quot;: [</div><div class="line">        &quot;worker0.example.com:2222&quot;,</div><div class="line">        &quot;worker1.example.com:2222&quot;,</div><div class="line">        &quot;worker2.example.com:2222&quot;</div><div class="line">    ],</div><div class="line">    &quot;ps&quot;: [</div><div class="line">        &quot;ps0.example.com:2222&quot;,</div><div class="line">        &quot;ps1.example.com:2222&quot;</div><div class="line">    ]&#125;)</div></pre></td></tr></table></figure><ul><li><strong>/job:worker/task:0</strong></li><li><strong>/job:worker/task:1</strong></li><li><strong>/job:worker/task:2</strong></li><li><strong>/job:ps/task:0</strong></li><li><strong>/job:ps/task:1</strong></li></ul><h3 id="在每个task中创建tf-train-Server实例"><a href="#在每个task中创建tf-train-Server实例" class="headerlink" title="在每个task中创建tf.train.Server实例"></a>在每个task中创建<code>tf.train.Server</code>实例</h3><p>一个<code>tf.train.Server</code>对象包含一组本地设备，一组与由<code>tf.train.ClusterSpec</code>定义的设备的连接，和一个可以使用这些设备进行分布式计算的<code>tf.Session</code>。每个server都是被命名的job的成员，有自己的task索引号。集群中的一个server可以同其他server进行通信。</p><p>例如，要启动运行在<code>localhost:2222</code>和<code>localhost:2223</code>，有两个servers的集群，在本地主机上运行下面的脚本，启动两个不同的进程：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line"># In task 0:</div><div class="line">cluster = tf.train.ClusterSpec(&#123;&quot;local&quot;: [&quot;localhost:2222&quot;, &quot;localhost:2223&quot;]&#125;)</div><div class="line">server = tf.train.Server(cluster, job_name=&quot;local&quot;, task_index=0)</div></pre></td></tr></table></figure></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line"># In task 1:</div><div class="line">cluster = tf.train.ClusterSpec(&#123;&quot;local&quot;: [&quot;localhost:2222&quot;, &quot;localhost:2223&quot;]&#125;)</div><div class="line">server = tf.train.Server(cluster, job_name=&quot;local&quot;, task_index=1)</div></pre></td></tr></table></figure><h2 id="在模型中指定分布式设备"><a href="#在模型中指定分布式设备" class="headerlink" title="在模型中指定分布式设备"></a>在模型中指定分布式设备</h2><p>在特定设备上定义运算操作，可以使用相同的<code>tf.device</code>函数，用来指定op在CPU或GPU上运行。<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div></pre></td><td class="code"><pre><div class="line">with tf.device(&quot;/job:ps/task:0&quot;):</div><div class="line">  weights_1 = tf.Variable(...)</div><div class="line">  biases_1 = tf.Variable(...)</div><div class="line"></div><div class="line">with tf.device(&quot;/job:ps/task:1&quot;):</div><div class="line">  weights_2 = tf.Variable(...)</div><div class="line">  biases_2 = tf.Variable(...)</div><div class="line"></div><div class="line">with tf.device(&quot;/job:worker/task:7&quot;):</div><div class="line">  input, labels = ...</div><div class="line">  layer_1 = tf.nn.relu(tf.matmul(input, weights_1) + biases_1)</div><div class="line">  logits = tf.nn.relu(tf.matmul(layer_1, weights_2) + biases_2)</div><div class="line">  # ...</div><div class="line">  train_op = ...</div><div class="line"></div><div class="line">with tf.Session(&quot;grpc://worker7.example.com:2222&quot;) as sess:</div><div class="line">  for _ in range(10000):</div><div class="line">    sess.run(train_op)</div></pre></td></tr></table></figure></p><p>在上面的例子中，变量是在<code>ps</code>job中的两个task里被创建，而模型的运算部分是在<code>worker</code>job中被创建。TensorFlow会在jobs之间进行数据传输（从<code>ps</code>传到<code>worker</code>是前向计算，从<code>worker</code>到’ps’是应用梯度）。</p><h2 id="复制训练"><a href="#复制训练" class="headerlink" title="复制训练"></a>复制训练</h2><p>一种常见的训练配置，称为“数据并行”，在<code>worker</code>job的多个tasks中使用不同的mini-batches训练相同的模型，在一个或多个<code>ps</code>job上更新共享的参数。所有的tasks可以运行在不同的机器上。TensorFlow中有多种方法可以定义这样的结构，我们正在构建多个库，将用于简化构建复制的模型的工作。现有的方法包括：</p><ul><li><strong>图内复制（in-graph replication）</strong> 这种方法中，client建立单一的<code>tf.Graph</code>，图中既有创建在<code>/job:ps</code>上的<code>tf.Variable</code>等变量的集合，也有在<code>job：worker</code>上的模型的计算部分的多个副本。</li><li><strong>图间复制（between-graph replication）</strong> 每个<code>/job:worker</code>task有其独立的client，通常运行在worker task相同的进程中。每个client建立包含参数的相似的图（参数原本在<code>/job:ps</code>上，通过使用<code>tf.train.replica_device_setter</code>复制到task）；和在本地task<code>/job:worker</code>上的模型计算部分的一个副本。</li><li><strong>异步训练（asynchronous trainng）</strong> 每个图的副本进行独立的训练，不与其他图协同。它与上述两种复制形式兼容。</li><li><strong>同步训练（synchronous training）</strong> 所有副本读取当前参数的值，并行计算梯度，然后共同应用梯度更新参数。兼容图内复制（如使用平均梯度，<a href="https://github.com/tensorflow/models/tree/master/tutorials/image/cifar10/cifar10_multi_gpu_train.py" target="_blank" rel="external">CIFAR-10 多GPU训练</a>），兼容图间复制（如使用<code>tf.train.SyncReplicasOptimizer</code>)</li></ul><h3 id="训练程序示例"><a href="#训练程序示例" class="headerlink" title="训练程序示例"></a>训练程序示例</h3><p>下面是一个分布式训练程序，实现了<strong>图间复制</strong>和<strong>同步训练</strong>：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div><div class="line">42</div><div class="line">43</div><div class="line">44</div><div class="line">45</div><div class="line">46</div><div class="line">47</div><div class="line">48</div><div class="line">49</div><div class="line">50</div><div class="line">51</div><div class="line">52</div><div class="line">53</div><div class="line">54</div><div class="line">55</div><div class="line">56</div><div class="line">57</div><div class="line">58</div><div class="line">59</div><div class="line">60</div><div class="line">61</div><div class="line">62</div><div class="line">63</div><div class="line">64</div><div class="line">65</div><div class="line">66</div><div class="line">67</div><div class="line">68</div><div class="line">69</div><div class="line">70</div><div class="line">71</div><div class="line">72</div><div class="line">73</div><div class="line">74</div><div class="line">75</div><div class="line">76</div><div class="line">77</div><div class="line">78</div><div class="line">79</div><div class="line">80</div><div class="line">81</div><div class="line">82</div><div class="line">83</div></pre></td><td class="code"><pre><div class="line">import argparse</div><div class="line">import sys</div><div class="line"></div><div class="line">import tensorflow as tf</div><div class="line"></div><div class="line">FLAGS = None</div><div class="line"></div><div class="line">def main(_):</div><div class="line">  ps_hosts = FLAGS.ps_hosts.split(&quot;,&quot;)</div><div class="line">  worker_hosts = FLAGS.worker_hosts.split(&quot;,&quot;)</div><div class="line"></div><div class="line">  # Create a cluster from the parameter server and worker hosts.</div><div class="line">  cluster = tf.train.ClusterSpec(&#123;&quot;ps&quot;: ps_hosts, &quot;worker&quot;: worker_hosts&#125;)</div><div class="line"></div><div class="line">  # Create and start a server for the local task.</div><div class="line">  server = tf.train.Server(cluster,</div><div class="line">                           job_name=FLAGS.job_name,</div><div class="line">                           task_index=FLAGS.task_index)</div><div class="line"></div><div class="line">  if FLAGS.job_name == &quot;ps&quot;:</div><div class="line">    server.join()</div><div class="line">  elif FLAGS.job_name == &quot;worker&quot;:</div><div class="line"></div><div class="line">    # Assigns ops to the local worker by default.</div><div class="line">    with tf.device(tf.train.replica_device_setter(</div><div class="line">        worker_device=&quot;/job:worker/task:%d&quot; % FLAGS.task_index,</div><div class="line">        cluster=cluster)):</div><div class="line"></div><div class="line">      # Build model...</div><div class="line">      loss = ...</div><div class="line">      global_step = tf.contrib.framework.get_or_create_global_step()</div><div class="line"></div><div class="line">      train_op = tf.train.AdagradOptimizer(0.01).minimize(</div><div class="line">          loss, global_step=global_step)</div><div class="line"></div><div class="line">    # The StopAtStepHook handles stopping after running given steps.</div><div class="line">    hooks=[tf.train.StopAtStepHook(last_step=1000000)]</div><div class="line"></div><div class="line">    # The MonitoredTrainingSession takes care of session initialization,</div><div class="line">    # restoring from a checkpoint, saving to a checkpoint, and closing when done</div><div class="line">    # or an error occurs.</div><div class="line">    with tf.train.MonitoredTrainingSession(master=server.target,</div><div class="line">                                           is_chief=(FLAGS.task_index == 0),</div><div class="line">                                           checkpoint_dir=&quot;/tmp/train_logs&quot;,</div><div class="line">                                           hooks=hooks) as mon_sess:</div><div class="line">      while not mon_sess.should_stop():</div><div class="line">        # Run a training step asynchronously.</div><div class="line">        # See `tf.train.SyncReplicasOptimizer` for additional details on how to</div><div class="line">        # perform *synchronous* training.</div><div class="line">        # mon_sess.run handles AbortedError in case of preempted PS.</div><div class="line">        mon_sess.run(train_op)</div><div class="line"></div><div class="line">if __name__ == &quot;__main__&quot;:</div><div class="line">  parser = argparse.ArgumentParser()</div><div class="line">  parser.register(&quot;type&quot;, &quot;bool&quot;, lambda v: v.lower() == &quot;true&quot;)</div><div class="line">  # Flags for defining the tf.train.ClusterSpec</div><div class="line">  parser.add_argument(</div><div class="line">      &quot;--ps_hosts&quot;,</div><div class="line">      type=str,</div><div class="line">      default=&quot;&quot;,</div><div class="line">      help=&quot;Comma-separated list of hostname:port pairs&quot;</div><div class="line">  )</div><div class="line">  parser.add_argument(</div><div class="line">      &quot;--worker_hosts&quot;,</div><div class="line">      type=str,</div><div class="line">      default=&quot;&quot;,</div><div class="line">      help=&quot;Comma-separated list of hostname:port pairs&quot;</div><div class="line">  )</div><div class="line">  parser.add_argument(</div><div class="line">      &quot;--job_name&quot;,</div><div class="line">      type=str,</div><div class="line">      default=&quot;&quot;,</div><div class="line">      help=&quot;One of &apos;ps&apos;, &apos;worker&apos;&quot;</div><div class="line">  )</div><div class="line">  # Flags for defining the tf.train.Server</div><div class="line">  parser.add_argument(</div><div class="line">      &quot;--task_index&quot;,</div><div class="line">      type=int,</div><div class="line">      default=0,</div><div class="line">      help=&quot;Index of task within the job&quot;</div><div class="line">  )</div><div class="line">  FLAGS, unparsed = parser.parse_known_args()</div><div class="line">  tf.app.run(main=main, argv=[sys.argv[0]] + unparsed)</div></pre></td></tr></table></figure></p><p>使用两个servers和两个workers启动训练，执行下面的脚本：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div></pre></td><td class="code"><pre><div class="line"># On ps0.example.com:</div><div class="line">$ python trainer.py \</div><div class="line">     --ps_hosts=ps0.example.com:2222,ps1.example.com:2222 \</div><div class="line">     --worker_hosts=worker0.example.com:2222,worker1.example.com:2222 \</div><div class="line">     --job_name=ps --task_index=0</div><div class="line"># On ps1.example.com:</div><div class="line">$ python trainer.py \</div><div class="line">     --ps_hosts=ps0.example.com:2222,ps1.example.com:2222 \</div><div class="line">     --worker_hosts=worker0.example.com:2222,worker1.example.com:2222 \</div><div class="line">     --job_name=ps --task_index=1</div><div class="line"># On worker0.example.com:</div><div class="line">$ python trainer.py \</div><div class="line">     --ps_hosts=ps0.example.com:2222,ps1.example.com:2222 \</div><div class="line">     --worker_hosts=worker0.example.com:2222,worker1.example.com:2222 \</div><div class="line">     --job_name=worker --task_index=0</div><div class="line"># On worker1.example.com:</div><div class="line">$ python trainer.py \</div><div class="line">     --ps_hosts=ps0.example.com:2222,ps1.example.com:2222 \</div><div class="line">     --worker_hosts=worker0.example.com:2222,worker1.example.com:2222 \</div><div class="line">     --job_name=worker --task_index=1</div></pre></td></tr></table></figure></p><p>##术语</p><h3 id="client"><a href="#client" class="headerlink" title="client"></a>client</h3><p>client指一个程序，构建TensorFlow图和TensorFlow Session与集群进行交互。一个client进程可以与多个TensorFlow服务器进行交互（见上面的章节），一个服务器上也可以运行多个client程序。</p><h3 id="cluster"><a href="#cluster" class="headerlink" title="cluster"></a>cluster</h3><p>一个TensorFlow集群包含一个或多个”jobs”，每个”job“可以分成一个或多个”tasks”。集群通常专用于特定的高级目标，如训练神经网络，并行使用许多机器。一个集群由<code>tf.train.ClusterSpec</code>对象指定。</p><h3 id="job"><a href="#job" class="headerlink" title="job"></a>job</h3><p>一个job由”tasks”列表组成，这些tasks通常用于相同的目的。例如，名为<code>ps</code>（参数服务器 parameter server）的job，通常保存、更新参数的节点；而名为<code>worker</code>的job通常承载承载执行计算密集型任务的无状态节点。一个job中的tasks通常运行在不同机器上。</p><h3 id="task"><a href="#task" class="headerlink" title="task"></a>task</h3><p>一个task代表一个特定的TensorFlow服务器，通常对应一个进程。一个task属于一个job，用该job的tasks列表的下标来区分。</p><h3 id="TensorFlow-server"><a href="#TensorFlow-server" class="headerlink" title="TensorFlow server"></a>TensorFlow server</h3><p>运行<code>tf.train.Server</code>实例的进程，是集群的一个成员。</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;原文地址: &lt;a href=&quot;https://tensorflow.google.cn/deploy/distributed&quot; target=&quot;_blank&quot; rel=&quot;external&quot;&gt;Distributed TensorFlow&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;执行简单的Ten
      
    
    </summary>
    
    
      <category term="tensorflow" scheme="http://libowei.net/tags/tensorflow/"/>
    
      <category term="分布式" scheme="http://libowei.net/tags/%E5%88%86%E5%B8%83%E5%BC%8F/"/>
    
  </entry>
  
  <entry>
    <title>TensorFlow运行多个计算图</title>
    <link href="http://libowei.net/TensorFlow%E8%BF%90%E8%A1%8C%E5%A4%9A%E4%B8%AA%E8%AE%A1%E7%AE%97%E5%9B%BE.html"/>
    <id>http://libowei.net/TensorFlow运行多个计算图.html</id>
    <published>2017-11-08T01:24:17.000Z</published>
    <updated>2017-11-23T04:13:13.012Z</updated>
    
    <content type="html"><![CDATA[<p>有这样的需求：在同一个程序中需要运行两个训练好的TensorFlow模型。<br>如果仅仅是把两个模型放到两个类中，简单的把两个类的代码组合到一起，会出现问题。</p><p>这是由于TensorFlow的运行基于计算图(Graph)，在代码中不明确指定当前所用的图的情况下，TensorFlow会自动创建一个图，作为当前的默认图。创建的所有Tensor和计算(op)都会添加到当前默认图中。</p><p>而TensorFlow的会话(Session)也是需要基于图来创建，<code>tf.Session()</code>可以通过<code>graph</code>参数来指定Session运行于哪个图上。</p><p>当在不明确指定图的情况下，创建两个模型时，两个模型会在同一张图中，可能会出现变量名相同、变量形状不匹配、Session不能启动等问题。</p><p>解决方法是在每个模型中自行定义所使用的图，并在向图中添加Tensor和op时将该模型的图作为默认图，在启动Session时指定运行图为当前的图。<br>例如两个模型分别封装在两个类中</p><ol><li>在创建类时，定义变量<code>self.graph = tf.Graph()</code></li><li>在定义网络结构时，在每个Tensor和op前添加<code>with self.graph.as_default():</code></li><li>运行Session时，传入当前的图：<code>self.sess = tf.Session(graph=self.graph)</code></li></ol><p>这样，定义模型结构和运行会话都分别在两个图中进行，不会产生冲突。</p><p>参考资料：</p><ol><li><a href="http://blog.csdn.net/xierhacker/article/details/53860379" target="_blank" rel="external">http://blog.csdn.net/xierhacker/article/details/53860379</a></li><li><a href="https://stackoverflow.com/questions/42593771/session-graph-is-empty" target="_blank" rel="external">https://stackoverflow.com/questions/42593771/session-graph-is-empty</a></li></ol>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;有这样的需求：在同一个程序中需要运行两个训练好的TensorFlow模型。&lt;br&gt;如果仅仅是把两个模型放到两个类中，简单的把两个类的代码组合到一起，会出现问题。&lt;/p&gt;
&lt;p&gt;这是由于TensorFlow的运行基于计算图(Graph)，在代码中不明确指定当前所用的图的情况下
      
    
    </summary>
    
    
      <category term="tensorflow" scheme="http://libowei.net/tags/tensorflow/"/>
    
  </entry>
  
  <entry>
    <title>TensorFlow DataSet API的一个问题</title>
    <link href="http://libowei.net/TensorFlow-DataSet-API%E7%9A%84%E4%B8%80%E4%B8%AA%E9%97%AE%E9%A2%98.html"/>
    <id>http://libowei.net/TensorFlow-DataSet-API的一个问题.html</id>
    <published>2017-11-01T02:06:48.000Z</published>
    <updated>2017-11-23T04:12:39.348Z</updated>
    
    <content type="html"><![CDATA[<p>在使用TensorFlow新的DataSet API时遇到了一个坑。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line">batched_dataset = dataset.batch(4)</div><div class="line">iterator = batched_dataset.make_one_shot_iterator()</div><div class="line">next_element = iterator.get_next()</div></pre></td></tr></table></figure><p><code>DataSet.batch(batch_size)</code>会返回一个第一个维度为<code>batch_size</code>大小的Tensor。但当不断调用<code>interator.get_next()</code>，遍历到样本集的末端时，返回的Tensor的大小可能不为<code>batch_size</code>。</p><p>例如样本集中有17个元素，<code>batch_size</code>为4时，第5次调用<code>iterator.get_next()</code>只能返回大小为1的batch。</p><p><code>DataSet.repeat()</code>会返回一个无限重复原数据集的DataSet对象，当调用了repeat()时，也会产生上面的问题，有问题代码如下：<br><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div></pre></td><td class="code"><pre><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">read_batch</span><span class="params">(self, batch_size=None, shuffle_buffer=None, repeat=None)</span>:</span></div><div class="line">    dataset = tf.contrib.data.TFRecordDataset(self._files)</div><div class="line">    dataset = dataset.map(self.__parse_function)</div><div class="line">    <span class="keyword">if</span> batch_size:</div><div class="line">        dataset = dataset.batch(batch_size)</div><div class="line">    <span class="keyword">if</span> shuffle_buffer:</div><div class="line">        dataset = dataset.shuffle(buffer_size=shuffle_buffer)</div><div class="line">    <span class="keyword">if</span> repeat != <span class="number">0</span>:</div><div class="line">        dataset = dataset.repeat(repeat)</div><div class="line">    iterator = dataset.make_one_shot_iterator()</div><div class="line">    batch_data = iterator.get_next()</div><div class="line">    <span class="keyword">return</span> self._extract_batch(batch_data)</div></pre></td></tr></table></figure></p><p>在遍历数据集的最后一个batch的大小不为<code>batch_size</code>。</p><p>在stackoverflow找到了类似的问题：<a href="https://stackoverflow.com/questions/46708822/returned-size-of-tensorflows-dataset-api-is-not-constant" target="_blank" rel="external">returned size of tensorflow’s dataset API is not constant</a></p><p>解决方法为：在<code>batch()</code>之前调用<code>repeat()</code>  …… ……</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div></pre></td><td class="code"><pre><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">read_batch</span><span class="params">(self, batch_size=None, shuffle_buffer=None, repeat=None)</span>:</span></div><div class="line">    dataset = tf.contrib.data.TFRecordDataset(self._files)</div><div class="line">    dataset = dataset.map(self.__parse_function)</div><div class="line">    <span class="keyword">if</span> shuffle_buffer:</div><div class="line">        dataset = dataset.shuffle(buffer_size=shuffle_buffer)</div><div class="line">    <span class="keyword">if</span> repeat != <span class="number">0</span>:</div><div class="line">        dataset = dataset.repeat(repeat)</div><div class="line">    <span class="keyword">if</span> batch_size:</div><div class="line">        dataset = dataset.batch(batch_size)</div><div class="line">    iterator = dataset.make_one_shot_iterator()</div><div class="line">    batch_data = iterator.get_next()</div><div class="line">    <span class="keyword">return</span> self._extract_batch(batch_data)</div></pre></td></tr></table></figure>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;在使用TensorFlow新的DataSet API时遇到了一个坑。&lt;/p&gt;
&lt;figure class=&quot;highlight plain&quot;&gt;&lt;table&gt;&lt;tr&gt;&lt;td class=&quot;gutter&quot;&gt;&lt;pre&gt;&lt;div class=&quot;line&quot;&gt;1&lt;/div&gt;&lt;div c
      
    
    </summary>
    
    
      <category term="tensorflow" scheme="http://libowei.net/tags/tensorflow/"/>
    
  </entry>
  
  <entry>
    <title>TensorFlow学习笔记：Importing Data</title>
    <link href="http://libowei.net/TensorFlow%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%EF%BC%9AImporting-Data.html"/>
    <id>http://libowei.net/TensorFlow学习笔记：Importing-Data.html</id>
    <published>2017-09-26T06:08:44.000Z</published>
    <updated>2017-11-23T04:13:05.630Z</updated>
    
    <content type="html"><![CDATA[<p><code>Dataset</code>API可使你从简单、可重用的片段构建复杂的输入管道。例如，一个图像模型的输入管道可能从分布式的系统中聚合数据，对每个图像进行随机扰动，并随机选择一批图像用于训练；文本模型的输入管道可能从原始的文本中提取特征，利用查找表将其转化为embedding identifiers，并将不同长度的序列组合在一起。<code>Dataset</code>API可以很容易的处理大量数据、不同数据格式和复杂的变换。</p><p><code>Dataset</code>API引入了两个抽象概念：</p><ul><li>一个<code>tf.contrib.data.Dataset</code>代表一个元素序列，每个元素包含一个或多个Tensor对象。例如图像管道中，一个元素可能是包含一对表示图像数据和类别标签的Tensor的单一训练样本。有两种创建dataset的方式：<ul><li>创建<strong>source</strong>（如<code>Dataset.from_tensor_slices()</code>），由一个或多个<code>tf.Tensor</code>对象构建一个dataset。</li><li>应用<strong>transformation</strong>（如<code>Dataset.batch()</code>），从一个或多个<code>tf.contrib.data.Dataset</code>对象中创建新dataset。</li></ul></li><li>一个<code>tf.contrib.data.Iterator</code>提供从dataset提取元素的方法。<code>Iterator.get_next()</code>返回的op执行时产生<code>Dataset</code>的下一个元素，这通常作为输入管道和模型之间的接口。最简单的迭代器是”one-shot 迭代器”，它与一个特定的<code>Dataset</code>相关联，作一次性遍历。<code>Iterator.initializer</code>op能够让你用不同的数据集再初始化和参数化一个迭代器，这样你可以在相同的程序中多次迭代训练集/验证集数据。</li></ul><h2 id="基本机制"><a href="#基本机制" class="headerlink" title="基本机制"></a>基本机制</h2><p>这一节讲了创建不同种类的<code>Dataset</code>和<code>Iterator</code>对象的基本方法，和如何从其中提取数据。</p><p>启动输入管道前，必须定义一个<em>source</em>。例如，从内存中的一些Tensor中创建一个<code>Dataset</code>，可使用<code>tf.contrib.data.Dataset.from_tensors()</code>或<code>tf.contrib.data.Dataset.from_tensor_slices()</code>。如果你的输入数据来自磁盘中的TFRecord，使用<code>tf.contrib.data.TFRecordDataset</code>。</p><p>一旦有了<code>Dataset</code>对象，可以<em>transform</em>它，调用链接方法生成新的<code>Dataset</code>。例如，可以应用每元变换（per-element transformations）如<code>Dataset.map()</code>，和多元变换（multi-element transformations）如<code>Dataset.batch()</code>。查看<a href="https://www.tensorflow.org/api_docs/python/tf/contrib/data/Dataset" target="_blank" rel="external"><code>tf.contrib.data.Dataset</code>文档</a>了解所有的变换方法。</p><p>从<code>Dataset</code>中取值最常见的方法是通过<strong>iterator</strong>对象每次获取数据集中的一个元素（如调用<code>Dataset.make_one_shot_iterator</code>）。<code>tf.contrib.data.Iterator</code>对象提供两种运算：<code>Iterator.initializer</code>可（再）初始化迭代器的状态；<code>Iterator.get_next()</code>返回下一个元素的Tensor对象。取决于你的应用，可以选择不同类型的迭代器，概述如下。</p><h3 id="数据集结构"><a href="#数据集结构" class="headerlink" title="数据集结构"></a>数据集结构</h3><p>一个dataset包含的每个元素都有相同的结构。每个元素包含一个或多个<code>tf.Tensor</code>对象，称为<em>component</em>。每个component的<code>tf.DType</code>代表tensor中元素的类型，<code>tf.TensorShape</code>代表每个元素的固定的形状（可能只指定一部分维度）。<code>Dataset.output_types</code>和<code>Dataset.output_shapes</code>属性使你能对推断出的数据集元素的类型和形状进行检查。这种属性的嵌套结构对应元素的结构，元素可以是单个Tensor，Tensor元组或嵌套的Tensor元组，例如：<br><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div></pre></td><td class="code"><pre><div class="line">dataset1 = tf.contrib.data.Dataset.from_tensor_slices(tf.random_uniform([<span class="number">4</span>, <span class="number">10</span>]))</div><div class="line">print(dataset1.output_types)  <span class="comment"># ==&gt; "tf.float32"</span></div><div class="line">print(dataset1.output_shapes)  <span class="comment"># ==&gt; "(10,)"</span></div><div class="line"></div><div class="line">dataset2 = tf.contrib.data.Dataset.from_tensor_slices(</div><div class="line">   (tf.random_uniform([<span class="number">4</span>]),</div><div class="line">    tf.random_uniform([<span class="number">4</span>, <span class="number">100</span>], maxval=<span class="number">100</span>, dtype=tf.int32)))</div><div class="line">print(dataset2.output_types)  <span class="comment"># ==&gt; "(tf.float32, tf.int32)"</span></div><div class="line">print(dataset2.output_shapes)  <span class="comment"># ==&gt; "((), (100,))"</span></div><div class="line"></div><div class="line">dataset3 = tf.contrib.data.Dataset.zip((dataset1, dataset2))</div><div class="line">print(dataset3.output_types)  <span class="comment"># ==&gt; (tf.float32, (tf.float32, tf.int32))</span></div><div class="line">print(dataset3.output_shapes)  <span class="comment"># ==&gt; "(10, ((), (100,)))"</span></div></pre></td></tr></table></figure></p><p>给元素的每个component命名很方便，如果不同的component表示训练样本的不同的特征。除了使用元组之外，还可以用<code>collections.namedtuple</code>或一个{string : tensor}的字典表示数据集的一个元素。<br><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div></pre></td><td class="code"><pre><div class="line">dataset = tf.contrib.data.Dataset.from_tensor_slices(</div><div class="line">   &#123;<span class="string">"a"</span>: tf.random_uniform([<span class="number">4</span>]),</div><div class="line">    <span class="string">"b"</span>: tf.random_uniform([<span class="number">4</span>, <span class="number">100</span>], maxval=<span class="number">100</span>, dtype=tf.int32)&#125;)</div><div class="line">print(dataset.output_types)  <span class="comment"># ==&gt; "&#123;'a': tf.float32, 'b': tf.int32&#125;"</span></div><div class="line">print(dataset.output_shapes)  <span class="comment"># ==&gt; "&#123;'a': (), 'b': (100,)&#125;"</span></div></pre></td></tr></table></figure></p><p><code>Dataset</code>变换支持任何结构的数据集。当使用<code>Dataset.map()</code>，<code>Dataset.flat_map()</code>和<code>Dataset.filter()</code>变换时，需要根据元素的结构来定义这些函数的参数：<br><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div></pre></td><td class="code"><pre><div class="line">dataset1 = dataset1.map(<span class="keyword">lambda</span> x: ...)</div><div class="line"></div><div class="line">dataset2 = dataset2.flat_map(<span class="keyword">lambda</span> x, y: ...)</div><div class="line"></div><div class="line"><span class="comment"># Note: Argument destructuring is not available in Python 3.</span></div><div class="line">dataset3 = dataset3.filter(<span class="keyword">lambda</span> x, (y, z): ...)</div></pre></td></tr></table></figure></p><h3 id="创建迭代器"><a href="#创建迭代器" class="headerlink" title="创建迭代器"></a>创建迭代器</h3><p>你已经创建了表示输入数据的<code>Dataset</code>，下一步就是创建一个<code>Iterator</code>来获取数据集中的元素。<code>Dataset</code>API目前支持三种迭代器：</p><ul><li><strong>one-shot</strong></li><li><strong>initializable</strong></li><li><strong>reinitializable</strong> 和 <strong>feedable</strong></li></ul><p><strong>one shot</strong> 迭代器是最简单的形式，只支持一次性遍历数据集，不需要显式的初始化操作。当前绝大多数基于队列的输入管道所支持的情况它都能处理，但不支持参数化。<br><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div></pre></td><td class="code"><pre><div class="line">dataset = tf.contrib.data.Dataset.range(<span class="number">100</span>)</div><div class="line">iterator = dataset.make_one_shot_iterator()</div><div class="line">next_element = iterator.get_next()</div><div class="line"></div><div class="line"><span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">100</span>):</div><div class="line">  value = sess.run(next_element)</div><div class="line">  <span class="keyword">assert</span> i == value</div></pre></td></tr></table></figure></p><p><strong>initializable</strong> 迭代器需要在使用前显示的定义<code>iterator.initializer</code>。可以根据数据集对迭代器操作进行参数化，可在初始化时使用<code>tf.placeholder()</code>Tensor作为feed。<br><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div></pre></td><td class="code"><pre><div class="line">max_value = tf.placeholder(tf.int64, shape=[])</div><div class="line">dataset = tf.contrib.data.Dataset.range(max_value)</div><div class="line">iterator = dataset.make_initializable_iterator()</div><div class="line">next_element = iterator.get_next()</div><div class="line"></div><div class="line"><span class="comment"># Initialize an iterator over a dataset with 10 elements.</span></div><div class="line">sess.run(iterator.initializer, feed_dict=&#123;max_value: <span class="number">10</span>&#125;)</div><div class="line"><span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">10</span>):</div><div class="line">  value = sess.run(next_element)</div><div class="line">  <span class="keyword">assert</span> i == value</div><div class="line"></div><div class="line"><span class="comment"># Initialize the same iterator over a dataset with 100 elements.</span></div><div class="line">sess.run(iterator.initializer, feed_dict=&#123;max_value: <span class="number">100</span>&#125;)</div><div class="line"><span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">100</span>):</div><div class="line">  value = sess.run(next_element)</div><div class="line">  <span class="keyword">assert</span> i == value</div></pre></td></tr></table></figure></p><p><strong>reinitializable</strong> 迭代器可以以多个不同的<code>Dataset</code>对象进行初始化。例如，你可能有一个这样的训练集输入管道：对输入的图像进行随机扰动，和用于在原始图像进行评估的验证集输入管道。这两个管道使用不同的<code>Dataset</code>对象，但是它们的结构是相同的（每个component有相同类型和兼容的形状）。<br><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div></pre></td><td class="code"><pre><div class="line"><span class="comment"># Define training and validation datasets with the same structure.</span></div><div class="line">training_dataset = tf.contrib.data.Dataset.range(<span class="number">100</span>).map(</div><div class="line">    <span class="keyword">lambda</span> x: x + tf.random_uniform([], <span class="number">-10</span>, <span class="number">10</span>, tf.int64))</div><div class="line">validation_dataset = tf.contrib.data.Dataset.range(<span class="number">50</span>)</div><div class="line"></div><div class="line"><span class="comment"># A reinitializable iterator is defined by its structure. We could use the</span></div><div class="line"><span class="comment"># `output_types` and `output_shapes` properties of either `training_dataset`</span></div><div class="line"><span class="comment"># or `validation_dataset` here, because they are compatible.</span></div><div class="line">iterator = Iterator.from_structure(training_dataset.output_types,</div><div class="line">                                   training_dataset.output_shapes)</div><div class="line">next_element = iterator.get_next()</div><div class="line"></div><div class="line">training_init_op = iterator.make_initializer(training_dataset)</div><div class="line">validation_init_op = iterator.make_initializer(validation_dataset)</div><div class="line"></div><div class="line"><span class="comment"># Run 20 epochs in which the training dataset is traversed, followed by the</span></div><div class="line"><span class="comment"># validation dataset.</span></div><div class="line"><span class="keyword">for</span> _ <span class="keyword">in</span> range(<span class="number">20</span>):</div><div class="line">  <span class="comment"># Initialize an iterator over the training dataset.</span></div><div class="line">  sess.run(training_init_op)</div><div class="line">  <span class="keyword">for</span> _ <span class="keyword">in</span> range(<span class="number">100</span>):</div><div class="line">    sess.run(next_element)</div><div class="line"></div><div class="line">  <span class="comment"># Initialize an iterator over the validation dataset.</span></div><div class="line">  sess.run(validation_init_op)</div><div class="line">  <span class="keyword">for</span> _ <span class="keyword">in</span> range(<span class="number">50</span>):</div><div class="line">    sess.run(next_element)</div></pre></td></tr></table></figure></p><p><strong>feedable</strong> 迭代器可以在调用<code>tf.Session.run</code>时和<code>tf.placeholder</code>一起使用，通过<code>feed_dict</code>机制，以选择使用哪种<code>Iterator</code>。它与reinitializable迭代器有相同的功能，但在你切换迭代器时不需要从数据集的开始初始化迭代器。举例：使用和上面例子相同的训练和验证样本，可以使用<code>tf.contrib.data.Iterator.from_string_handle</code>来定义一个feedable迭代器，用来在两个数据集间切换。<br><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div></pre></td><td class="code"><pre><div class="line"><span class="comment"># Define training and validation datasets with the same structure.</span></div><div class="line">training_dataset = tf.contrib.data.Dataset.range(<span class="number">100</span>).map(</div><div class="line">    <span class="keyword">lambda</span> x: x + tf.random_uniform([], <span class="number">-10</span>, <span class="number">10</span>, tf.int64)).repeat()</div><div class="line">validation_dataset = tf.contrib.data.Dataset.range(<span class="number">50</span>)</div><div class="line"></div><div class="line"><span class="comment"># A feedable iterator is defined by a handle placeholder and its structure. We</span></div><div class="line"><span class="comment"># could use the `output_types` and `output_shapes` properties of either</span></div><div class="line"><span class="comment"># `training_dataset` or `validation_dataset` here, because they have</span></div><div class="line"><span class="comment"># identical structure.</span></div><div class="line">handle = tf.placeholder(tf.string, shape=[])</div><div class="line">iterator = tf.contrib.data.Iterator.from_string_handle(</div><div class="line">    handle, training_dataset.output_types, training_dataset.output_shapes)</div><div class="line">next_element = iterator.get_next()</div><div class="line"></div><div class="line"><span class="comment"># You can use feedable iterators with a variety of different kinds of iterator</span></div><div class="line"><span class="comment"># (such as one-shot and initializable iterators).</span></div><div class="line">training_iterator = training_dataset.make_one_shot_iterator()</div><div class="line">validation_iterator = validation_dataset.make_initializable_iterator()</div><div class="line"></div><div class="line"><span class="comment"># The `Iterator.string_handle()` method returns a tensor that can be evaluated</span></div><div class="line"><span class="comment"># and used to feed the `handle` placeholder.</span></div><div class="line">training_handle = sess.run(training_iterator.string_handle())</div><div class="line">validation_handle = sess.run(validation_iterator.string_handle())</div><div class="line"></div><div class="line"><span class="comment"># Loop forever, alternating between training and validation.</span></div><div class="line"><span class="keyword">while</span> <span class="keyword">True</span>:</div><div class="line">  <span class="comment"># Run 200 steps using the training dataset. Note that the training dataset is</span></div><div class="line">  <span class="comment"># infinite, and we resume from where we left off in the previous `while` loop</span></div><div class="line">  <span class="comment"># iteration.</span></div><div class="line">  <span class="keyword">for</span> _ <span class="keyword">in</span> range(<span class="number">200</span>):</div><div class="line">    sess.run(next_element, feed_dict=&#123;handle: training_handle&#125;)</div><div class="line"></div><div class="line">  <span class="comment"># Run one pass over the validation dataset.</span></div><div class="line">  sess.run(validation_iterator.initializer)</div><div class="line">  <span class="keyword">for</span> _ <span class="keyword">in</span> range(<span class="number">50</span>):</div><div class="line">    sess.run(next_element, feed_dict=&#123;handle: validation_handle&#125;)</div></pre></td></tr></table></figure></p><h3 id="从迭代器中取值"><a href="#从迭代器中取值" class="headerlink" title="从迭代器中取值"></a>从迭代器中取值</h3><p><code>Iterator.get_next()</code>方法返回一个或多个Tensor对象，对应迭代器的下一个元素。每次这些Tensor被求值时，它们从底层的数据集中取下一个元素的值。（注意：同其他具有状态的对象相似，调用<code>Iterator.get_next()</code>不会立即执行，你必须在<code>tf.Session.run</code>中传递其返回的Tensor对象的相应表达式，才会取得下一个元素并推进迭代器。）</p><p>如果迭代器到达数据集的末端，执行<code>Iterator.get_next()</code>会产生<code>tf.errors.OutOfRangeError</code>。之后迭代器会变成不可用状态，如果想再次使用必须对其重新初始化。<br><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div></pre></td><td class="code"><pre><div class="line">dataset = tf.contrib.data.Dataset.range(<span class="number">5</span>)</div><div class="line">iterator = dataset.make_initializable_iterator()</div><div class="line">next_element = iterator.get_next()</div><div class="line"></div><div class="line"><span class="comment"># Typically `result` will be the output of a model, or an optimizer's</span></div><div class="line"><span class="comment"># training operation.</span></div><div class="line">result = tf.add(next_element, next_element)</div><div class="line"></div><div class="line">sess.run(iterator.initializer)</div><div class="line">print(sess.run(result))  <span class="comment"># ==&gt; "0"</span></div><div class="line">print(sess.run(result))  <span class="comment"># ==&gt; "2"</span></div><div class="line">print(sess.run(result))  <span class="comment"># ==&gt; "4"</span></div><div class="line">print(sess.run(result))  <span class="comment"># ==&gt; "6"</span></div><div class="line">print(sess.run(result))  <span class="comment"># ==&gt; "8"</span></div><div class="line"><span class="keyword">try</span>:</div><div class="line">  sess.run(result)</div><div class="line"><span class="keyword">except</span> tf.errors.OutOfRangeError:</div><div class="line">  print(<span class="string">"End of dataset"</span>)  <span class="comment"># ==&gt; "End of dataset"</span></div></pre></td></tr></table></figure></p><p>常见的形式是在<code>try-except</code>里进行训练循环：<br><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div></pre></td><td class="code"><pre><div class="line">sess.run(iterator.initializer)</div><div class="line"><span class="keyword">while</span> <span class="keyword">True</span>:</div><div class="line">  <span class="keyword">try</span>:</div><div class="line">    sess.run(result)</div><div class="line">  <span class="keyword">except</span> tf.errors.OutOfRangeError:</div><div class="line">    <span class="keyword">break</span></div></pre></td></tr></table></figure></p><p>如果数据集中的每个元素都是嵌套结构，<code>Iterator.get_next()</code>返回的一个或多个Tensor也是这种相同的嵌套结构：<br><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div></pre></td><td class="code"><pre><div class="line">dataset1 = tf.contrib.data.Dataset.from_tensor_slices(tf.random_uniform([<span class="number">4</span>, <span class="number">10</span>]))</div><div class="line">dataset2 = tf.contrib.data.Dataset.from_tensor_slices((tf.random_uniform([<span class="number">4</span>]), tf.random_uniform([<span class="number">4</span>, <span class="number">100</span>])))</div><div class="line">dataset3 = tf.contrib.data.Dataset.zip((dataset1, dataset2))</div><div class="line"></div><div class="line">iterator = dataset3.make_initializable_iterator()</div><div class="line"></div><div class="line">sess.run(iterator.initializer)</div><div class="line">next1, (next2, next3) = iterator.get_next()</div></pre></td></tr></table></figure></p><p>注意对<code>next1</code>,<code>next2</code>或<code>next3</code>任意一个求值都会推进迭代器，典型的迭代器取值的做法是在一个表达式中包含其所有component。</p><h2 id="读输入数据"><a href="#读输入数据" class="headerlink" title="读输入数据"></a>读输入数据</h2><h3 id="Numpy-数组"><a href="#Numpy-数组" class="headerlink" title="Numpy 数组"></a>Numpy 数组</h3><p>如果全部输入数据都在内存中，最简单的方式是由其创建<code>Dataset</code>，转化为<code>tf.Tensor</code>，使用<code>Dataset.from_tensor_slices()</code><br><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div></pre></td><td class="code"><pre><div class="line"><span class="comment"># Load the training data into two NumPy arrays, for example using `np.load()`.</span></div><div class="line"><span class="keyword">with</span> np.load(<span class="string">"/var/data/training_data.npy"</span>) <span class="keyword">as</span> data:</div><div class="line">  features = data[<span class="string">"features"</span>]</div><div class="line">  labels = data[<span class="string">"labels"</span>]</div><div class="line"></div><div class="line"><span class="comment"># Assume that each row of `features` corresponds to the same row as `labels`.</span></div><div class="line"><span class="keyword">assert</span> features.shape[<span class="number">0</span>] == labels.shape[<span class="number">0</span>]</div><div class="line"></div><div class="line">dataset = tf.contrib.data.Dataset.from_tensor_slices((features, labels))</div></pre></td></tr></table></figure></p><p>上面的代码会以<code>tf.constant()</code>op在TensorFlow计算图种表示<code>features</code>和<code>labels</code>数组。这适合小的数据集，但是很浪费内存，因为数组的内容会被复制多次，可能会超出<code>tf.GraphDef</code>定义的protocol buffer的2GB大小限制。</p><p>作为替代方案，可以以<code>tf.placeholder()</code>Tensor的形式定义<code>Dataset</code>，在迭代器初始化时 <em>feed</em> Numpy 数组。<br><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div></pre></td><td class="code"><pre><div class="line"><span class="comment"># Load the training data into two NumPy arrays, for example using `np.load()`.</span></div><div class="line"><span class="keyword">with</span> np.load(<span class="string">"/var/data/training_data.npy"</span>) <span class="keyword">as</span> data:</div><div class="line">  features = data[<span class="string">"features"</span>]</div><div class="line">  labels = data[<span class="string">"labels"</span>]</div><div class="line"></div><div class="line"><span class="comment"># Assume that each row of `features` corresponds to the same row as `labels`.</span></div><div class="line"><span class="keyword">assert</span> features.shape[<span class="number">0</span>] == labels.shape[<span class="number">0</span>]</div><div class="line"></div><div class="line">features_placeholder = tf.placeholder(features.dtype, features.shape)</div><div class="line">labels_placeholder = tf.placeholder(labels.dtype, labels.shape)</div><div class="line"></div><div class="line">dataset = tf.contrib.data.Dataset.from_tensor_slices((features_placeholder, labels_placeholder))</div><div class="line"><span class="comment"># [Other transformations on `dataset`...]</span></div><div class="line">dataset = ...</div><div class="line">iterator = dataset.make_initializable_iterator()</div><div class="line"></div><div class="line">sess.run(iterator.initializer, feed_dict=&#123;features_placeholder: features,</div><div class="line">                                          labels_placeholder: labels&#125;)</div></pre></td></tr></table></figure></p><h3 id="TFRecord-数据"><a href="#TFRecord-数据" class="headerlink" title="TFRecord 数据"></a>TFRecord 数据</h3><p><code>Dataset</code>API支持多种文件格式类型，所以你可以处理不能完整读入内存的大的数据集。以TFRecord为例，TFRecord是一种面向记录的二进制文件，很多TensorFlow应用使用它作为训练数据。<code>tf.contrib.data.TFRecordDataset</code>能够使TFRecord文件作为输入管道的输入流。<br><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line"><span class="comment"># Creates a dataset that reads all of the examples from two files.</span></div><div class="line">filenames = [<span class="string">"/var/data/file1.tfrecord"</span>, <span class="string">"/var/data/file2.tfrecord"</span>]</div><div class="line">dataset = tf.contrib.data.TFRecordDataset(filenames)</div></pre></td></tr></table></figure></p><p>传递给<code>TFRecordDataset</code>的参数<code>filenames</code>可以是字符串，字符串列表或<code>tf.Tensor</code>类型的字符串。因此，如果有两组文件分别作为训练和验证，可以使用<code>tf.placeholder(tf.string)</code>来表示文件名，使用适当的文件名来初始化一个迭代器。<br><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div></pre></td><td class="code"><pre><div class="line">filenames = tf.placeholder(tf.string, shape=[<span class="keyword">None</span>])</div><div class="line">dataset = tf.contrib.data.TFRecordDataset(filenames)</div><div class="line">dataset = dataset.map(...)  <span class="comment"># Parse the record into tensors.</span></div><div class="line">dataset = dataset.repeat()  <span class="comment"># Repeat the input indefinitely.</span></div><div class="line">dataset = dataset.batch(<span class="number">32</span>)</div><div class="line">iterator = dataset.make_initializable_iterator()</div><div class="line"></div><div class="line"><span class="comment"># You can feed the initializer with the appropriate filenames for the current</span></div><div class="line"><span class="comment"># phase of execution, e.g. training vs. validation.</span></div><div class="line"></div><div class="line"><span class="comment"># Initialize `iterator` with training data.</span></div><div class="line">training_filenames = [<span class="string">"/var/data/file1.tfrecord"</span>, <span class="string">"/var/data/file2.tfrecord"</span>]</div><div class="line">sess.run(iterator.initializer, feed_dict=&#123;filenames: training_filenames&#125;)</div><div class="line"></div><div class="line"><span class="comment"># Initialize `iterator` with validation data.</span></div><div class="line">validation_filenames = [<span class="string">"/var/data/validation1.tfrecord"</span>, ...]</div><div class="line">sess.run(iterator.initializer, feed_dict=&#123;filenames: validation_filenames&#125;)</div></pre></td></tr></table></figure></p><p>###文本数据<br>很多数据集是一个或多个文本文件。<code>tf.contrib.data.TextLineDataset</code>提供了一种简单的方式来提取这些文件的每一行。给定一个或多个文件名，<code>TextLineDataset</code>会对这些文件的每行生成一个值为字符串的元素。<code>TextLineDataset</code>也可以接受<code>tf.Tensor</code>作为<code>filenames</code>，所以你可以传递一个<code>tf.placeholder(tf.string)</code>作为参数。<br><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">filenames = [<span class="string">"/var/data/file1.txt"</span>, <span class="string">"/var/data/file2.txt"</span>]</div><div class="line">dataset = tf.contrib.data.TextLineDataset(filenames)</div></pre></td></tr></table></figure></p><p>默认下，<code>TextLineDataset</code>生成每个文件中的每一行，这可能不是你所需要的，例如文件中有标题行，或包含注释。可以使用<code>Dataset.skip()</code>和<code>Dataset.filter()</code>来剔除这些行。为了对每个文件都各自应用这些变换，使用<code>Dataset.flat_map()</code>来对每个文件创建一个嵌套的<code>Dataset</code><br><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div></pre></td><td class="code"><pre><div class="line">filenames = [<span class="string">"/var/data/file1.txt"</span>, <span class="string">"/var/data/file2.txt"</span>]</div><div class="line"></div><div class="line">dataset = tf.contrib.data.Dataset.from_tensor_slices(filenames)</div><div class="line"></div><div class="line"><span class="comment"># Use `Dataset.flat_map()` to transform each file as a separate nested dataset,</span></div><div class="line"><span class="comment"># and then concatenate their contents sequentially into a single "flat" dataset.</span></div><div class="line"><span class="comment"># * Skip the first line (header row).</span></div><div class="line"><span class="comment"># * Filter out lines beginning with "#" (comments).</span></div><div class="line">dataset = dataset.flat_map(</div><div class="line">    <span class="keyword">lambda</span> filename: (</div><div class="line">        tf.contrib.data.TextLineDataset(filename)</div><div class="line">        .skip(<span class="number">1</span>)</div><div class="line">        .filter(<span class="keyword">lambda</span> line: tf.not_equal(tf.substr(line, <span class="number">0</span>, <span class="number">1</span>), <span class="string">"#"</span>))))</div></pre></td></tr></table></figure></p><h2 id="使用Dataset-map-进行数据预处理"><a href="#使用Dataset-map-进行数据预处理" class="headerlink" title="使用Dataset.map()进行数据预处理"></a>使用Dataset.map()进行数据预处理</h2><p><code>Dataset.map(f)</code>变换对每个元素应用给定的函数<code>f</code>生成一个新的dataset。这个函数基于函数式编程中通用的<code>map()</code>函数，应用于列表型的数据结构。函数<code>f</code>接受代表单个元素的<code>tf.Tensor</code>作为输入，返回<code>tf.Tensor</code>对象作为新数据集中的元素。在实现上，使用了标准的TensorFlow op将一个元素转换为另一个。</p><p>这一节介绍了使用<code>Dataset.map()</code>的例子。</p><h3 id="解析tf-Example-protocol-buffer"><a href="#解析tf-Example-protocol-buffer" class="headerlink" title="解析tf.Example protocol buffer"></a>解析tf.Example protocol buffer</h3><p>很多输入管道从TFRecord格式的文件中提取<code>tf.train.Example</code> protocol buffer。每个<code>tf.train.Example</code>记录包含一个或多个features，输入管道把这些featrues转换成Tensor。<br><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div></pre></td><td class="code"><pre><div class="line"><span class="comment"># Transforms a scalar string `example_proto` into a pair of a scalar string and</span></div><div class="line"><span class="comment"># a scalar integer, representing an image and its label, respectively.</span></div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">_parse_function</span><span class="params">(example_proto)</span>:</span></div><div class="line">  features = &#123;<span class="string">"image"</span>: tf.FixedLenFeature((), tf.string, default_value=<span class="string">""</span>),</div><div class="line">              <span class="string">"label"</span>: tf.FixedLenFeature((), tf.int32, default_value=<span class="number">0</span>)&#125;</div><div class="line">  parsed_features = tf.parse_single_example(example_proto, features)</div><div class="line">  <span class="keyword">return</span> parsed_features[<span class="string">"image"</span>], parsed_features[<span class="string">"label"</span>]</div><div class="line"></div><div class="line"><span class="comment"># Creates a dataset that reads all of the examples from two files, and extracts</span></div><div class="line"><span class="comment"># the image and label features.</span></div><div class="line">filenames = [<span class="string">"/var/data/file1.tfrecord"</span>, <span class="string">"/var/data/file2.tfrecord"</span>]</div><div class="line">dataset = tf.contrib.data.TFRecordDataset(filenames)</div><div class="line">dataset = dataset.map(_parse_function)</div></pre></td></tr></table></figure></p><h3 id="解码图像、调整大小"><a href="#解码图像、调整大小" class="headerlink" title="解码图像、调整大小"></a>解码图像、调整大小</h3><p>利用真实世界的图像数据训练神经网络时，通常需要把不同大小的图像转换为常见的尺寸，这样可以组织为一批固定大小的数据。<br><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div></pre></td><td class="code"><pre><div class="line"><span class="comment"># Reads an image from a file, decodes it into a dense tensor, and resizes it</span></div><div class="line"><span class="comment"># to a fixed shape.</span></div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">_parse_function</span><span class="params">(filename, label)</span>:</span></div><div class="line">  image_string = tf.read_file(filename)</div><div class="line">  image_decoded = tf.image.decode_image(image_string)</div><div class="line">  image_resized = tf.image.resize_images(image_decoded, [<span class="number">28</span>, <span class="number">28</span>])</div><div class="line">  <span class="keyword">return</span> image_resized, label</div><div class="line"></div><div class="line"><span class="comment"># A vector of filenames.</span></div><div class="line">filenames = tf.constant([<span class="string">"/var/data/image1.jpg"</span>, <span class="string">"/var/data/image2.jpg"</span>, ...])</div><div class="line"></div><div class="line"><span class="comment"># `labels[i]` is the label for the image in `filenames[i].</span></div><div class="line">labels = tf.constant([<span class="number">0</span>, <span class="number">37</span>, ...])</div><div class="line"></div><div class="line">dataset = tf.contrib.data.Dataset.from_tensor_slices((filenames, labels))</div><div class="line">dataset = dataset.map(_parse_function)</div></pre></td></tr></table></figure></p><h3 id="使用tf-py-func-应用Python逻辑"><a href="#使用tf-py-func-应用Python逻辑" class="headerlink" title="使用tf.py_func()应用Python逻辑"></a>使用tf.py_func()应用Python逻辑</h3><p>为了提高性能，我们鼓励你使用TensorFlow中的操作进行数据预处理。但有时使用内置的Python库来解析输入数据也有效。<br><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">import</span> cv2</div><div class="line"></div><div class="line"><span class="comment"># Use a custom OpenCV function to read the image, instead of the standard</span></div><div class="line"><span class="comment"># TensorFlow `tf.read_file()` operation.</span></div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">_read_py_function</span><span class="params">(filename, label)</span>:</span></div><div class="line">  image_decoded = cv2.imread(image_string, cv2.IMREAD_GRAYSCALE)</div><div class="line">  <span class="keyword">return</span> image_decoded, label</div><div class="line"></div><div class="line"><span class="comment"># Use standard TensorFlow operations to resize the image to a fixed shape.</span></div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">_resize_function</span><span class="params">(image_decoded, label)</span>:</span></div><div class="line">  image_decoded.set_shape([<span class="keyword">None</span>, <span class="keyword">None</span>, <span class="keyword">None</span>])</div><div class="line">  image_resized = tf.image.resize_images(image_decoded, [<span class="number">28</span>, <span class="number">28</span>])</div><div class="line">  <span class="keyword">return</span> image_resized, label</div><div class="line"></div><div class="line">filenames = [<span class="string">"/var/data/image1.jpg"</span>, <span class="string">"/var/data/image2.jpg"</span>, ...]</div><div class="line">labels = [<span class="number">0</span>, <span class="number">37</span>, <span class="number">29</span>, <span class="number">1</span>, ...]</div><div class="line"></div><div class="line">dataset = tf.contrib.data.Dataset.from_tensor_slices((filenames, labels))</div><div class="line">dataset = dataset.map(</div><div class="line">    <span class="keyword">lambda</span> filename, label: tf.py_func(</div><div class="line">        _read_py_function, [filename, label], [tf.uint8, label.dtype]))</div><div class="line">dataset = dataset.map(_resize_function)</div></pre></td></tr></table></figure></p><h2 id="输出Batch"><a href="#输出Batch" class="headerlink" title="输出Batch"></a>输出Batch</h2><h3 id="简单的Batching"><a href="#简单的Batching" class="headerlink" title="简单的Batching"></a>简单的Batching</h3><p>最简单的形式是堆叠n个连续的数据集元素，组织成一个单一的元素。<code>Dataset.batch()</code>变换正是这么做的。和<code>tf.stack()</code>具有相同的限制条件，对每个元素、每个component的Tensor都必须有相同的形状。<br><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div></pre></td><td class="code"><pre><div class="line">inc_dataset = tf.contrib.data.Dataset.range(<span class="number">100</span>)</div><div class="line">dec_dataset = tf.contrib.data.Dataset.range(<span class="number">0</span>, <span class="number">-100</span>, <span class="number">-1</span>)</div><div class="line">dataset = tf.contrib.data.Dataset.zip((inc_dataset, dec_dataset))</div><div class="line">batched_dataset = dataset.batch(<span class="number">4</span>)</div><div class="line"></div><div class="line">iterator = batched_dataset.make_one_shot_iterator()</div><div class="line">next_element = iterator.get_next()</div><div class="line"></div><div class="line">print(sess.run(next_element))  <span class="comment"># ==&gt; ([0, 1, 2,   3],   [ 0, -1,  -2,  -3])</span></div><div class="line">print(sess.run(next_element))  <span class="comment"># ==&gt; ([4, 5, 6,   7],   [-4, -5,  -6,  -7])</span></div><div class="line">print(sess.run(next_element))  <span class="comment"># ==&gt; ([8, 9, 10, 11],   [-8, -9, -10, -11])</span></div></pre></td></tr></table></figure></p><h3 id="带边距的Batching-Tensors"><a href="#带边距的Batching-Tensors" class="headerlink" title="带边距的Batching Tensors"></a>带边距的Batching Tensors</h3><p>上面的方法可以处理相同形状的Tensor。但是很多模型的输入数据的大小不一（如长度不同的序列）。为了处理这个情况，<code>Dataset.padded_batch()</code>变换可以对某些维度指定边距，以处理不同形状的Tensor。<br><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div></pre></td><td class="code"><pre><div class="line">dataset = tf.contrib.data.Dataset.range(<span class="number">100</span>)</div><div class="line">dataset = dataset.map(<span class="keyword">lambda</span> x: tf.fill([tf.cast(x, tf.int32)], x))</div><div class="line">dataset = dataset.padded_batch(<span class="number">4</span>, padded_shapes=[<span class="keyword">None</span>])</div><div class="line"></div><div class="line">iterator = dataset.make_one_shot_iterator()</div><div class="line">next_element = iterator.get_next()</div><div class="line"></div><div class="line">print(sess.run(next_element))  <span class="comment"># ==&gt; [[0, 0, 0], [1, 0, 0], [2, 2, 0], [3, 3, 3]]</span></div><div class="line">print(sess.run(next_element))  <span class="comment"># ==&gt; [[4, 4, 4, 4, 0, 0, 0],</span></div><div class="line">                               <span class="comment">#      [5, 5, 5, 5, 5, 0, 0],</span></div><div class="line">                               <span class="comment">#      [6, 6, 6, 6, 6, 6, 0],</span></div><div class="line">                               <span class="comment">#      [7, 7, 7, 7, 7, 7, 7]]</span></div></pre></td></tr></table></figure></p><p><code>Dataset.padded_batch()</code>变换允许你对每一个component的每一维度设置不同的边距，边距也可以是可变的长度（设置为<code>None</code>，参考上面的例子），也可以为边距赋值，默认为0。</p><h2 id="训练流程"><a href="#训练流程" class="headerlink" title="训练流程"></a>训练流程</h2><h3 id="处理多轮epoch"><a href="#处理多轮epoch" class="headerlink" title="处理多轮epoch"></a>处理多轮epoch</h3><p><code>Dataset</code>API提供了两种处理多轮相同数据的方法。</p><p>迭代一个数据集多次最简单的方法是使用<code>Dataset.repeat()</code>变换。下面的例子是创建一个数据集来重复输入10个epochs：<br><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div></pre></td><td class="code"><pre><div class="line">filenames = [<span class="string">"/var/data/file1.tfrecord"</span>, <span class="string">"/var/data/file2.tfrecord"</span>]</div><div class="line">dataset = tf.contrib.data.TFRecordDataset(filenames)</div><div class="line">dataset = dataset.map(...)</div><div class="line">dataset = dataset.repeat(<span class="number">10</span>)</div><div class="line">dataset = dataset.batch(<span class="number">32</span>)</div></pre></td></tr></table></figure></p><p>不带参数的<code>Dataset.repeat()</code>会无限的重复输入。<code>Dataset.repeat()</code>变换不会提示一个epoch的结束或新的epoch的开始。</p><p>如果想接收epoch起始或结束的信号，可以写一个训练循环，在每个数据集结束时捕获<code>tf.errors.OutOfRangeError</code>，在每轮结束时可以计算一些统计量（如验证集错误率）<br><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div></pre></td><td class="code"><pre><div class="line">filenames = [<span class="string">"/var/data/file1.tfrecord"</span>, <span class="string">"/var/data/file2.tfrecord"</span>]</div><div class="line">dataset = tf.contrib.data.TFRecordDataset(filenames)</div><div class="line">dataset = dataset.map(...)</div><div class="line">dataset = dataset.batch(<span class="number">32</span>)</div><div class="line">iterator = dataset.make_initializable_iterator()</div><div class="line">next_element = iterator.get_next()</div><div class="line"></div><div class="line"><span class="comment"># Compute for 100 epochs.</span></div><div class="line"><span class="keyword">for</span> _ <span class="keyword">in</span> range(<span class="number">100</span>):</div><div class="line">  sess.run(iterator.initializer)</div><div class="line">  <span class="keyword">while</span> <span class="keyword">True</span>:</div><div class="line">    <span class="keyword">try</span>:</div><div class="line">      sess.run(next_element)</div><div class="line">    <span class="keyword">except</span> tf.errors.OutOfRangeError:</div><div class="line">      <span class="keyword">break</span></div><div class="line"></div><div class="line">  <span class="comment"># [Perform end-of-epoch calculations here.]</span></div></pre></td></tr></table></figure></p><h3 id="随机打乱数据"><a href="#随机打乱数据" class="headerlink" title="随机打乱数据"></a>随机打乱数据</h3><p><code>Dataset.shuffle()</code>变换使用类似<code>tf.RandomShuffleQueue</code>的算法来随机打乱输入数据：其内部有固定长度的缓冲区，并从该缓冲区中随机的选择下一个元素。<br><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div></pre></td><td class="code"><pre><div class="line">filenames = [<span class="string">"/var/data/file1.tfrecord"</span>, <span class="string">"/var/data/file2.tfrecord"</span>]</div><div class="line">dataset = tf.contrib.data.TFRecordDataset(filenames)</div><div class="line">dataset = dataset.map(...)</div><div class="line">dataset = dataset.shuffle(buffer_size=<span class="number">10000</span>)</div><div class="line">dataset = dataset.batch(<span class="number">32</span>)</div><div class="line">dataset = dataset.repeat()</div></pre></td></tr></table></figure></p><h3 id="使用高级API"><a href="#使用高级API" class="headerlink" title="使用高级API"></a>使用高级API</h3><p><code>tf.train.MonitoredTrainingSession</code>API在分布式的设置下在很多方面简化运行TensorFlow。<code>MonitoredTrainSession</code>使用<code>tf.errors.OutOfRangeError</code>来标志训练完成，如果和<code>Dataset</code>API一起用的话，推荐使用<code>Dataset.make_one_shot_iterator()</code>。<br><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div></pre></td><td class="code"><pre><div class="line">filenames = [<span class="string">"/var/data/file1.tfrecord"</span>, <span class="string">"/var/data/file2.tfrecord"</span>]</div><div class="line">dataset = tf.contrib.data.TFRecordDataset(filenames)</div><div class="line">dataset = dataset.map(...)</div><div class="line">dataset = dataset.shuffle(buffer_size=<span class="number">10000</span>)</div><div class="line">dataset = dataset.batch(<span class="number">32</span>)</div><div class="line">dataset = dataset.repeat(num_epochs)</div><div class="line">iterator = dataset.make_one_shot_iterator()</div><div class="line"></div><div class="line">next_example, next_label = iterator.get_next()</div><div class="line">loss = model_function(next_example, next_label)</div><div class="line"></div><div class="line">training_op = tf.train.AdagradOptimizer(...).minimize(loss)</div><div class="line"></div><div class="line"><span class="keyword">with</span> tf.train.MonitoredTrainingSession(...) <span class="keyword">as</span> sess:</div><div class="line">  <span class="keyword">while</span> <span class="keyword">not</span> sess.should_stop():</div><div class="line">    sess.run(training_op)</div></pre></td></tr></table></figure></p><p>如果把<code>Dataset</code>作为<code>tf.estimator.Estimator</code>的<code>input_fn</code>来使用，也推荐用<code>Dataset.make_one_shot_iterator</code><br><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div></pre></td><td class="code"><pre><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">dataset_input_fn</span><span class="params">()</span>:</span></div><div class="line">  filenames = [<span class="string">"/var/data/file1.tfrecord"</span>, <span class="string">"/var/data/file2.tfrecord"</span>]</div><div class="line">  dataset = tf.contrib.data.TFRecordDataset(filenames)</div><div class="line"></div><div class="line">  <span class="comment"># Use `tf.parse_single_example()` to extract data from a `tf.Example`</span></div><div class="line">  <span class="comment"># protocol buffer, and perform any additional per-record preprocessing.</span></div><div class="line">  <span class="function"><span class="keyword">def</span> <span class="title">parser</span><span class="params">(record)</span>:</span></div><div class="line">    keys_to_features = &#123;</div><div class="line">        <span class="string">"image_data"</span>: tf.FixedLenFeature((), tf.string, default_value=<span class="string">""</span>),</div><div class="line">        <span class="string">"date_time"</span>: tf.FixedLenFeature((), tf.int64, default_value=<span class="string">""</span>),</div><div class="line">        <span class="string">"label"</span>: tf.FixedLenFeature((), tf.int64,</div><div class="line">                                    default_value=tf.zeros([], dtype=tf.int64)),</div><div class="line">    &#125;</div><div class="line">    parsed = tf.parse_single_example(record, keys_to_features)</div><div class="line"></div><div class="line">    <span class="comment"># Perform additional preprocessing on the parsed data.</span></div><div class="line">    image = tf.decode_jpeg(parsed[<span class="string">"image_data"</span>])</div><div class="line">    image = tf.reshape(image, [<span class="number">299</span>, <span class="number">299</span>, <span class="number">1</span>])</div><div class="line">    label = tf.cast(parsed[<span class="string">"label"</span>], tf.int32)</div><div class="line"></div><div class="line">    <span class="keyword">return</span> &#123;<span class="string">"image_data"</span>: image, <span class="string">"date_time"</span>: parsed[<span class="string">"date_time"</span>]&#125;, label</div><div class="line"></div><div class="line">  <span class="comment"># Use `Dataset.map()` to build a pair of a feature dictionary and a label</span></div><div class="line">  <span class="comment"># tensor for each example.</span></div><div class="line">  dataset = dataset.map(parser)</div><div class="line">  dataset = dataset.shuffle(buffer_size=<span class="number">10000</span>)</div><div class="line">  dataset = dataset.batch(<span class="number">32</span>)</div><div class="line">  dataset = dataset.repeat(num_epochs)</div><div class="line">  iterator = dataset.make_one_shot_iterator()</div><div class="line"></div><div class="line">  <span class="comment"># `features` is a dictionary in which each value is a batch of values for</span></div><div class="line">  <span class="comment"># that feature; `labels` is a batch of labels.</span></div><div class="line">  features, labels = iterator.get_next()</div><div class="line">  <span class="keyword">return</span> features, labels</div></pre></td></tr></table></figure></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;&lt;code&gt;Dataset&lt;/code&gt;API可使你从简单、可重用的片段构建复杂的输入管道。例如，一个图像模型的输入管道可能从分布式的系统中聚合数据，对每个图像进行随机扰动，并随机选择一批图像用于训练；文本模型的输入管道可能从原始的文本中提取特征，利用查找表将其转化为emb
      
    
    </summary>
    
    
      <category term="tensorflow" scheme="http://libowei.net/tags/tensorflow/"/>
    
  </entry>
  
  <entry>
    <title>LaTeX数学公式符号</title>
    <link href="http://libowei.net/LaTeX%E6%95%B0%E5%AD%A6%E5%85%AC%E5%BC%8F%E7%AC%A6%E5%8F%B7.html"/>
    <id>http://libowei.net/LaTeX数学公式符号.html</id>
    <published>2017-08-17T12:03:56.000Z</published>
    <updated>2017-11-23T04:12:14.120Z</updated>
    
    <content type="html"><![CDATA[<h2 id="角标"><a href="#角标" class="headerlink" title="角标"></a>角标</h2><ul><li>上标 <code>^{}</code></li><li>下标 <code>_{}</code></li></ul><p>角标为单个字符时，可以不用<code>{}</code>；角标为多字符或多层次，必须使用<code>{}</code></p><p><code>x^2</code> $x^2$</p><p><code>x^2_1</code> $x^2_1$</p><p><code>x^{(n)}_{22}</code> $x^{(n)}_{22}$</p><p><code>^{16}O^{2-}_{32}</code>$^{16}O^{2-}_{32}$</p><p><code>x^{y_z}</code> $x^{y_z}$</p><h2 id="分式"><a href="#分式" class="headerlink" title="分式"></a>分式</h2><ul><li><code>\frac{分子}{分母}</code></li></ul><p><code>\frac{x+y}{y+z}</code> $\frac{x+y}{y+z}$</p><p><code>\displaystyle \frac{x+y}{y+z}</code> $\displaystyle \frac{x+y}{y+z}$</p><h2 id="根式"><a href="#根式" class="headerlink" title="根式"></a>根式</h2><ul><li>二次根式 <code>\sqrt{表达式}</code></li><li>n次根式 <code>\sqrt[n]{表达式}</code></li><li>根号上没有横线 <code>\surd{表达式}</code></li></ul><p>被开方表达式字符高度不一致时，为使横线在同一水平线上，插入<code>(\mathstruct)</code></p><ul><li>$\sqrt{a}+\sqrt{b}+\sqrt{c}$ <code>\sqrt{a}+\sqrt{b}+\sqrt{c}</code> </li><li>$\sqrt{\mathstrut a}+\sqrt{\mathstrut b}+\sqrt{\mathstrut c}$ <code>\sqrt{\mathstrut a}+\sqrt{\mathstrut b}+\sqrt{\mathstrut c}</code></li></ul><h2 id="求和、积分"><a href="#求和、积分" class="headerlink" title="求和、积分"></a>求和、积分</h2><ul><li>求和 <code>\sum_{k=1}^n求和项</code></li><li>积分 <code>\int_a^b积分项</code></li><li>无穷级数 <code>\infty</code></li></ul><p><code>\sum_{a=1}^n</code> $\sum_{a=1}^nx$</p><p><code>\int_1^{10}x</code> $\int_1^{10}x$</p><p><code>\sum_{k=1}^\infty \frac{x^n}{n!} =\int_0 ^\infty e^x</code> $\sum_{k=1}^\infty \frac{x^n}{n!} =\int_0 ^\infty e^x$</p><p>多行效果：$$\sum_{k=1}^\infty \frac{x^n}{n!} =\int_0 ^\infty e^x$$ </p><p>改变上下限位置：</p><ul><li>强制在上下侧<code>\limits</code></li><li>强制在左右侧<code>\nolimits</code></li></ul><p>行内在上下侧：<code>\sum\limits_{n=0}^\infty x^n</code> $\sum\limits_{n=0}^\infty x^n$</p><p>行间在左右侧：<code>$$\sum\nolimits_{n=0}^\infty x^n$$</code> $$\sum\nolimits_{n=0}^\infty x^n$$</p><h2 id="上下划线"><a href="#上下划线" class="headerlink" title="上下划线"></a>上下划线</h2><ul><li>上划线 <code>\overline</code></li><li>下划线 <code>\underline</code></li><li>上花括弧 <code>\overbrace</code></li><li>下花括弧 <code>\underbrace</code></li></ul><p><code>$\overline{\overline{a^2}+\underline{ab}+\bar{a}^3}$</code> $\overline{\overline{a^2}+\underline{ab}+\bar{a}^3}$</p><p><code>$\underbrace{a+\overbrace{b+\dots+b}^{m\mbox个}+c}_{20\mbox个}$</code> $\underbrace{a+\overbrace{b+\dots+b}^{m\mbox个}+c}_{20\mbox个}$</p><h2 id="数学符号"><a href="#数学符号" class="headerlink" title="数学符号"></a>数学符号</h2><ul><li><code>\leq</code> $\leq$</li><li><code>\geq</code> $\geq$</li><li><code>\sim</code> $\sim$</li><li><code>\approx</code> $\approx$</li><li><code>\in</code> $\in$</li><li><code>\notin</code> $\notin$</li><li><code>\neq</code> $\neq$</li><li><code>\ll</code> $\ll$</li><li><code>\gg</code> $\gg$</li></ul><h2 id="希腊字符"><a href="#希腊字符" class="headerlink" title="希腊字符"></a>希腊字符</h2><ul><li>$\alpha$ <code>\alpha</code></li><li>$\beta$ <code>\beta</code></li><li>$\gamma,\Gamma$ <code>\gamma,\Gamma</code></li><li>$\delta,\Delta$ <code>\delta,\Delta</code></li><li>$\epsilon,\varepsilon$ <code>\epsilon,\varepsilon</code></li><li>$\eta$ <code>\eta</code></li><li>$\theta,\Theta,\vartheta$ <code>\theta,\Theta,\vartheta</code></li><li>$\iota$ <code>\iota</code></li><li>$\lambda$ <code>\lambda</code></li><li>$\mu$ <code>\mu</code></li><li>$\phi,\Phi,\varphi$ <code>\phi,\Phi,\varphi</code></li><li>$\psi,\Psi$ <code>\psi,\Psi</code></li><li>$\omega,\Omega$ <code>\omega,\Omega</code></li></ul>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h2 id=&quot;角标&quot;&gt;&lt;a href=&quot;#角标&quot; class=&quot;headerlink&quot; title=&quot;角标&quot;&gt;&lt;/a&gt;角标&lt;/h2&gt;&lt;ul&gt;
&lt;li&gt;上标 &lt;code&gt;^{}&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;下标 &lt;code&gt;_{}&lt;/code&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;角
      
    
    </summary>
    
    
      <category term="latex" scheme="http://libowei.net/tags/latex/"/>
    
  </entry>
  
  <entry>
    <title>12306席别代码</title>
    <link href="http://libowei.net/12306%E5%B8%AD%E5%88%AB%E4%BB%A3%E7%A0%81.html"/>
    <id>http://libowei.net/12306席别代码.html</id>
    <published>2017-08-14T08:35:39.000Z</published>
    <updated>2017-11-10T03:14:52.808Z</updated>
    
    <content type="html"><![CDATA[<p>在12306查询车票时，可从返回的JSON字符串得到车次、始发站、终点站、时间、余票数量等信息。其中有一个字段，其包含的字符串常见的有<code>OM9</code>,<code>1413</code>,<code>OMO</code>等。</p><p>这个字段代表该车次发售车票的席别，即硬卧、软卧、二等座、一等座等。</p><p>字符串中每一个字符代表一种席别：</p><ul><li><strong><em>1</em></strong> 硬座</li><li><strong><em>2</em></strong> 软座</li><li><strong><em>3</em></strong> 硬卧</li><li><strong><em>4</em></strong> 软卧</li><li><strong><em>6</em></strong> 高级软卧</li><li><strong><em>9</em></strong> 商务座</li><li><strong><em>F</em></strong> 动卧</li><li><strong><em>M</em></strong> 一等座</li><li><strong><em>O</em></strong> 二等座</li><li><strong><em>P</em></strong> 特等座</li></ul><p><strong><em>无座</em></strong>的情况：若某个代码重复出现，则代表该席别出售无座票，如</p><ul><li><code>OMO</code> 表示二等座席位出售无座票</li><li><code>1413</code> 表示硬座席位出售无座票</li></ul><p>12306页面上还有一个席别为<strong><em>其他</em></strong>，可能的席位类型为软卧包厢，单人包厢等，可能的代码为<code>H</code>,<code>F</code>等（具体代表哪种席别我也没弄清楚）</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;在12306查询车票时，可从返回的JSON字符串得到车次、始发站、终点站、时间、余票数量等信息。其中有一个字段，其包含的字符串常见的有&lt;code&gt;OM9&lt;/code&gt;,&lt;code&gt;1413&lt;/code&gt;,&lt;code&gt;OMO&lt;/code&gt;等。&lt;/p&gt;
&lt;p&gt;这个字段代表该车次
      
    
    </summary>
    
    
      <category term="12306" scheme="http://libowei.net/tags/12306/"/>
    
  </entry>
  
  <entry>
    <title>TensorBoard说明文档</title>
    <link href="http://libowei.net/TensorBoard%E8%AF%B4%E6%98%8E%E6%96%87%E6%A1%A3.html"/>
    <id>http://libowei.net/TensorBoard说明文档.html</id>
    <published>2017-07-26T12:19:10.000Z</published>
    <updated>2017-11-10T04:12:33.430Z</updated>
    
    <content type="html"><![CDATA[<h2 id="TensorBoard"><a href="#TensorBoard" class="headerlink" title="TensorBoard"></a>TensorBoard</h2><p>TensorBoard是用来了解TensorFlow如何运行、展示图表的一套Web应用程序。</p><p>本文档介绍如何利用TensorBoard还嫌可视化，并给出TensorBoard的一些关键概念。更深入的例子请参见教程<a href="https://www.tensorflow.org/get_started/summaries_and_tensorboard" target="_blank" rel="external"> TensorBoard: Visualizing Learning</a>。要更深入了解图表可视化工具，请看教程<a href="https://www.tensorflow.org/get_started/graph_viz" target="_blank" rel="external">TensorBoard: Graph Visualization</a>。</p><p>还可以看这个<a href="https://www.youtube.com/watch?v=eBbEDRsCmv4" target="_blank" rel="external">视频教程</a>，了解如何安装和使用TensorBoard。</p><h2 id="使用方法"><a href="#使用方法" class="headerlink" title="使用方法"></a>使用方法</h2><p>在运行TensorBoard之前，确定已经生成并通过<code>summary writer</code>把训练摘要数据保存到文件夹中：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">file_writer = tf.summary.FileWriter(&apos;/path/to/logs&apos;, sess.graph)</div></pre></td></tr></table></figure></p><p>查看<a href="https://www.tensorflow.org/get_started/summaries_and_tensorboard" target="_blank" rel="external">TensorBoard tutorial</a>了解更多细节。当有了事件文件，指定其所在文件夹，就可以运行TensorBoard：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">tensorboard --logdir=path/to/logs</div></pre></td></tr></table></figure></p><p>执行上述命令后会显示TensorBoard已经启动的信息。现在可以访问<a href="http://localhost:6006" target="_blank" rel="external">http://localhost:6006</a>。</p><p>TensorBoard从<code>logdir</code>中读入日志数据。若要了解TensorBoard的配置等信息，运行<code>tensorboard --help</code></p><p>TensorBoard可以在Chrome和Firefox浏览器中使用，其他浏览器可能会出现错误或其他性能问题。</p><h2 id="关键概念"><a href="#关键概念" class="headerlink" title="关键概念"></a>关键概念</h2><h3 id="摘要运算：TensorBoard如何从TensorFlow中获取数据"><a href="#摘要运算：TensorBoard如何从TensorFlow中获取数据" class="headerlink" title="摘要运算：TensorBoard如何从TensorFlow中获取数据"></a>摘要运算：TensorBoard如何从TensorFlow中获取数据</h3><p>使用TensorBoard的第一步是从运行中的TensorFlow程序中获取数据，这需要摘要运算(summary ops)，摘要操作像<code>tf.matmul</code>或<code>tf.nn.relu</code>等运算操作一样：接收Tensor、产生Tensor，在TensorFlow Graph中求值。但摘要运算有所不同的是：这些Tensor包含serialized protobufs(我理解为可以持久化的一些东西)，可以写入磁盘并在TensorBoard中展示。为了在TensorBoard中对记录的摘要数据可视化，需要对摘要计算求值，得到其结果并将其以摘要的形式写入磁盘。对FileWriter的详细解释和例子在<a href="https://www.tensorflow.org/get_started/summaries_and_tensorboard" target="_blank" rel="external">教程</a>中。</p><p>支持的摘要计算包括：</p><ul><li>tf.summary.scalar</li><li>tf.summary.image</li><li>tf.summary.audio</li><li>tf.summary.text</li><li>tf.summary.histogram</li></ul><h3 id="标签：对数据取名字"><a href="#标签：对数据取名字" class="headerlink" title="标签：对数据取名字"></a>标签：对数据取名字</h3><p>进行摘要计算时，要对其取一个标签<code>tag</code>。这个标签是该计算所记录的数据的名字，而且用于在前端页面展示。Scalar和histogram面板通过标签来标记数据，通过层次化的标签名对数据分组。如果有很多标签的话，我们建议使用<code>/</code>对其分组。</p><h3 id="事件文件和日志文件夹：TensorBoard如何读取数据"><a href="#事件文件和日志文件夹：TensorBoard如何读取数据" class="headerlink" title="事件文件和日志文件夹：TensorBoard如何读取数据"></a>事件文件和日志文件夹：TensorBoard如何读取数据</h3><h3 id="Runs：对比模型不同的训练结果"><a href="#Runs：对比模型不同的训练结果" class="headerlink" title="Runs：对比模型不同的训练结果"></a>Runs：对比模型不同的训练结果</h3><h2 id="可视化"><a href="#可视化" class="headerlink" title="可视化"></a>可视化</h2><h3 id="数值面板（Scalar）"><a href="#数值面板（Scalar）" class="headerlink" title="数值面板（Scalar）"></a>数值面板（Scalar）</h3><h3 id="柱状图面板（Histogram）"><a href="#柱状图面板（Histogram）" class="headerlink" title="柱状图面板（Histogram）"></a>柱状图面板（Histogram）</h3><h3 id="统计分布面板（Distribution）"><a href="#统计分布面板（Distribution）" class="headerlink" title="统计分布面板（Distribution）"></a>统计分布面板（Distribution）</h3><h3 id="图像面板（Image）"><a href="#图像面板（Image）" class="headerlink" title="图像面板（Image）"></a>图像面板（Image）</h3><h3 id="声音面板（Audio）"><a href="#声音面板（Audio）" class="headerlink" title="声音面板（Audio）"></a>声音面板（Audio）</h3><h3 id="计算图展示（Graph-Explorer）"><a href="#计算图展示（Graph-Explorer）" class="headerlink" title="计算图展示（Graph Explorer）"></a>计算图展示（Graph Explorer）</h3><h3 id="Embedding-Projector"><a href="#Embedding-Projector" class="headerlink" title="Embedding Projector"></a>Embedding Projector</h3><h3 id="文本面板（Text）"><a href="#文本面板（Text）" class="headerlink" title="文本面板（Text）"></a>文本面板（Text）</h3><h2 id="常见问题"><a href="#常见问题" class="headerlink" title="常见问题"></a>常见问题</h2><h3 id="TensorBoard不显示任何数据"><a href="#TensorBoard不显示任何数据" class="headerlink" title="TensorBoard不显示任何数据"></a>TensorBoard不显示任何数据</h3><h3 id="TensorBoard只显示一部分数据"><a href="#TensorBoard只显示一部分数据" class="headerlink" title="TensorBoard只显示一部分数据"></a>TensorBoard只显示一部分数据</h3><h3 id="TensorBoard支持多线程或分布式的summary-writer吗？"><a href="#TensorBoard支持多线程或分布式的summary-writer吗？" class="headerlink" title="TensorBoard支持多线程或分布式的summary writer吗？"></a>TensorBoard支持多线程或分布式的summary writer吗？</h3><h3 id="数据重叠在一起"><a href="#数据重叠在一起" class="headerlink" title="数据重叠在一起"></a>数据重叠在一起</h3><h3 id="如何处理ensorFlow程序重新启动的问题"><a href="#如何处理ensorFlow程序重新启动的问题" class="headerlink" title="如何处理ensorFlow程序重新启动的问题"></a>如何处理ensorFlow程序重新启动的问题</h3><h3 id="如何从TensorBoard导出数据"><a href="#如何从TensorBoard导出数据" class="headerlink" title="如何从TensorBoard导出数据"></a>如何从TensorBoard导出数据</h3><h3 id="可以重叠多个图表吗"><a href="#可以重叠多个图表吗" class="headerlink" title="可以重叠多个图表吗"></a>可以重叠多个图表吗</h3><h3 id="可以自己生成散点图吗（或其他自定义图表）"><a href="#可以自己生成散点图吗（或其他自定义图表）" class="headerlink" title="可以自己生成散点图吗（或其他自定义图表）"></a>可以自己生成散点图吗（或其他自定义图表）</h3>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h2 id=&quot;TensorBoard&quot;&gt;&lt;a href=&quot;#TensorBoard&quot; class=&quot;headerlink&quot; title=&quot;TensorBoard&quot;&gt;&lt;/a&gt;TensorBoard&lt;/h2&gt;&lt;p&gt;TensorBoard是用来了解TensorFlow如何运行、展示图
      
    
    </summary>
    
    
      <category term="tensorflow" scheme="http://libowei.net/tags/tensorflow/"/>
    
      <category term="tensorboard" scheme="http://libowei.net/tags/tensorboard/"/>
    
  </entry>
  
  <entry>
    <title>TensorFlow CNN 相关API</title>
    <link href="http://libowei.net/TensorFlow-CNN-%E7%9B%B8%E5%85%B3API.html"/>
    <id>http://libowei.net/TensorFlow-CNN-相关API.html</id>
    <published>2017-07-24T13:03:47.000Z</published>
    <updated>2017-11-23T04:12:26.166Z</updated>
    
    <content type="html"><![CDATA[<h2 id="tf-nn-conv2d"><a href="#tf-nn-conv2d" class="headerlink" title="tf.nn.conv2d"></a><code>tf.nn.conv2d</code></h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div></pre></td><td class="code"><pre><div class="line">conv2d(</div><div class="line">    input,</div><div class="line">    filter,</div><div class="line">    strides,</div><div class="line">    padding,</div><div class="line">    use_cudnn_on_gpu=None,</div><div class="line">    data_format=None,</div><div class="line">    name=None</div><div class="line">)</div></pre></td></tr></table></figure><p>给定4维的输入和4维的过滤器Tensor，进行2卷积运算。</p><p>输入Tensor的形状为[batch,height,width,channels]，过滤器张量的形状为[filter_height, filter_width, in_channels, out_channels]</p><ol><li>把过滤器转化为2-D，其形状为<code>[filter_height * filter_width * in_channels, output_channels]</code></li><li>在图像中提取形状为<code>[batch, out_height, out_width, filter_height * filter_width * in_channels]</code>的Tensor</li><li>图像块向量和过滤器矩阵相乘</li></ol><p>步长参数strides中，必须满足<code>strides[0]=strides[3]=1</code>。大多数情况下水平步长和垂直步长相等。</p><p>参数：</p><ul><li><code>input</code> Tensor。维度的顺序由<code>data-format</code>确定。</li><li><code>filter</code> Tensor。与<code>input</code>数据类型相同。形状是<code>[filter_height, filter_width, in_channels, out_channels]</code></li><li><code>strides</code> 整数列表（长度为4的1-D Tensor）。每个维度滑动窗口的步长，维度的顺序由<code>data-format</code>确定。</li><li><code>padding</code> 字符串：<code>&quot;SAME&quot;</code>或<code>&quot;VALID&quot;</code>。<code>&quot;SAME&quot;</code>是对输入的首尾补0，以满足每个滑动窗口的大小。<code>&quot;VALID&quot;</code>是丢弃末尾的数据。</li><li><code>use_cudnn_on_gpu</code> 布尔值，默认为<code>True</code>。</li><li><code>data_format</code> 可选<code>&quot;NHWC&quot;</code>或<code>&quot;NCHW&quot;</code>，默认为<code>&quot;NHWC&quot;</code></li><li><code>name</code> 为这个操作取一个名字。</li></ul><h2 id="tf-nn-bias-add"><a href="#tf-nn-bias-add" class="headerlink" title="tf.nn.bias_add"></a><code>tf.nn.bias_add</code></h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div></pre></td><td class="code"><pre><div class="line">bias_add(</div><div class="line">    value,</div><div class="line">    bias,</div><div class="line">    data_format=None,</div><div class="line">    name=None</div><div class="line">)</div></pre></td></tr></table></figure><p>这个函数的作用是将偏差项<code>bias</code>加到<code>value</code>上面。</p><p>这个操作是<code>tf.add</code>的一个特例。其中<code>bias</code>必须为1维，而<code>value</code>可以为任意维度。与<code>tf.add</code>不同的是，数据被量化的情况下，<code>value</code>和<code>bias</code>的类型可以不同。</p><p>参数：</p><ul><li><code>value</code> Tensor。</li><li><code>bias</code> 1维的Tensor。大小与<code>value</code>最后一维相同。类型也须与<code>value</code>相同，除非<code>value</code>被量化。</li><li><code>data_format</code> <code>&quot;NHWC&quot;</code>或<code>&quot;NCHW&quot;</code>。</li><li><code>name</code> 为这个操作取一个名字。</li></ul><h2 id="tf-nn-relu"><a href="#tf-nn-relu" class="headerlink" title="tf.nn.relu"></a><code>tf.nn.relu</code></h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div></pre></td><td class="code"><pre><div class="line">relu(</div><div class="line">    features,</div><div class="line">    name=None</div><div class="line">)</div></pre></td></tr></table></figure><p>计算非线性激活函数<code>max(features,0)</code></p><p>参数:</p><ul><li><code>features</code> Tensor。</li><li><code>name</code> 为这个操作取个名字</li></ul><h2 id="tf-nn-max-pool"><a href="#tf-nn-max-pool" class="headerlink" title="tf.nn.max_pool"></a><code>tf.nn.max_pool</code></h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div></pre></td><td class="code"><pre><div class="line">    value,</div><div class="line">    ksize,</div><div class="line">    strides,</div><div class="line">    padding,</div><div class="line">    data_format=&apos;NHWC&apos;,</div><div class="line">    name=None</div><div class="line">)</div></pre></td></tr></table></figure><p>对输入数据进行最大池化</p><p>参数：</p><ul><li><code>value</code> 4维Tensor，形状为<code>[batch, height, width, channels]</code>，类型为<code>tf.float32</code></li><li><code>ksize</code> 长度&gt;=4的整形列表。每个维度的窗口大小。</li><li><code>strides</code> 长度&gt;=4的整形列表。每个维度的滑动步长。</li><li><code>padding</code> <code>&quot;SAME&quot;</code>或<code>&quot;VALID&quot;</code>。输出形状为：<code>output_height/output_width = (height/width - pool_size) stride + 1</code>，<code>&quot;SAME&quot;</code>为向上取整，<code>&quot;VALID&quot;</code>为向下取整</li></ul><h2 id="tf-nn-lrn"><a href="#tf-nn-lrn" class="headerlink" title="tf.nn.lrn"></a><code>tf.nn.lrn</code></h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div></pre></td><td class="code"><pre><div class="line">lrn(</div><div class="line">    input, </div><div class="line">    depth_radius=None, </div><div class="line">    bias=None, </div><div class="line">    alpha=None, </div><div class="line">    beta=None,</div><div class="line">    name=None</div><div class="line">)</div></pre></td></tr></table></figure><p>局部响应归一化</p><p>4维的<code>input</code>Tensor可以看做3维的1维向量（沿着最后一维），对每个向量归一化。<br>计算公式为：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">sqr_sum[a, b, c, d] = sum(input[a, b, c, d - depth_radius : d + depth_radius + 1] ** 2)</div><div class="line">output = input / (bias + alpha * sqr_sum) ** beta</div></pre></td></tr></table></figure></p><p>参数:</p><ul><li><code>input</code> Tensor</li><li><code>depth_radius</code> 默认为5</li><li><code>bias</code> 默认为1.0</li><li><code>alpha</code> 默认为1.0</li><li><code>beta</code> 默认为0.5，指数项</li><li><code>name</code> 为这个操作取个名字</li></ul><h2 id="tf-nn-sparse-softmax-cross-entropy-with-logits"><a href="#tf-nn-sparse-softmax-cross-entropy-with-logits" class="headerlink" title="tf.nn.sparse_softmax_cross_entropy_with_logits"></a><code>tf.nn.sparse_softmax_cross_entropy_with_logits</code></h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div></pre></td><td class="code"><pre><div class="line">sparse_softmax_cross_entropy_with_logits(</div><div class="line">    _sentinel=None,</div><div class="line">    labels=None,</div><div class="line">    logits=None,</div><div class="line">    name=None</div><div class="line">)</div></pre></td></tr></table></figure><p>计算<code>logists</code>和<code>labels</code>的稀疏softmax交叉熵。</p><p>用于度量互斥的离散分类任务（每个样本只属于一类）的概率误差，如CIFAR-10图像，每个图像只属于一类。</p><p>注意：这个操作接受未处理的输入，为了提高效率，函数内部对<code>logits</code>做了softmax运算。所以不要输入softmax后的数值，这会产生不正确的结果。</p><p>常见的情况，<code>logits</code>的形状为<code>[batch_size, num_classes]</code>，<code>labels</code>为<code>[batch_size]</code>。更高维度的也支持。</p><p>为了避免混淆，传递参数时需要带上参数名（如<code>sparse_softmax_cross_entropy_with_logits(logits=logits,labels=labels)</code></p><p>参数：</p><ul><li><code>labels</code> 形状为<code>[d_0, d_1, ..., d_{r-1}]</code>的Tensor（<code>r</code>为<code>label</code>的rank）。<code>labels</code>取值必须为<code>[0, num_classes)</code></li><li><code>logits</code> 对数概率(?)，形状为<code>[d_0, d_1, ..., d_{r-1}, num_classes]</code></li><li><code>name</code> 为操作取一个名字</li></ul><h2 id="tf-train-exponential-decay"><a href="#tf-train-exponential-decay" class="headerlink" title="tf.train.exponential_decay"></a><code>tf.train.exponential_decay</code></h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div></pre></td><td class="code"><pre><div class="line">exponential_decay(</div><div class="line">    learning_rate, </div><div class="line">    global_step, </div><div class="line">    decay_steps, </div><div class="line">    decay_rate,</div><div class="line">    staircase=False, </div><div class="line">    name=None</div><div class="line">)</div></pre></td></tr></table></figure><p>对学习率应用指数衰减。</p><p>训练模型时，随着训练的进行，逐渐减小学习率是好的做法。这个函数对初始的学习率应用指数衰减方法。需要<code>global_step</code>作为参数计算衰减后的学习率。可以传递一个TensorFlow变量，每次训练对其加一。</p><p>函数计算方法：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">ecayed_learning_rate = learning_rate * decay_rate ^ (global_step / decay_steps)</div></pre></td></tr></table></figure></p><p>若<code>staircase</code>为<code>True</code>，<code>global_step / decay_steps</code>计算结果为整数，此时学习率呈阶梯状下降。</p><p>参数:</p><ul><li><code>learning_rate</code> 标量，初始学习率</li><li><code>global_step</code> 标量（整数），不能为负</li><li><code>decay_steps</code> 标量（整数），必须为正</li><li><code>decay_rate</code> 标量，衰减率</li><li><code>staircase</code> 布尔。</li></ul><h2 id="tf-control-dependencies"><a href="#tf-control-dependencies" class="headerlink" title="tf.control_dependencies"></a><code>tf.control_dependencies</code></h2><p>控制计算顺序</p><h2 id="tf-train-ExponentialMovingAverage"><a href="#tf-train-ExponentialMovingAverage" class="headerlink" title="tf.train.ExponentialMovingAverage"></a><code>tf.train.ExponentialMovingAverage</code></h2><p><a href="https://www.tensorflow.org/versions/r1.3/api_docs/python/tf/train/ExponentialMovingAverage" target="_blank" rel="external">tf.train.ExponentialMovingAverage</a></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h2 id=&quot;tf-nn-conv2d&quot;&gt;&lt;a href=&quot;#tf-nn-conv2d&quot; class=&quot;headerlink&quot; title=&quot;tf.nn.conv2d&quot;&gt;&lt;/a&gt;&lt;code&gt;tf.nn.conv2d&lt;/code&gt;&lt;/h2&gt;&lt;figure class=&quot;highl
      
    
    </summary>
    
    
      <category term="tensorflow" scheme="http://libowei.net/tags/tensorflow/"/>
    
      <category term="卷积神经网络" scheme="http://libowei.net/tags/%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/"/>
    
  </entry>
  
  <entry>
    <title>CentOS安装TensorFlow</title>
    <link href="http://libowei.net/CentOS%E5%AE%89%E8%A3%85TensorFlow.html"/>
    <id>http://libowei.net/CentOS安装TensorFlow.html</id>
    <published>2017-07-21T02:21:26.000Z</published>
    <updated>2017-11-10T04:12:10.167Z</updated>
    
    <content type="html"><![CDATA[<p>在CentOS6上安装TensorFlow1.2后，<code>import tensorflow</code>时出现以下问题：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">ImportError: /lib64/libc.so.6: version `GLIBC_2.17&apos; not found</div></pre></td></tr></table></figure></p><p>这个错误的原因是未安装2.17版本的glibc库。</p><p>而在CentOS上，使用<code>yum install glibc</code>命令，只能更新到2.12版本。需要手动下载编译安装。</p><p>glibc-2.17下载地址：<a href="https://ftp.gnu.org/gnu/glibc/glibc-2.17.tar.gz" target="_blank" rel="external">https://ftp.gnu.org/gnu/glibc/glibc-2.17.tar.gz</a></p><p>下载glibc并解压缩<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">wget https://ftp.gnu.org/gnu/glibc/glibc-2.17.tar.gz</div><div class="line">tar -xvf glibc-2.17.tar.gz</div></pre></td></tr></table></figure></p><p>编译安装<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div></pre></td><td class="code"><pre><div class="line">cd glibc-2.17</div><div class="line">mkdir build</div><div class="line">cd build</div><div class="line">../configure --prefix=/usr --disable-profile --enable-add-ons --with-headers=/usr/include --with-binutils=/usr/bin</div><div class="line">make &amp;&amp; make install</div></pre></td></tr></table></figure></p><p>查看glibc共享库：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">ll /lib64/libc.so.6</div></pre></td></tr></table></figure></p><p>现<code>libc.so.6</code>已经软链接到2.17版本<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">lrwxrwxrwx 1 root root 12 7月  21 10:11 /lib64/libc.so.6 -&gt; libc-2.17.so</div></pre></td></tr></table></figure></p><p>可以查看系统中可使用的glibc版本<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">strings /lib64/libc.so.6 |grep GLIBC_</div></pre></td></tr></table></figure></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div></pre></td><td class="code"><pre><div class="line">GLIBC_2.2.5</div><div class="line">GLIBC_2.2.6</div><div class="line">GLIBC_2.3</div><div class="line">GLIBC_2.3.2</div><div class="line">GLIBC_2.3.3</div><div class="line">GLIBC_2.3.4</div><div class="line">GLIBC_2.4</div><div class="line">GLIBC_2.5</div><div class="line">GLIBC_2.6</div><div class="line">GLIBC_2.7</div><div class="line">GLIBC_2.8</div><div class="line">GLIBC_2.9</div><div class="line">GLIBC_2.10</div><div class="line">GLIBC_2.11</div><div class="line">GLIBC_2.12</div><div class="line">GLIBC_2.13</div><div class="line">GLIBC_2.14</div><div class="line">GLIBC_2.15</div><div class="line">GLIBC_2.16</div><div class="line">GLIBC_2.17</div><div class="line">GLIBC_PRIVATE</div></pre></td></tr></table></figure><p>现在应该就没问题了。</p><p>参考资料：<a href="http://blog.csdn.net/officercat/article/details/39520227" target="_blank" rel="external">Linux/CentOS 升级C基本运行库CLIBC的注意事项</a></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;在CentOS6上安装TensorFlow1.2后，&lt;code&gt;import tensorflow&lt;/code&gt;时出现以下问题：&lt;br&gt;&lt;figure class=&quot;highlight plain&quot;&gt;&lt;table&gt;&lt;tr&gt;&lt;td class=&quot;gutter&quot;&gt;&lt;pre&gt;&lt;d
      
    
    </summary>
    
    
      <category term="centos" scheme="http://libowei.net/tags/centos/"/>
    
      <category term="tensorflow" scheme="http://libowei.net/tags/tensorflow/"/>
    
  </entry>
  
  <entry>
    <title>TensorFlow学习笔记：CIFAR-10 CNN</title>
    <link href="http://libowei.net/TensorFlow%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%EF%BC%9ACNN-CIFAR-10.html"/>
    <id>http://libowei.net/TensorFlow学习笔记：CNN-CIFAR-10.html</id>
    <published>2017-07-19T11:23:41.000Z</published>
    <updated>2017-11-23T04:12:51.166Z</updated>
    
    <content type="html"><![CDATA[<p>官方文档地址：<a href="https://www.tensorflow.org/tutorials/deep_cnn" target="_blank" rel="external">Convolutional Neural Networks</a></p><h2 id="概述"><a href="#概述" class="headerlink" title="概述"></a>概述</h2><p>CIFAR-10分类是机器学习中常见的标准问题。CIFAR-10分类目标是把32*32像素的RGB图像分为10类<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">airplane, automobile, bird, cat, deer, dog, frog, horse, ship, and truck.</div></pre></td></tr></table></figure></p><h3 id="目标"><a href="#目标" class="headerlink" title="目标"></a>目标</h3><p>建立一个小型的CNN用于图像识别，做到：</p><ol><li>规范的组织神经网络的结构、训练和评估</li><li>为构建更大、更复杂的模型提供模板</li></ol><p>选用CIFAR-10数据的原因，一方面它足够复杂，能符合TensorFlow处理大模型的能力；另一方面它数据量较小，训练迅速，可以用来做测试和实验。</p><h3 id="重点"><a href="#重点" class="headerlink" title="重点"></a>重点</h3><p>CIFAR-10教程示范了一些重要的结构，可以用来在TensorFlow种设计大型的复杂的模型</p><ul><li>核心计算部分：卷积（convolution）, 修正线性激活（rectified linear activation）, 最大池化（max pooling）, 局部响应归一化（local response normalization）</li><li>神经网络训练时行为的可视化，包括图像输入，损失，神经网络行为的分布和梯度等。</li><li>计算学到的参数的moving avearage，并在评估中使用以提高预测效果</li><li>实现学习率随时间增加而减少</li><li>预取队列：将磁盘延迟和高代价的图像预处理与模型分开</li></ul><p>提供了多GPU训练的版本，实现了：</p><ul><li>在多个GPU卡间并行训练</li><li>在多个GPU间共享变量、更新变量值</li></ul><h3 id="结构"><a href="#结构" class="headerlink" title="结构"></a>结构</h3><p>该模型是由卷积层和非线性层组成的多层的结构。这些层之后连接全连接层，最后是Softmax分类器。模型结构大致和Alex Krizhevsky提出的模型一致，前面几层略有不同。</p><p>这个模型在单个GPU上训练若干小时后，就能达到非常好的效果：86%正确率。模型由1068298个可学习的参数组成，对一个图像分类需要19.5M次乘加操作。</p><h2 id="代码组织"><a href="#代码组织" class="headerlink" title="代码组织"></a>代码组织</h2><p>代码在<a href="https://www.tensorflow.org/code/tensorflow_models/tutorials/image/cifar10/" target="_blank" rel="external"><code>models/tutorials/image/cifar10/</code></a></p><ul><li><code>cifar10_input.py</code> 读原始的二进制CIFAR-10数据</li><li><code>cifar10.py</code> 建立模型</li><li><code>cifar10_train.py</code> 在CPU或GPU上训练模型</li><li><code>cifar10_multi_gpu_train.py</code> 在多GPU环境中训练模型</li><li><code>cifar10_eval.py</code> 评估模型</li></ul><h2 id="CIFAR-10模型"><a href="#CIFAR-10模型" class="headerlink" title="CIFAR-10模型"></a>CIFAR-10模型</h2><p>神经网络模型代码在<code>cifar10.py</code>中。全部的训练图包括765个操作。建立下面的模块，编写重用性高的图结构代码：</p><ol><li>模型输入：<code>inputs()</code>和<code>disorted_inputs()</code>，为训练和评估读入、预处理图像数据</li><li>模型预测：<code>inference()</code> 对提供的图片进行分类</li><li>模型训练：<code>loss()</code>和<code>train()</code>，计算损失、梯度、更新变量、结果可视化</li></ol><h3 id="模型输入"><a href="#模型输入" class="headerlink" title="模型输入"></a>模型输入</h3><p>输入模块由<code>inputs()</code>和<code>distorted_inputs()</code>构成，两个函数读入CIFAR-10二进制数据，文件由固定字节长度的记录组成，所以使用<code>tf.FixedLengthRecordReader</code></p><p>图像被处理成：</p><ul><li>裁切为24*24像素，评估裁剪中间部分，训练时随机</li><li>近似白化处理，使模型对图片动态的范围变化不敏感</li></ul><p>从磁盘中读图片需要相当长的处理时间，为避免其使训练时间变长，使用16个独立的线程读图片来填充TenorFlow队列。</p><h3 id="模型预测"><a href="#模型预测" class="headerlink" title="模型预测"></a>模型预测</h3><p>预测部分的代码在<code>inference()</code>中，计算预测的得分（logits），这部分的代码组织如下：</p><ul><li><code>conv1</code> 卷积、修正线性激活（rectified linear activation）</li><li><code>pool1</code> 最大池化</li><li><code>norm1</code> 局部响应归一化</li><li><code>conv2</code> 卷积、修正线性激活</li><li><code>norm2</code> 局部响应归一化</li><li><code>pool2</code> 最大池化</li><li><code>local3</code> 带有“修正线性激活的”的全连接</li><li><code>local4</code> 带有“修正线性激活的”的全连接</li><li><code>softmax_linear</code> 线性变换，输出结果</li></ul><p>下图由TensorBoard生成，展示预测部分的操作</p><p><img src="https://www.tensorflow.org/images/cifar_graph.png" alt=""></p><blockquote><p>练习：<code>inference</code>的输出是未归一化的logits，尝试使用<code>tf.nn.softmax</code>修改网络结构，使其返回归一化的预测结果</p><p>练习：<code>inference</code>中的模型结构和<a href="https://code.google.com/p/cuda-convnet/" target="_blank" rel="external">cuda-convnet</a>中的CIFAR-10模型有些许的不同。其中，在Alex的原始模型中，最上几层是局部连接而非全连接。尝试修改网络结构，在最上层形成局部连接的结构。</p></blockquote><h3 id="模型训练"><a href="#模型训练" class="headerlink" title="模型训练"></a>模型训练</h3><p>训练多分类网络常用的方法是多项式逻辑回归，如softmax regression。Softmax回归对结果应用非线性的softmax，计算归一化的预测结果与one-hot编码标签的交叉熵。为了正则化，使学习的变量的权重逐渐减小。模型的目标函数是交叉熵损失和，和权重衰减项的和，在<code>loss()</code>函数中返回。</p><p>应用<code>tf.summary.scalar</code>，在TensorBoard中将这一过程展示：<br><img src="https://www.tensorflow.org/images/cifar_loss.png" alt=""></p><p>使用标准的梯度下降算法来训练，学习率随时间变化呈指数衰减：<br><img src="https://www.tensorflow.org/images/cifar_lr_decay.png" alt=""></p><p><code>train()</code>中，通过计算梯度、更新学习变量（<code>tf.train.GradientDescentOptimizer</code>），使目标函数最小化。<code>train()</code>返回一个操作，这个操作中执行一批图像的计算，以训练和更新模型。</p><h2 id="启动和训练模型"><a href="#启动和训练模型" class="headerlink" title="启动和训练模型"></a>启动和训练模型</h2><p>让训练跑起来：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">python cifar10_train.py</div></pre></td></tr></table></figure></p><p>结果如下：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div></pre></td><td class="code"><pre><div class="line">Filling queue with 20000 CIFAR images before starting to train. This will take a few minutes.</div><div class="line">2015-11-04 11:45:45.927302: step 0, loss = 4.68 (2.0 examples/sec; 64.221 sec/batch)</div><div class="line">2015-11-04 11:45:49.133065: step 10, loss = 4.66 (533.8 examples/sec; 0.240 sec/batch)</div><div class="line">2015-11-04 11:45:51.397710: step 20, loss = 4.64 (597.4 examples/sec; 0.214 sec/batch)</div><div class="line">2015-11-04 11:45:54.446850: step 30, loss = 4.62 (391.0 examples/sec; 0.327 sec/batch)</div><div class="line">2015-11-04 11:45:57.152676: step 40, loss = 4.61 (430.2 examples/sec; 0.298 sec/batch)</div><div class="line">2015-11-04 11:46:00.437717: step 50, loss = 4.59 (406.4 examples/sec; 0.315 sec/batch)</div><div class="line">...</div></pre></td></tr></table></figure></p><p>每10步显示一次损失、训练速度。下面是一些提示：</p><ul><li>第一批(batch)数据训练很慢，因为需要预处理线程读取20000张CIFAR图像，打乱顺序加入队列。</li><li>显示的损失是最近一批数据的平均损失，这个损失是交叉熵与权重衰减项之和</li><li>在Tesla K40c上得到显示的训练速度，如果用CPU训练，速度较慢。</li></ul><blockquote><p>练习：在实验时，第一步训练耗时太长。尝试减少填入队列的图像的数目。在<code>cifar10_input.py</code>中查找<code>min_fraction_of_examples_in_queue</code></p></blockquote><p><code>cifar10_train.py</code>周期性的以checkpoints files保存模型参数，但不评估模型。Checkpoints会在<code>cifar10_eval.py</code>中使用，用来预测模型性能。</p><p>如果读完了前面的步骤，现在可以训练CIFAR-10模型了。Congratulations!</p><p><code>cifar10_train.py</code>返回的文字中包含少量的模型训练的信息。我们需要更多的训练时的信息，包括：</p><ul><li>损失是真实的在减小，还是只是噪声？</li><li>为模型提供的图片是否合适？</li><li>梯度、激活值、权重是否合理？</li><li>学习率是多少</li></ul><p>TensorBoard提供了这些功能。在<code>cifar10_train.py</code>中，通过<code>tf.summary.FileWriter</code>周期性的显示这些数据。</p><p>例如，可以观察激活值的分布和特征的稀疏程度：<br><img src="https://www.tensorflow.org/images/cifar_sparsity.png" alt=""><br><img src="https://www.tensorflow.org/images/cifar_activations.png" alt=""></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;官方文档地址：&lt;a href=&quot;https://www.tensorflow.org/tutorials/deep_cnn&quot; target=&quot;_blank&quot; rel=&quot;external&quot;&gt;Convolutional Neural Networks&lt;/a&gt;&lt;/p&gt;
&lt;h2 i
      
    
    </summary>
    
    
      <category term="tensorflow" scheme="http://libowei.net/tags/tensorflow/"/>
    
      <category term="卷积神经网络" scheme="http://libowei.net/tags/%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/"/>
    
      <category term="深度学习" scheme="http://libowei.net/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
  </entry>
  
  <entry>
    <title>TensorFlow学习笔记：CNN MNIST</title>
    <link href="http://libowei.net/TensorFlow%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%EF%BC%9ACNN-MNIST.html"/>
    <id>http://libowei.net/TensorFlow学习笔记：CNN-MNIST.html</id>
    <published>2017-07-18T11:17:12.000Z</published>
    <updated>2017-11-23T04:12:57.883Z</updated>
    
    <content type="html"><![CDATA[<p>官方文档地址：<a href="https://www.tensorflow.org/tutorials/layers" target="_blank" rel="external">A Guide to TF Layers: Building a Convolutional Neural Network</a></p><h2 id="卷积神经网络"><a href="#卷积神经网络" class="headerlink" title="卷积神经网络"></a>卷积神经网络</h2><p>卷积神经网络是图片识别任务最先进的模型。</p><p>CNN应用一系列filters在原始像素数据上，提取学习高级特征，模型使用这些特征来分类。</p><h3 id="CNN组成"><a href="#CNN组成" class="headerlink" title="CNN组成"></a>CNN组成</h3><ol><li>卷积层： 指定数量的卷积核。对图像的每个区域进行一系列数学计算，得到单一的值，作为特征。再使用ReLU作为激活函数，以引入非线性特征。</li><li>池化层：对卷积得到的特征再次降维以减少处理时间。常用max_pooling，对每个2*2区域保留最大啊值，丢弃其他值。</li><li>全连接层：对卷积层提取、池化层采样的特征进行分类。每个节点都和上一层的每个节点相连接。</li></ol><p>CNN由一堆卷积模块组成，每个模块都有卷积层，后跟一池化层。</p><p>最后一个卷积模块包含一个或多个全连接层，用作分类。</p><p>最后一个全连接层中，每个类别有一个节点（使模型能预测所有类别），用softmax作为激活函数（所有softmax值和为1），每个softmax值可以解释为图像属于该类的概率。</p><p>斯坦福CNN课程资料<br><a href="http://cs231n.github.io/convolutional-networks/" target="_blank" rel="external">http://cs231n.github.io/convolutional-networks/</a></p><h2 id="使用CNN构建MNIST分类器"><a href="#使用CNN构建MNIST分类器" class="headerlink" title="使用CNN构建MNIST分类器"></a>使用CNN构建MNIST分类器</h2><h3 id="CNN结构"><a href="#CNN结构" class="headerlink" title="CNN结构"></a>CNN结构</h3><ul><li>卷积层-1：32个5*5filter，ReLU激活函数</li><li>池化层-1：:2*2max pooling，步长2（使采样区域不重复）</li><li>卷积层-2：64个5*5filter, ReLu激活函数</li><li>池化层-2：2*2max pooling，步长2</li><li>全连接层-1：1024神经元，dropout率0.4（训练中0.4概率给定的元素被丢弃）</li><li>全连接层-2：10神经元，每个代表一类</li></ul><p>使用tf.layers模块构建上述各类型的层</p><ul><li>conv2d 2维卷积层，给定卷积核数量、大小、padding、激活函数作为参数</li><li>max_pooling2d    2维池化层（max_pooling），给定filter大小、步长作为参数</li><li>dense    全连接层，参数为神经元数量、激活函数</li></ul><p>以上方法都接收tensor作为输入，然后输出一个处理后的tensor。可以直接使用返回值作为下一层的输入。</p><h3 id="cnn-model-fn"><a href="#cnn-model-fn" class="headerlink" title="cnn_model_fn()"></a>cnn_model_fn()</h3><p><code>cnn_mnist.py</code>接受MNIST特征数据、标签、模型类型（TRAIN、EVAL、INFER）作为输入参数，配置CNN网络，返回预测结果、损失和训练步骤。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div><div class="line">42</div><div class="line">43</div><div class="line">44</div><div class="line">45</div><div class="line">46</div><div class="line">47</div><div class="line">48</div><div class="line">49</div><div class="line">50</div><div class="line">51</div><div class="line">52</div><div class="line">53</div><div class="line">54</div><div class="line">55</div><div class="line">56</div><div class="line">57</div><div class="line">58</div><div class="line">59</div><div class="line">60</div><div class="line">61</div><div class="line">62</div></pre></td><td class="code"><pre><div class="line">def cnn_model_fn(features, labels, mode):</div><div class="line">  &quot;&quot;&quot;Model function for CNN.&quot;&quot;&quot;</div><div class="line">  # Input Layer</div><div class="line">  input_layer = tf.reshape(features, [-1, 28, 28, 1])</div><div class="line"></div><div class="line">  # Convolutional Layer #1</div><div class="line">  conv1 = tf.layers.conv2d(</div><div class="line">      inputs=input_layer,</div><div class="line">      filters=32,</div><div class="line">      kernel_size=[5, 5],</div><div class="line">      padding=&quot;same&quot;,</div><div class="line">      activation=tf.nn.relu)</div><div class="line"></div><div class="line">  # Pooling Layer #1</div><div class="line">  pool1 = tf.layers.max_pooling2d(inputs=conv1, pool_size=[2, 2], strides=2)</div><div class="line"></div><div class="line">  # Convolutional Layer #2 and Pooling Layer #2</div><div class="line">  conv2 = tf.layers.conv2d(</div><div class="line">      inputs=pool1,</div><div class="line">      filters=64,</div><div class="line">      kernel_size=[5, 5],</div><div class="line">      padding=&quot;same&quot;,</div><div class="line">      activation=tf.nn.relu)</div><div class="line">  pool2 = tf.layers.max_pooling2d(inputs=conv2, pool_size=[2, 2], strides=2)</div><div class="line"></div><div class="line">  # Dense Layer</div><div class="line">  pool2_flat = tf.reshape(pool2, [-1, 7 * 7 * 64])</div><div class="line">  dense = tf.layers.dense(inputs=pool2_flat, units=1024, activation=tf.nn.relu)</div><div class="line">  dropout = tf.layers.dropout(</div><div class="line">      inputs=dense, rate=0.4, training=mode == learn.ModeKeys.TRAIN)</div><div class="line"></div><div class="line">  # Logits Layer</div><div class="line">  logits = tf.layers.dense(inputs=dropout, units=10)</div><div class="line"></div><div class="line">  loss = None</div><div class="line">  train_op = None</div><div class="line"></div><div class="line">  # Calculate Loss (for both TRAIN and EVAL modes)</div><div class="line">  if mode != learn.ModeKeys.INFER:</div><div class="line">    onehot_labels = tf.one_hot(indices=tf.cast(labels, tf.int32), depth=10)</div><div class="line">    loss = tf.losses.softmax_cross_entropy(</div><div class="line">        onehot_labels=onehot_labels, logits=logits)</div><div class="line"></div><div class="line">  # Configure the Training Op (for TRAIN mode)</div><div class="line">  if mode == learn.ModeKeys.TRAIN:</div><div class="line">    train_op = tf.contrib.layers.optimize_loss(</div><div class="line">        loss=loss,</div><div class="line">        global_step=tf.contrib.framework.get_global_step(),</div><div class="line">        learning_rate=0.001,</div><div class="line">        optimizer=&quot;SGD&quot;)</div><div class="line"></div><div class="line">  # Generate Predictions</div><div class="line">  predictions = &#123;</div><div class="line">      &quot;classes&quot;: tf.argmax(</div><div class="line">          input=logits, axis=1),</div><div class="line">      &quot;probabilities&quot;: tf.nn.softmax(</div><div class="line">          logits, name=&quot;softmax_tensor&quot;)</div><div class="line">  &#125;</div><div class="line"></div><div class="line">  # Return a ModelFnOps object</div><div class="line">  return model_fn_lib.ModelFnOps(</div><div class="line">      mode=mode, predictions=predictions, loss=loss, train_op=train_op)</div></pre></td></tr></table></figure><p>上述代码使用<code>tf.layers</code>模块构建各个层，计算损失，配置训练步骤，并生成预测。</p><h4 id="输入层"><a href="#输入层" class="headerlink" title="输入层"></a>输入层</h4><p>对2维的图像数据构建卷积层和池化层需要输入的tensor的形状为<code>[batch_size,image_width,image_height, channels]</code></p><ul><li>batch_size 梯度下降训练时子集大小</li><li>image_width 输入图像宽度</li><li>image_height 输入图像高度</li><li>channels 彩色图片为3（红绿蓝）,黑白图片为1（黑）</li></ul><p>MNIST数据为单色28*28像素，所以输入的shape为<code>[batch_size, 28, 28, 1]</code></p><p>把输入转换为该shape，执行<code>reshape</code></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">input_layer = tf.reshape(features, [-1, 28, 28, 1])</div></pre></td></tr></table></figure><p>上面的<code>batch_size</code>为-1表示该维度不确定，由输入的<code>features</code>动态计算。这允许我们把<code>batch_size</code>当做可调整的超参数。例如：batch为5时，<code>features</code>包含3920 (5<em>28</em>28)个值，shape为<code>[5,28,28,1]</code>，batch为100时，输入的值个数为78400</p><h4 id="卷积层-1"><a href="#卷积层-1" class="headerlink" title="卷积层-1"></a>卷积层-1</h4><p>对输入层应用32个5*5的filter，并使用ReLU作为激活函数。可以使用<code>conv2d()</code>函数来创建这一层</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div></pre></td><td class="code"><pre><div class="line">conv1 = tf.layers.conv2d(</div><div class="line">    inputs=input_layer,</div><div class="line">    filters=32,</div><div class="line">    kernel_size=[5, 5],</div><div class="line">    padding=&quot;same&quot;,</div><div class="line">    activation=tf.nn.relu)</div></pre></td></tr></table></figure><p>输入的tensor的形状必须是<code>[batch_size, image_width,image_height,channels]</code>,这样可以与输入层相连。</p><ul><li>filters 卷积核数量</li><li>kernel_size 卷积核维度</li><li>padding 为”valid”或”same”。padding=”same”时，输出的tensor和输入tensor维度相同（边缘处补0）。没有padding的话，28<em>28经过5</em>5卷积，会产生24*24的tensor</li><li>activation 指定激活函数，这里使用ReLU（tf.nn.relu）</li></ul><p>输出tensor的形状为<code>[batch_size,28,28,32]</code>，宽度和高度和原来相同，但经过32个卷积核卷积，现在有32个channels</p><h4 id="池化层-1"><a href="#池化层-1" class="headerlink" title="池化层-1"></a>池化层-1</h4><p>现在可以把第一个pooling层和上面创建好的卷积层连接。使用<code>max_pooling2d()</code>构建池化层</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">pool1 = tf.layers.max_pooling2d(inputs=conv1, pool_size=[2, 2], strides=2)</div></pre></td></tr></table></figure><ul><li>inputs 输入tensor。形状为<code>[batch_size, image_width, image_height, channels]</code>。这里的输入为上一层的输出，形状是<code>[batch_size, 28, 28, 32]</code></li><li>pool_size max pool filter的大小（[width, height]），这里为[2,2]</li><li>strides 步长的大小。2表示长和宽每2像素划分为一个子区域（2*2的filter使用strides=2，使子区域没有重叠）。也可设置其他大小，如[3,6]</li></ul><p><code>max_pooling2d()</code>输入的tensor形状为<code>[batch_size, 14, 14, 32]</code>，2*2的filter把高度和宽度减少了50%</p><h4 id="卷积层-2和池化层-2"><a href="#卷积层-2和池化层-2" class="headerlink" title="卷积层-2和池化层-2"></a>卷积层-2和池化层-2</h4><p>和前面一样，第二个卷积层和池化层使用<code>conv2d()</code>和<code>max_pooling2d()</code>。在这里，卷积层使用64个5<em>5卷积核，池化层的设置与前面一样（2</em>2的max pool，步长2）</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div></pre></td><td class="code"><pre><div class="line">conv2 = tf.layers.conv2d(</div><div class="line">    inputs=pool1,</div><div class="line">    filters=64,</div><div class="line">    kernel_size=[5, 5],</div><div class="line">    padding=&quot;same&quot;,</div><div class="line">    activation=tf.nn.relu)</div><div class="line"></div><div class="line">pool2 = tf.layers.max_pooling2d(inputs=conv2, pool_size=[2, 2], strides=2)</div></pre></td></tr></table></figure><p>第二层卷积层使用第一层池化层的输出<code>pool1</code>作为输入，输出<code>h_conv2</code>tensor，<code>h_conv2</code>的形状为<code>[batch_size,14,14,64]</code>，高和宽与pool1相同，使用64个卷积核，所以channel为64</p><p>第二层池化层的形状为<code>[batch_size, 7, 7, 64]</code>（高和宽再次减少50%）</p><h4 id="全连接层"><a href="#全连接层" class="headerlink" title="全连接层"></a>全连接层</h4><p>接下来添加一层包含1024神经元，使用ReLU的dense层，对卷积/池化采样的特征进行分类。在连接该层前，需要把特征映射（feature map）变换形状，使其变为2维</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">pool2_flat = tf.reshape(pool2, [-1, 7 * 7 * 64])</div></pre></td></tr></table></figure><p>-1表示<code>batch_size</code>维度不定，由输入的样本数量动态计算。每个样本有7<em>7</em>64个特征（长度/宽度/channels）。现在<code>pool2_flat</code>的形状为<code>[batch_size,3136]</code></p><p>使用<code>dense()</code>连接该dense层</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">dense = tf.layers.dense(inputs=pool2_flat, units=1024, activation=tf.nn.relu)</div></pre></td></tr></table></figure><ul><li>input 输入的tensor （展开的特征映射）</li><li>units 神经元数量</li><li>activation 激活函数</li></ul><p>为了提高模型效果，需要添加dropout regularization，使用<code>tf.layers.dropout()</code></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">dropout = tf.layers.dropout(</div><div class="line">    inputs=dense, rate=0.4, training=mode == learn.ModeKeys.TRAIN)</div></pre></td></tr></table></figure><ul><li>input输入的tensor（<code>dense</code>）</li><li>rate dropout率：训练中40%的元素被随机丢弃</li><li>training 指定模型是否正在训练中。dropout只有在training=True时被启用。</li></ul><h4 id="Logits层"><a href="#Logits层" class="headerlink" title="Logits层"></a>Logits层</h4><p>神经网络最后一层是logits层，返回预测的原始值。该层有10个神经元（代表数字0-9）,使用线性激活函数（默认）</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">logits = tf.layers.dense(inputs=dropout, units=10)</div></pre></td></tr></table></figure><h4 id="计算损失"><a href="#计算损失" class="headerlink" title="计算损失"></a>计算损失</h4><p>训练和评估模型时，需要定义损失函数来度量模型预测结果和真实值的差异。多分类问题（例如MNIST）中，交叉熵（cross entropy）是典型的损失度量。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div></pre></td><td class="code"><pre><div class="line">loss = None</div><div class="line">train_op = None</div><div class="line"></div><div class="line"># Calculate loss for both TRAIN and EVAL modes</div><div class="line">if mode != learn.ModeKeys.INFER:</div><div class="line">  onehot_labels = tf.one_hot(indices=tf.cast(labels, tf.int32), depth=10)</div><div class="line">  loss = tf.losses.softmax_cross_entropy(</div><div class="line">      onehot_labels=onehot_labels, logits=logits)</div></pre></td></tr></table></figure><p><code>labels</code> tensor包括一组样本的预测分类值，例如[1,9….]。为了计算交叉熵，需要将其变为one-hot编码：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line">[[0, 1, 0, 0, 0, 0, 0, 0, 0, 0],</div><div class="line"> [0, 0, 0, 0, 0, 0, 0, 0, 0, 1],</div><div class="line"> ...]</div></pre></td></tr></table></figure><p>使用<code>tf.onehot()</code>进行转换。</p><ul><li>indices 输入，哪个位置变为one-hot值</li><li>depth 类别的数目</li></ul><p>下面的代码生成one-hot tensor</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">onehot_labels = tf.one_hot(indices=tf.cast(labels, tf.int32), depth=10)</div></pre></td></tr></table></figure><p><code>labels</code> 中为0-9的数值（[1,9…]），将其转换为int类型。<code>depth</code>为10，因为有10个可能的目标类别。</p><p>使用<code>tf.losses.softmax_cross_entropy()</code>对logits层进行softmax，计算交叉熵，结果为标量tensor</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">loss = tf.losses.softmax_cross_entropy(</div><div class="line">        onehot_labels=onehot_labels, logits=logits)</div></pre></td></tr></table></figure><h4 id="设置训练步骤"><a href="#设置训练步骤" class="headerlink" title="设置训练步骤"></a>设置训练步骤</h4><p>前面定义了CNN结构，现在设置训练步骤，以优化训练损失。使用<code>tf.contrib.layers.optimize_loss()</code>, 这里使用0.001学习率，并使用随机梯度下降（stochastic gradient descent）作为优化算法</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div></pre></td><td class="code"><pre><div class="line"># Configure the Training Op (for TRAIN mode)</div><div class="line">if mode == learn.ModeKeys.TRAIN:</div><div class="line">    train_op = tf.contrib.layers.optimize_loss(</div><div class="line">        loss=loss,</div><div class="line">        global_step=tf.contrib.framework.get_global_step(),</div><div class="line">        learning_rate=0.001,</div><div class="line">        optimizer=&quot;SGD&quot;)</div></pre></td></tr></table></figure><h4 id="预测"><a href="#预测" class="headerlink" title="预测"></a>预测</h4><p>logits层返回[batch_size, 10]的tensor，现将这些原始值转换为两种形式</p><ol><li>预测的类别</li><li>各类别的概率<br>例子中，预测的类别是返回的tensor中每行最大值对应的列，使用<code>tf.argmax</code>找到其索引</li></ol><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">tf.argmax(input=logits, axis=1)</div></pre></td></tr></table></figure><ul><li>input 从该tensor中提取最大值</li><li>axis  从哪个维度提取（logits形状为[batch_size, 10]）</li></ul><p>通过<code>tf.nn.softmax()</code>得到概率</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">tf.nn.softmax(logits, name=&quot;softmax_tensor&quot;)</div></pre></td></tr></table></figure><p>将输出整理为字典形式：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div></pre></td><td class="code"><pre><div class="line">predictions = &#123;</div><div class="line">    &quot;classes&quot;: tf.argmax(</div><div class="line">        input=logits, axis=1),</div><div class="line">    &quot;probabilities&quot;: tf.nn.softmax(</div><div class="line">        logits, name=&quot;softmax_tensor&quot;)</div><div class="line">&#125;</div></pre></td></tr></table></figure><p>现在，得到了<code>predictions</code>,<code>loss</code>,<code>train_op</code>，连同<code>mode</code>参数可以把它们返回。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line"># Return a ModelFnOps object</div><div class="line">return model_fn_lib.ModelFnOps(</div><div class="line">    mode=mode, predictions=predictions, loss=loss, train_op=train_op)</div></pre></td></tr></table></figure><h2 id="训练和评估CNN-MNIST模型"><a href="#训练和评估CNN-MNIST模型" class="headerlink" title="训练和评估CNN MNIST模型"></a>训练和评估CNN MNIST模型</h2><h3 id="加载训练集-测试集"><a href="#加载训练集-测试集" class="headerlink" title="加载训练集/测试集"></a>加载训练集/测试集</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div></pre></td><td class="code"><pre><div class="line">def main(unused_argv):</div><div class="line">  # Load training and eval data</div><div class="line">  mnist = learn.datasets.load_dataset(&quot;mnist&quot;)</div><div class="line">  train_data = mnist.train.images # Returns np.array</div><div class="line">  train_labels = np.asarray(mnist.train.labels, dtype=np.int32)</div><div class="line">  eval_data = mnist.test.images # Returns np.array</div><div class="line">  eval_labels = np.asarray(mnist.test.labels, dtype=np.int32)</div></pre></td></tr></table></figure><p>把训练集数据（55000图像的原始像素）和标签（0-9）以numpy arrays保存在<code>train_data</code>和<code>train_lable</code>中。<br>相似的，保存10000数据和标签作为测试集。</p><h3 id="创建Estimator"><a href="#创建Estimator" class="headerlink" title="创建Estimator"></a>创建Estimator</h3><p>Estimator是TensorFlow用来进行高级模型训练、评价和预测的类。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line"># Create the Estimator</div><div class="line">mnist_classifier = learn.Estimator(</div><div class="line">      model_fn=cnn_model_fn, model_dir=&quot;/tmp/mnist_convnet_model&quot;)</div></pre></td></tr></table></figure><ul><li>model_fn 指定训练/评估/预测的模型</li><li>model_dir 保存checkpoints的路径</li></ul><h3 id="Logging"><a href="#Logging" class="headerlink" title="Logging"></a>Logging</h3><p>创建日志以跟踪训练过程。创建一个<code>tf.train.LoggingTensorHook</code>，会记录Softmax层计算出的概率</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div></pre></td><td class="code"><pre><div class="line"># Set up logging for predictions</div><div class="line">  tensors_to_log = &#123;&quot;probabilities&quot;: &quot;softmax_tensor&quot;&#125;</div><div class="line">  logging_hook = tf.train.LoggingTensorHook(</div><div class="line">      tensors=tensors_to_log, every_n_iter=50)</div></pre></td></tr></table></figure><ul><li>将需要记录的tensors存入tensor_to_log字典，key作为日志中的标签，相应的值是指定tensor的名称（name属性）</li><li>every_n_iter 每训练50步记录一次</li></ul><h3 id="训练模型"><a href="#训练模型" class="headerlink" title="训练模型"></a>训练模型</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div></pre></td><td class="code"><pre><div class="line"># Train the model</div><div class="line">mnist_classifier.fit(</div><div class="line">    x=train_data,</div><div class="line">    y=train_labels,</div><div class="line">    batch_size=100,</div><div class="line">    steps=20000,</div><div class="line">    monitors=[logging_hook])</div></pre></td></tr></table></figure><ul><li>x 训练数据</li><li>y 训练集标签</li><li>batch 每次训练样本数</li><li>steps 训练次数</li><li>monitors logging_hook</li></ul><h3 id="评估模型"><a href="#评估模型" class="headerlink" title="评估模型"></a>评估模型</h3><p>在MNIST测试集上评价训练好的模型。使用<code>tf.contrib.learn.MetricSpec</code>创建metrics字典来计算准确率</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div></pre></td><td class="code"><pre><div class="line"># Configure the accuracy metric for evaluation</div><div class="line">metrics = &#123;</div><div class="line">    &quot;accuracy&quot;:</div><div class="line">        learn.MetricSpec(</div><div class="line">            metric_fn=tf.metrics.accuracy, prediction_key=&quot;classes&quot;),</div><div class="line">&#125;</div></pre></td></tr></table></figure><ul><li>metric_fn 计算metric的函数，这里使用预定义的<code>tf.metrics.accuracy</code></li><li>prediction_key 包含预测值的tensor的名称（前面定义过）</li></ul><p>通过下面代码打印评估结果：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div></pre></td><td class="code"><pre><div class="line"># Evaluate the model and print results</div><div class="line">eval_results = mnist_classifier.evaluate(</div><div class="line">    x=eval_data, y=eval_labels, metrics=metrics)</div><div class="line">print(eval_results)</div></pre></td></tr></table></figure><h3 id="运行模型"><a href="#运行模型" class="headerlink" title="运行模型"></a>运行模型</h3><p>结果显示如下</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div></pre></td><td class="code"><pre><div class="line">INFO:tensorflow:loss = 2.36026, step = 1</div><div class="line">INFO:tensorflow:probabilities = [[ 0.07722801  0.08618255  0.09256398, ...]]</div><div class="line">...</div><div class="line">INFO:tensorflow:loss = 2.13119, step = 101</div><div class="line">INFO:tensorflow:global_step/sec: 5.44132</div><div class="line">...</div><div class="line">INFO:tensorflow:Loss for final step: 0.553216.</div><div class="line"></div><div class="line">INFO:tensorflow:Restored model from /tmp/mnist_convnet_model</div><div class="line">INFO:tensorflow:Eval steps [0,inf) for training step 20000.</div><div class="line">INFO:tensorflow:Input iterator is exhausted.</div><div class="line">INFO:tensorflow:Saving evaluation summary for step 20000: accuracy = 0.9733, loss = 0.0902271</div><div class="line">&#123;&apos;loss&apos;: 0.090227105, &apos;global_step&apos;: 20000, &apos;accuracy&apos;: 0.97329998&#125;</div></pre></td></tr></table></figure>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;官方文档地址：&lt;a href=&quot;https://www.tensorflow.org/tutorials/layers&quot; target=&quot;_blank&quot; rel=&quot;external&quot;&gt;A Guide to TF Layers: Building a Convolutiona
      
    
    </summary>
    
    
      <category term="tensorflow" scheme="http://libowei.net/tags/tensorflow/"/>
    
      <category term="卷积神经网络" scheme="http://libowei.net/tags/%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/"/>
    
      <category term="深度学习" scheme="http://libowei.net/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
  </entry>
  
  <entry>
    <title>网络攻防实验(3)-逆向分析</title>
    <link href="http://libowei.net/%E7%BD%91%E7%BB%9C%E6%94%BB%E9%98%B2%E5%AE%9E%E9%AA%8C-3-%E9%80%86%E5%90%91%E5%88%86%E6%9E%90.html"/>
    <id>http://libowei.net/网络攻防实验-3-逆向分析.html</id>
    <published>2017-03-31T11:39:31.000Z</published>
    <updated>2017-11-10T04:14:02.711Z</updated>
    
    <content type="html"><![CDATA[<h2 id="题目说明"><a href="#题目说明" class="headerlink" title="题目说明"></a>题目说明</h2><p>说明：该题目由CrackMee.exe一个文件组成。该文件是一个简单的小程序，请大家逆向该程序，找到正确的字符串，输入到程序后即为成功。</p><h2 id="题目下载"><a href="#题目下载" class="headerlink" title="题目下载"></a>题目下载</h2><p><a href="/file/网络攻防实验3.rar">实验程序下载</a></p><h2 id="使用工具"><a href="#使用工具" class="headerlink" title="使用工具"></a>使用工具</h2><ul><li>IDA pro</li></ul><h2 id="解题步骤"><a href="#解题步骤" class="headerlink" title="解题步骤"></a>解题步骤</h2><ol><li><p>使用IDA pro F5查看反编译的程序代码。<br><img src="/image/网络攻防实验3-1.png" alt="代码"><br>这段程序就是构造出一个字符串，输入正确的话，就输出”Key Right!”。构造过程如下：前5个字节从q中取得，第6到9字节通过x和ans获得，第10字节为’3’(ASCII码51)，第11字节为’y’(ASCII码121)</p></li><li><p>找到以上变量的值。<br><img src="/image/网络攻防实验3-2.png" alt="变量q"><br>q为DWORD型，每四字节取一个值，可得v5前5字节为”thi5_”<br><img src="/image/网络攻防实验3-3.png" alt="变量x"><br>这里可找到x的值。<br><img src="/image/网络攻防实验3-4.png" alt="变量ans"><br>同理，可找到ans的值。</p></li><li><p>编写一个程序来构造目标字符串的第6-11字节，参照(1)中的代码。这里我选择用python来写<br><img src="/image/网络攻防实验3-5.png" alt="构造字符串"><br>运行结果为:<br><img src="/image/网络攻防实验3-6.png" alt="运行结果"><br>即要输入的字符串为<code>&quot;thi5_1s_K3y&quot;</code></p></li></ol><h2 id="实验结果"><a href="#实验结果" class="headerlink" title="实验结果"></a>实验结果</h2><p><img src="/image/网络攻防实验3-7.png" alt="实验结果"></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h2 id=&quot;题目说明&quot;&gt;&lt;a href=&quot;#题目说明&quot; class=&quot;headerlink&quot; title=&quot;题目说明&quot;&gt;&lt;/a&gt;题目说明&lt;/h2&gt;&lt;p&gt;说明：该题目由CrackMee.exe一个文件组成。该文件是一个简单的小程序，请大家逆向该程序，找到正确的字符串，输入到程序
      
    
    </summary>
    
    
      <category term="网络攻防实验" scheme="http://libowei.net/tags/%E7%BD%91%E7%BB%9C%E6%94%BB%E9%98%B2%E5%AE%9E%E9%AA%8C/"/>
    
      <category term="逆向分析" scheme="http://libowei.net/tags/%E9%80%86%E5%90%91%E5%88%86%E6%9E%90/"/>
    
  </entry>
  
  <entry>
    <title>网络攻防实验(2)-缓冲区溢出</title>
    <link href="http://libowei.net/%E7%BD%91%E7%BB%9C%E6%94%BB%E9%98%B2%E5%AE%9E%E9%AA%8C-2-%E7%BC%93%E5%86%B2%E5%8C%BA%E6%BA%A2%E5%87%BA.html"/>
    <id>http://libowei.net/网络攻防实验-2-缓冲区溢出.html</id>
    <published>2017-03-31T09:13:35.000Z</published>
    <updated>2017-11-10T04:13:56.253Z</updated>
    
    <content type="html"><![CDATA[<h2 id="题目说明"><a href="#题目说明" class="headerlink" title="题目说明"></a>题目说明</h2><p>说明：该题目由memory、flag两个文件组成。memory为可执行代码，存在缓冲区溢出漏洞，请同学们分析该程序的二进制代码找到漏洞成因，并通过修改程序的执行流程导致任意代码执行漏洞将flag中的FLAG信息输出出来。<br>单机执行：请同学们在自己的机器上运行脚本：./host，5555端口即为程序输入端口。如果编写exploit攻击成功，会出现以下提示：<br>{FLAG:this is a flag}</p><h2 id="题目下载"><a href="#题目下载" class="headerlink" title="题目下载"></a>题目下载</h2><p><a href="/file/网络攻防实验2.rar">实验程序下载</a></p><h2 id="使用工具"><a href="#使用工具" class="headerlink" title="使用工具"></a>使用工具</h2><ul><li>IDA pro</li><li>pwntools(python2.7)</li><li>socat </li><li>linux</li></ul><h2 id="解题步骤"><a href="#解题步骤" class="headerlink" title="解题步骤"></a>解题步骤</h2><ol><li><p>使用IDA pro打开程序文件，按F5查看其反编译代码<br><img src="/image/网络攻防实验2-1.png" alt="代码1"><br><img src="/image/网络攻防实验2-2.png" alt="代码2"><br>观察到mem_test()函数中调用了scanf()，可能造成缓冲区溢出。推测此题的解题思路为，scanf读入一定长度的字符，覆盖mem_test()的返回地址，导致任意代码执行。</p></li><li><p>寻找要执行的函数。打开函数视图，找到win_func()函数，这个函数直接调用了system(),可以传递参数为”cat flag”，从而显示flag文件中的内容。<br><img src="/image/网络攻防实验2-3.png" alt="win_func"></p></li><li><p>使用IDA pro远程调试linux下的memory程序，在scanf()后添加断点。输入若干’0’后，观察栈变化。<br><img src="/image/网络攻防实验2-4.png" alt="栈"><br>观察到栈地址0xFFB21C5C保存的正式mem_test()的返回地址，将其覆盖为win_func()的地址，即可跳转执行win_func()<br>函数调用时，先将其返回地址入栈，再将参数入栈。0xFFB21C5C后面4字节为返回地址（可构造为任意可访问的地址）,再后面4字节为参数地址。</p></li><li><p>查看string视图，找到”cat flag”字符串，其地址为0x08048840。查看function视图，找到win_func()函数地址为0x08048610<br><img src="/image/网络攻防实验2-5.png" alt="字符串地址"><br><img src="/image/网络攻防实验2-6.png" alt="函数地址"></p></li><li><p>构造payload为23字节字符+win_func地址+任意可访问地址+”cat flag”地址。使用pwntools编写exploit。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">from</span> pwn <span class="keyword">import</span> *</div><div class="line">conn=remote(<span class="string">'localhost'</span>,<span class="number">5555</span>)</div><div class="line">conn.send(<span class="string">'a'</span>*<span class="number">23</span>+p32(<span class="number">0x08048610</span>)+p32(<span class="number">0x08048610</span>)+p32(<span class="number">0x08048840</span>))</div><div class="line">conn.interactive()</div></pre></td></tr></table></figure></li></ol><h2 id="实验结果"><a href="#实验结果" class="headerlink" title="实验结果"></a>实验结果</h2><p><img src="/image/网络攻防实验2-7.png" alt="实验结果"></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h2 id=&quot;题目说明&quot;&gt;&lt;a href=&quot;#题目说明&quot; class=&quot;headerlink&quot; title=&quot;题目说明&quot;&gt;&lt;/a&gt;题目说明&lt;/h2&gt;&lt;p&gt;说明：该题目由memory、flag两个文件组成。memory为可执行代码，存在缓冲区溢出漏洞，请同学们分析该程序的二进制代
      
    
    </summary>
    
    
      <category term="缓冲区溢出" scheme="http://libowei.net/tags/%E7%BC%93%E5%86%B2%E5%8C%BA%E6%BA%A2%E5%87%BA/"/>
    
      <category term="网络攻防实验" scheme="http://libowei.net/tags/%E7%BD%91%E7%BB%9C%E6%94%BB%E9%98%B2%E5%AE%9E%E9%AA%8C/"/>
    
  </entry>
  
  <entry>
    <title>网络攻防实验(1)-缓冲区溢出</title>
    <link href="http://libowei.net/%E7%BD%91%E7%BB%9C%E6%94%BB%E9%98%B2%E5%AE%9E%E9%AA%8C-1-%E7%BC%93%E5%86%B2%E5%8C%BA%E6%BA%A2%E5%87%BA.html"/>
    <id>http://libowei.net/网络攻防实验-1-缓冲区溢出.html</id>
    <published>2017-03-31T04:14:34.000Z</published>
    <updated>2017-11-10T04:13:49.760Z</updated>
    
    <content type="html"><![CDATA[<h2 id="题目说明"><a href="#题目说明" class="headerlink" title="题目说明"></a>题目说明</h2><p>说明：该题目由100、flag两个文件组成。100为可执行代码，存在缓冲区溢出漏洞。请同学们分析该程序的二进制代码找到漏洞成因，并通过修改程序的执行流程导致任意代码执行漏洞将flag中的FLAG信息输出出来。<br>单机执行：请同学们在自己的机器上运行脚本：./host，9999端口即为程序输入端口。如果编写exploit攻击成功，会出现以下提示：<br>{FLAG:this is a flag}</p><h2 id="题目下载"><a href="#题目下载" class="headerlink" title="题目下载"></a>题目下载</h2><p><a href="/file/网络攻防实验1.rar">实验程序下载</a></p><h2 id="使用工具"><a href="#使用工具" class="headerlink" title="使用工具"></a>使用工具</h2><ul><li>IDA pro</li><li>pwntools (python2.7)</li><li>socat</li></ul><h2 id="解题步骤"><a href="#解题步骤" class="headerlink" title="解题步骤"></a>解题步骤</h2><ol><li><p>使用IDA pro打开程序，按F5可查看其反汇编代码。<br><img src="/image/网络攻防实验1-1.png" alt="IDA pro F5"><br>注意到scanf函数可能存在缓冲区溢出漏洞。推断此题的解题思路为：通过scanf函数输入一定长度的字符，覆盖栈中的v5()函数的地址。</p></li><li><p>寻找要执行函数的地址。打开函数视图(view-&gt;subview-&gt;function)，找到win()函数：<br><img src="/image/网络攻防实验1-2.png" alt="win函数"><br>执行这个函数可以读出flag文件中内容。</p></li><li><p>寻找溢出点。在scanf函数后添加断点，调试程序。在程序中输入若干个’0’后，栈信息如下：<br><img src="/image/网络攻防实验1-3.png" alt="栈"><br>输入的’a’的ASCII码为0x30，栈中lose()函数的地址为<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div></pre></td><td class="code"><pre><div class="line"></div><div class="line">4. 构造payload为 64字节字符+win()函数地址。本题中，执行host.sh脚本可将程序输入/输出重定向到9999端口,可使用pwntools编写exploit。</div><div class="line">```python</div><div class="line">from pwn import *</div><div class="line">conn=remote(&apos;localhost&apos;,9999)</div><div class="line">conn.send(&apos;a&apos;*64+p32(0x0804861B))</div><div class="line">conn.interactive()</div></pre></td></tr></table></figure></p></li></ol><h2 id="实验结果"><a href="#实验结果" class="headerlink" title="实验结果"></a>实验结果</h2><p><img src="/image/网络攻防实验1-4.png" alt="实验结果"></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h2 id=&quot;题目说明&quot;&gt;&lt;a href=&quot;#题目说明&quot; class=&quot;headerlink&quot; title=&quot;题目说明&quot;&gt;&lt;/a&gt;题目说明&lt;/h2&gt;&lt;p&gt;说明：该题目由100、flag两个文件组成。100为可执行代码，存在缓冲区溢出漏洞。请同学们分析该程序的二进制代码找到漏洞成
      
    
    </summary>
    
    
      <category term="缓冲区溢出" scheme="http://libowei.net/tags/%E7%BC%93%E5%86%B2%E5%8C%BA%E6%BA%A2%E5%87%BA/"/>
    
      <category term="网络攻防实验" scheme="http://libowei.net/tags/%E7%BD%91%E7%BB%9C%E6%94%BB%E9%98%B2%E5%AE%9E%E9%AA%8C/"/>
    
  </entry>
  
  <entry>
    <title>依法治国的发展及对当代大学生法治素养的探讨</title>
    <link href="http://libowei.net/%E4%BE%9D%E6%B3%95%E6%B2%BB%E5%9B%BD%E7%9A%84%E5%8F%91%E5%B1%95%E5%8F%8A%E5%AF%B9%E5%BD%93%E4%BB%A3%E5%A4%A7%E5%AD%A6%E7%94%9F%E6%B3%95%E6%B2%BB%E7%B4%A0%E5%85%BB%E7%9A%84%E6%8E%A2%E8%AE%A8.html"/>
    <id>http://libowei.net/依法治国的发展及对当代大学生法治素养的探讨.html</id>
    <published>2016-11-11T14:41:16.000Z</published>
    <updated>2017-11-10T04:14:29.221Z</updated>
    
    <content type="html"><![CDATA[<h2 id="摘要"><a href="#摘要" class="headerlink" title="摘要"></a>摘要</h2><p>　　本文阐述了依法治国的发展历程：从法治建设“十六字方针”到提出依法治国的基本方略；从依法治国被写入宪法，到提出全面推进依法治国。社会主义依法治国理论日益走向成熟，社会主义依法治国实践迈入更高阶段。最后，在总结依法治国的基础上，探讨了依法治国大背景下如何提高当代大学生的法制素养。</p><h2 id="依法治国的发展历程"><a href="#依法治国的发展历程" class="headerlink" title="依法治国的发展历程"></a>依法治国的发展历程</h2><p>　　1954年制定了共和国第一部宪法，初步奠定了社会主义法制的基础。但在“文革”十年，社会主义法制遭到严重破坏。在党的十一届三中全会召开前的中央工作会议上，邓小平提出了“为了保障人民民主，必须加强法制，使民主制度化、法律化，做到有法可依，有法必依，执法必严，违法必究。”这段谈话，为我国依法治国基本方略的形成奠定了基本理论基础。</p><p>　　党的十一届三中全会确立了解放思想、实事求是的思想路线，同时提出了加强社会主义民主，健全社会主义法制的任务目标。会议公报<sup>[1]</sup>指出：“为了保障人民民主，必须加强社会主义法制，使民主制度化、法律化，使这种制度和法律具有稳定性、连续性和极大的权威，做到<strong>有法可依，有法必依，执法必严，违法必究</strong>。”这准确地描述了法治的基本精神内核，阐述了依法治国的基本内涵，为依法治国方略的最终提出奠定了思想基础。</p><p>　　在党的十一届三中全会精神指引下，在党的领导下我国进行了一系列重大立法工作。党的十一届三中全会以后，按照**“建设有中国特色的社会主义”和“以经济建设为中心”的方针和指导思想，立法进程被不断推进，先后制定了一系列重要的民事、经济法律，为改革开放和社会主义现代化建设提供了坚实的法律保障。</p><p>　　1989年，颁布行政诉讼法，是我国法治政府建设的重要开端。1993年，党的十四届三中全会通过的《决定》<sup>[2]</sup>提出：<strong>“各级政府都要依法行政，依法办事。”</strong>这是第一次在党的正式文件中提出“依法行政”，将法治政府建设作为法治建设的重点，进一步丰富了依法治国的内涵。此时，依法治国方略虽然尚未提出，但“十六字方针”和宪法及一系列重要法律的修订出台，清晰阐释了依法治国的基本精神，社会主义法制体系开始形成，这为依法治国方略的形成奠定了思想基础和制度基础。</p><p>　　党的十五大正式提出依法治国基本方略。十五大报告<sup>[3]</sup>中指出：“<strong>依法治国，是党领导人民治理国家的基本方略</strong>，是发展社会主义市场经济的客观需要，是社会文明进步的重要标志，是国家长治久安的重要保障。”这就正式将依法治国提升为国家治理的基本方略。依法治国方略的提出，是对我们党治国理政经验的全面总结与升华，标志着党在执政理念、领导方式上实现了一次历史性跨越，为我国此后的国家治理和社会治理指明了方向，具有里程碑意义。</p><p>　　1999年3月通过的宪法修正案<sup>[4]</sup>规定：<strong>“中华人民共和国实行依法治国，建设社会主义法治国家”</strong>。这正式将依法治国确立为宪法的基本原则，通过国家根本法对依法治国予以保障，使其有了宪法保障，也使“依法治国”这一基本方略有了长期性、稳定性的制度基础。</p><p>　　在党的十五大提出依法治国基本方略的基础上，党的十六大提出了坚持依法执政、不断提高执政能力的思想，要求不断改革和完善党的领导方式和执政方式，将民主、法治、人权建设从以往的“精神文明”范畴中独立出来，正式提出“政治文明”的概念，这就进一步丰富了依法治国的内涵，明晰了依法治国与其他治理方式的关系。党的十六大还提出“三统一”的法治原则，即“发展社会主义民主政治，最根本的是要<strong>把坚持党的领导、人民当家作主和依法治国有机统一起来</strong>”，这就确立了中国特色社会主义依法治国方略的根本原则。</p><p>　　十八大确立了依法治国的新任务和目标，即到2020年全面建成小康社会时，实现“依法治国基本方略全面落实，法治政府基本建成，司法公信力不断提高，人权得到切实尊重和保障。”这个战略目标与全面建成小康社会的目标同时提出，进一步凸显了依法治国的重要性。2014年10月，十八届四中全会的主题为“依法治国”，总结了依法治国的经验，研究了全面推进依法治国若干重大问题，对依法治国进行总体部署和全面规划。</p><p>　　在我们这样一个13亿多人口的发展中大国全面推进依法治国，是国家治理领域一场广泛而深刻的革命。全面推进依法治国，总目标是建设中国特色社会主义法治体系，建设社会主义法治国家。这就是，在中国共产党领导下，坚持中国特色社会主义制度，贯彻中国特色社会主义法治理论，形成完备的法律规范体系、高效的法治实施体系、严密的法治监督体系、有力的法治保障体系，形成完善的党内法规体系，坚持依法治国、依法执政、依法行政共同推进，坚持法治国家、法治政府、法治社会一体建设，实现科学立法、严格执法、公正司法、全民守法，促进国家治理体系和治理能力现代化。</p><h2 id="对当代青年大学生的法治素养的探讨"><a href="#对当代青年大学生的法治素养的探讨" class="headerlink" title="对当代青年大学生的法治素养的探讨"></a>对当代青年大学生的法治素养的探讨</h2><p>　　党的十八届四中全会对全面推进依法治国战略做出了总部署，在《决定》<sup>[5]</sup>中提出了“把法治教育纳入国民教育体系，从青少年抓起”的要求和目标。高等学校是法治教育的重要场所，大学生是高素质青年群体的重要组成部分，是学法、守法、护法、用法的生力军，提升大学生的法制素养是实现全面依法治国方略的一个十分重要的部分。</p><p>　　大学生的综合素质发展不但要求有较高的文化素养、扎实的专业基础知识和较强的工作能力，还要有自觉的规则意识和法律观念。 树立宪法至上的观念，把遵守法律规范作为自己行为的首要标准，这是新时代对当代大学生素质结构的新要求，因此，提高大学生法治素养就成为了高校立德树人的重要内容。</p><p>　　全面建设小康社会所需要的不仅仅是高水平的科技人才，更需要思想政治素质高，具备一定法治素养的创新型复合型建设人才。只有加强大学生法治教育，提升法治素养， 才能够让他们意识到全面实施依法治国的重要性， 并成为一个懂法的社会主义建设者和接班人，才能够自觉拿起法律武器维护自身、他人以及国家的权益，才能在经济建设中、在法律框架内充分发挥自身优势，为全面建设小康社会作出贡献.</p><p>　　对于广大高校学生，只有他们的法治素养提升了，才能保证法治校园的建立，同样，也只有依法治校落到了实处，才能保证大学生法治教育内容与法治教育环境高度一致，从而有效提升大学生法治素养。</p><p>　　对于学校而言，只有各高校注重大学生法治素养的提升，形成大学生参与学校依法管理的良好氛围，使大学生能够积极参与制定、宣传、践行学校的各项规章制度，同时通过法治学习，做到知法、守法，懂得用法律武器维护自我权益，才能更好地保障高校教育的依法顺利实施。</p><p>　　通过建设校园法治文化可以潜移默化地提升大学生法治素养，让遵法守法意识在学校蔚然成风，让法治文化在师生心中落地生根。要充分利用校园新闻、校园报纸、学生社团以及新兴媒体等载体加大法治文化宣传，以学生喜闻乐见的形式宣传法治文化。还要将法治教育融入大学生教育中，融入学生社会实践中，在学生自我教育、自我管理、自我服务中体现和培养法治精神、法治思维等法治素养，达到内化于心、外化于行的效果。推进法治文化主题活动。搭建形式多样的法治文化主题活动，打造法治文化宣传教育活动品牌，让学生在活动中体验法治精神。 可以通过开展“国家宪法日”主题活动，让学生牢固树立公民权利与义务相统一的观念。通过开展“校园诚信教育”活动，使遵纪守法成为大学生的共同追求和自觉行动。</p><p>　　要探索法治文化长效机制。高校要注重将法治文化培育纳入学生成长成才的全过程，以制度形式规范法治文化建设的领导体制、运行机制、活动载体、队伍建设、考核管理、经费投入和硬件保障。要站在“四个全面”战略布局的高度，将通过法治文化熏陶提升大学生法治素养作为一项常抓不懈的工作，让法治精神在一代代大学生群体中传承和发扬。</p><h2 id="参考文献"><a href="#参考文献" class="headerlink" title="参考文献"></a>参考文献</h2><p>　　[1] 中国共产党第十一届中央委员会第三次全体会议公报[M]. 人民出版社, 1979.<br>　　[2] 中共中央关于建立社会主义市场经济体制若干问题的决定[J]. 求实, 1993(12):1-13.<br>　　[3] 江泽民. 高举邓小平理论伟大旗帜把建设有中国特色社会主义事业全面推向二十一世纪[M]. 人民出版社, 1997.<br>　　[4] 中华人民共和国宪法修正案[C] 1999.<br>　　[5] 中新网. 中共中央 关于全面推进依法治国若干重大问题的决定[M]. 中国法制出版社, 2014.<br>　　[6] 李艳. 大学生法治教育[J]. 教育, 2015(35):210-210.<br>　　[7] 李全文. 全面依法治国视域中的大学生法治教育[J]. 思想理论教育导刊, 2016(5).</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h2 id=&quot;摘要&quot;&gt;&lt;a href=&quot;#摘要&quot; class=&quot;headerlink&quot; title=&quot;摘要&quot;&gt;&lt;/a&gt;摘要&lt;/h2&gt;&lt;p&gt;　　本文阐述了依法治国的发展历程：从法治建设“十六字方针”到提出依法治国的基本方略；从依法治国被写入宪法，到提出全面推进依法治国。社会主义依
      
    
    </summary>
    
    
      <category term="中国特色社会主义" scheme="http://libowei.net/tags/%E4%B8%AD%E5%9B%BD%E7%89%B9%E8%89%B2%E7%A4%BE%E4%BC%9A%E4%B8%BB%E4%B9%89/"/>
    
  </entry>
  
  <entry>
    <title>网页设计作品</title>
    <link href="http://libowei.net/%E7%BD%91%E9%A1%B5%E8%AE%BE%E8%AE%A1%E4%BD%9C%E5%93%81.html"/>
    <id>http://libowei.net/网页设计作品.html</id>
    <published>2016-11-05T04:48:30.000Z</published>
    <updated>2017-11-23T04:14:28.355Z</updated>
    
    <content type="html"><![CDATA[<p>女朋友的部分网页设计作品</p><h3 id="教育年度盘点"><a href="#教育年度盘点" class="headerlink" title="教育年度盘点"></a>教育年度盘点</h3><p>网页链接：<a href="http://#" target="_blank" rel="external">尚未上线</a><br><img src="/image/xf-%E6%95%99%E8%82%B2%E5%B9%B4%E5%BA%A6%E7%9B%98%E7%82%B9.jpg" alt="教育年度盘点"></p><h3 id="数读中学专题-成都七中"><a href="#数读中学专题-成都七中" class="headerlink" title="数读中学专题-成都七中"></a>数读中学专题-成都七中</h3><p>网页链接：<a href="http://www.eol.cn/html/zhongxue/sdzx-7z/" target="_blank" rel="external">数读中学系列专题—成都七中｜中国教育在线</a><br><img src="/image/xf_%E6%95%B0%E8%AF%B4%E4%B8%AD%E5%AD%A6%E6%94%B9.jpg" alt="成都七中"></p><h3 id="新高三生规划"><a href="#新高三生规划" class="headerlink" title="新高三生规划"></a>新高三生规划</h3><p>网页链接：<a href="http://www.eol.cn/html/g/tjgk/32.shtml" target="_blank" rel="external">2017年高三生学习规划|高三规划|中国教育在线</a><br><img src="/image/xf_%E5%9B%BE%E8%A7%A3%E9%AB%98%E8%80%83.jpg" alt="高三规划"></p><h3 id="2016国庆专题"><a href="#2016国庆专题" class="headerlink" title="2016国庆专题"></a>2016国庆专题</h3><p>网页链接：<a href="http://#" target="_blank" rel="external">已下线</a><br><img src="/image/xf_%E5%9B%BD%E5%BA%86%E4%B8%93%E9%A2%98%E6%94%B9.jpg" alt="国庆专题"></p><h3 id="高三你准备好了吗？"><a href="#高三你准备好了吗？" class="headerlink" title="高三你准备好了吗？"></a>高三你准备好了吗？</h3><p>网页链接：<a href="http://www.eol.cn/html/g/gsnzbhlm/index.shtml" target="_blank" rel="external">高三,你准备好了吗_中国教育在线</a><br><img src="/image/%E9%AB%98%E4%B8%89%E4%BD%A0%E5%87%86%E5%A4%87%E5%A5%BD%E4%BA%86%E5%90%97.jpg" alt="高三准备好了吗"></p><h3 id="图解小升初"><a href="#图解小升初" class="headerlink" title="图解小升初"></a>图解小升初</h3><p>网页链接：<a href="http://www.eol.cn/html/jijiao/xiao/2015tj/23/index.shtml" target="_blank" rel="external">图解小升初：小升初过后，初中英语怎么学？</a><br><img src="/image/xf_%E5%9B%BE%E8%A7%A3%E5%B0%8F%E5%8D%87%E5%88%9D23.png" alt="图解小升初"></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;女朋友的部分网页设计作品&lt;/p&gt;
&lt;h3 id=&quot;教育年度盘点&quot;&gt;&lt;a href=&quot;#教育年度盘点&quot; class=&quot;headerlink&quot; title=&quot;教育年度盘点&quot;&gt;&lt;/a&gt;教育年度盘点&lt;/h3&gt;&lt;p&gt;网页链接：&lt;a href=&quot;http://#&quot; target=&quot;_b
      
    
    </summary>
    
    
      <category term="ui" scheme="http://libowei.net/tags/ui/"/>
    
  </entry>
  
  <entry>
    <title>棋盘覆盖问题</title>
    <link href="http://libowei.net/%E6%A3%8B%E7%9B%98%E8%A6%86%E7%9B%96%E9%97%AE%E9%A2%98.html"/>
    <id>http://libowei.net/棋盘覆盖问题.html</id>
    <published>2016-10-27T14:18:45.000Z</published>
    <updated>2017-11-23T04:14:02.617Z</updated>
    
    <content type="html"><![CDATA[<h2 id="问题描述"><a href="#问题描述" class="headerlink" title="问题描述"></a>问题描述</h2><p>在一个<code>2^k*2^k</code>个方格组成的棋盘中，恰有一个方格与其他方格不同，称该方格为一特殊方格，如图所示，红色方格为特殊方格。</p><p><img src="http://images.cnitblog.com/blog/328951/201306/14220958-a1cb3c8a13174c96901a174c1c04cd99.x-png" alt="棋盘图"></p><p>棋盘覆盖问题是指，要用下图中的四种不同形态的L型骨牌覆盖棋盘上除特殊方格以外的所有方格，且任何2个L型骨牌不能重叠覆盖。</p><p><img src="http://images.cnitblog.com/blog/328951/201306/14221040-4f79c8f9b1c1467e9f1f272737ba9ad7.x-png" alt="L型骨牌"></p><h2 id="解题思路"><a href="#解题思路" class="headerlink" title="解题思路"></a>解题思路</h2><p>采用分治法来求解此问题</p><ol><li>若k=2，则直接用一个L型骨牌来覆盖非特殊方格</li><li>k&gt;3时，可把原棋盘分成四个2^(k-1)阶的子棋盘，特殊方格位于四个棋盘中的一个。其他三个子棋盘也可转化为特殊棋盘，做法是用一个L型骨牌覆盖三个子棋盘的连接处的三个方格，如图所示。此时原问题转化为4个小规模棋盘的覆盖问题，可递归求解。</li></ol><p><img src="http://images.cnitblog.com/blog/328951/201306/14221452-5dde1a9cdb6d41dd8c73f0e23ce0abb1.x-png" alt="四个子棋盘"></p><h2 id="算法"><a href="#算法" class="headerlink" title="算法"></a>算法</h2><ol><li>设置一个全局变量mark，用于记录骨牌的序号</li><li>如果边长为2，则将给定的特殊方格(x,y)用给定的序号标记，其他三个方格用mark标记。</li><li><p>如果边长&gt;2，则把棋盘分割为四部分，判断特殊方格所处的子棋盘。对于有特殊方格的棋盘，继续递归求解。对于其他3个子棋盘：</p><ul><li>左上角的子棋盘：其右下角方格为“特殊方格”</li><li>右上角的子棋盘：其右下角方格为“特殊方格”</li><li>左下角的子棋盘：其右上角方格为“特殊方格”</li><li>右下角的子棋盘：其左上角方格为“特殊方格”</li></ul></li></ol><p>这三个子棋盘的假想的“特殊方格”用同一个骨牌覆盖（标记相同），继续递归处理。</p><h2 id="Python语言实现"><a href="#Python语言实现" class="headerlink" title="Python语言实现"></a>Python语言实现</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div><div class="line">42</div><div class="line">43</div><div class="line">44</div><div class="line">45</div><div class="line">46</div><div class="line">47</div><div class="line">48</div><div class="line">49</div><div class="line">50</div><div class="line">51</div><div class="line">52</div><div class="line">53</div><div class="line">54</div><div class="line">55</div><div class="line">56</div><div class="line">57</div><div class="line">58</div><div class="line">59</div><div class="line">60</div><div class="line">61</div><div class="line">62</div><div class="line">63</div><div class="line">64</div><div class="line">65</div><div class="line">66</div><div class="line">67</div><div class="line">68</div><div class="line">69</div><div class="line">70</div><div class="line">71</div><div class="line">72</div><div class="line">73</div><div class="line">74</div><div class="line">75</div><div class="line">76</div><div class="line">77</div><div class="line">78</div><div class="line">79</div><div class="line">80</div><div class="line">81</div></pre></td><td class="code"><pre><div class="line"><span class="comment"># coding=utf-8</span></div><div class="line">mark = <span class="number">1</span></div><div class="line">list = []</div><div class="line"><span class="comment"># 生成棋盘</span></div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">initTable</span><span class="params">(n)</span>:</span></div><div class="line">    <span class="keyword">global</span> list;</div><div class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(n):</div><div class="line">        row = []</div><div class="line">        <span class="keyword">for</span> j <span class="keyword">in</span> range(n):</div><div class="line">            row.append(<span class="number">-1</span>)</div><div class="line">        list.append(row)</div><div class="line"></div><div class="line"><span class="comment"># 检查特殊点区域</span></div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">check</span><span class="params">(midx, midy, x, y)</span>:</span></div><div class="line">    <span class="keyword">if</span> x &lt; midx <span class="keyword">and</span> y &lt; midy:</div><div class="line">        <span class="keyword">return</span> <span class="number">0</span>  <span class="comment"># 左上</span></div><div class="line">    <span class="keyword">elif</span> x &gt;= midx <span class="keyword">and</span> y &gt;= midy:</div><div class="line">        <span class="keyword">return</span> <span class="number">3</span>  <span class="comment"># 右下</span></div><div class="line">    <span class="keyword">elif</span> x &gt;= midx <span class="keyword">and</span> y &lt; midy:</div><div class="line">        <span class="keyword">return</span> <span class="number">2</span>  <span class="comment"># 右上</span></div><div class="line">    <span class="keyword">else</span>:</div><div class="line">        <span class="keyword">return</span> <span class="number">1</span>  <span class="comment"># 左下</span></div><div class="line"></div><div class="line"><span class="comment"># 覆盖x1~x2行，y1~y2列，特殊点为(x,y),用markX填充特殊点</span></div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">cover</span><span class="params">(x1, x2, y1, y2, x, y, markX)</span>:</span></div><div class="line">    <span class="keyword">global</span> mark</div><div class="line">    <span class="comment"># 如果是2*2的格子</span></div><div class="line">    <span class="keyword">if</span> x2 - x1 == <span class="number">1</span>:</div><div class="line">        <span class="comment"># 填充特殊点</span></div><div class="line">        list[x][y] = markX</div><div class="line">        <span class="comment"># 填充其他3个格子</span></div><div class="line">        <span class="keyword">for</span> row <span class="keyword">in</span> range(x1, x2 + <span class="number">1</span>):</div><div class="line">            <span class="keyword">for</span> col <span class="keyword">in</span> range(y1, y2 + <span class="number">1</span>):</div><div class="line">                <span class="keyword">if</span> row != x <span class="keyword">or</span> col != y:</div><div class="line">                    list[row][col] = mark</div><div class="line">        mark += <span class="number">1</span></div><div class="line">        <span class="keyword">return</span></div><div class="line">    <span class="comment"># 其他情况: 2*2以上的格子</span></div><div class="line">    <span class="comment"># 用midx,midy把棋盘分为4部分</span></div><div class="line">    midx = (x1 + x2 + <span class="number">1</span>) / <span class="number">2</span></div><div class="line">    midy = (y1 + y2 + <span class="number">1</span>) / <span class="number">2</span></div><div class="line">    covered = &#123;<span class="number">0</span>:<span class="keyword">False</span>, <span class="number">1</span>:<span class="keyword">False</span>, <span class="number">2</span>:<span class="keyword">False</span>, <span class="number">3</span>:<span class="keyword">False</span>&#125;</div><div class="line">    <span class="comment"># 检查特殊点在哪个区域</span></div><div class="line">    area = check(midx, midy, x, y)</div><div class="line">    <span class="keyword">if</span> area == <span class="number">0</span>:  <span class="comment"># 特殊格子在左上</span></div><div class="line">        cover(x1, midx - <span class="number">1</span>, y1, midy - <span class="number">1</span>, x, y, markX)</div><div class="line">    <span class="keyword">if</span> area == <span class="number">1</span>:  <span class="comment"># 特殊格子在右上</span></div><div class="line">        cover(x1, midx - <span class="number">1</span>, midy, y2, x, y, markX)</div><div class="line">    <span class="keyword">if</span> area == <span class="number">2</span>:  <span class="comment"># 特殊格子在左下</span></div><div class="line">        cover(midx, x2, y1, midy - <span class="number">1</span>, x, y, markX)</div><div class="line">    <span class="keyword">if</span> area == <span class="number">3</span>:  <span class="comment"># 特殊格子在右下</span></div><div class="line">        cover(midx, x2, midy, y2, x, y, markX)</div><div class="line">        </div><div class="line">    <span class="comment"># 标记已经处理过的区域</span></div><div class="line">    covered[area] = <span class="keyword">True</span> </div><div class="line">    <span class="comment"># 其他3个区域 取内角的三个小格子为特殊点</span></div><div class="line">    newMark = mark</div><div class="line">    mark += <span class="number">1</span></div><div class="line">    <span class="comment"># 处理其他3个区域</span></div><div class="line">    <span class="keyword">if</span> <span class="keyword">not</span> covered[<span class="number">0</span>]: </div><div class="line">        cover(x1, midx - <span class="number">1</span>, y1, midy - <span class="number">1</span>, midx - <span class="number">1</span>, midy - <span class="number">1</span>, newMark)</div><div class="line">    <span class="keyword">if</span> <span class="keyword">not</span> covered[<span class="number">1</span>]:</div><div class="line">        cover(x1, midx - <span class="number">1</span>, midy, y2, midx - <span class="number">1</span>, midy, newMark)</div><div class="line">    <span class="keyword">if</span> <span class="keyword">not</span> covered[<span class="number">2</span>]:</div><div class="line">        cover(midx, x2, y1, midy - <span class="number">1</span>, midx, midy - <span class="number">1</span>, newMark)</div><div class="line">    <span class="keyword">if</span> <span class="keyword">not</span> covered[<span class="number">3</span>]:</div><div class="line">        cover(midx, x2, midy, y2, midx, midy, newMark)</div><div class="line">        </div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">printTable</span><span class="params">()</span>:</span></div><div class="line">    <span class="keyword">global</span> list</div><div class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(len(list)):</div><div class="line">        row = <span class="string">""</span></div><div class="line">        <span class="keyword">for</span> j <span class="keyword">in</span> range(len(list)):</div><div class="line">            row += (str(list[i][j]) + <span class="string">"\t"</span>)</div><div class="line">        print(row+<span class="string">"\n"</span>)</div><div class="line"></div><div class="line"><span class="keyword">if</span> __name__ == <span class="string">'__main__'</span>:</div><div class="line">    n = <span class="number">8</span></div><div class="line">    initTable(n)</div><div class="line">    cover(<span class="number">0</span>, <span class="number">7</span>, <span class="number">0</span>, <span class="number">7</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">0</span>)</div><div class="line">    printTable()</div></pre></td></tr></table></figure>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h2 id=&quot;问题描述&quot;&gt;&lt;a href=&quot;#问题描述&quot; class=&quot;headerlink&quot; title=&quot;问题描述&quot;&gt;&lt;/a&gt;问题描述&lt;/h2&gt;&lt;p&gt;在一个&lt;code&gt;2^k*2^k&lt;/code&gt;个方格组成的棋盘中，恰有一个方格与其他方格不同，称该方格为一特殊方格，如图所示
      
    
    </summary>
    
    
      <category term="python" scheme="http://libowei.net/tags/python/"/>
    
      <category term="算法" scheme="http://libowei.net/tags/%E7%AE%97%E6%B3%95/"/>
    
  </entry>
  
  <entry>
    <title>Django环境搭建及简单的视图与网址的配置</title>
    <link href="http://libowei.net/Django%E7%8E%AF%E5%A2%83%E6%90%AD%E5%BB%BA%E5%8F%8A%E7%AE%80%E5%8D%95%E7%9A%84%E8%A7%86%E5%9B%BE%E4%B8%8E%E7%BD%91%E5%9D%80%E7%9A%84%E9%85%8D%E7%BD%AE.html"/>
    <id>http://libowei.net/Django环境搭建及简单的视图与网址的配置.html</id>
    <published>2016-10-12T17:18:26.000Z</published>
    <updated>2017-11-10T04:12:20.993Z</updated>
    
    <content type="html"><![CDATA[<h2 id="安装Django"><a href="#安装Django" class="headerlink" title="安装Django"></a>安装Django</h2><p>用pip安装Django：<br><code>pip install Django</code></p><p>检查是否安装成功</p><p>python命令行下<br><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">import</span> django</div><div class="line">django.VERSION</div><div class="line"><span class="comment">#django.get_version()</span></div></pre></td></tr></table></figure></p><p>若能显示出版本信息，则安装成功</p><h2 id="Django基本命令"><a href="#Django基本命令" class="headerlink" title="Django基本命令"></a>Django基本命令</h2><p>创建一个Django project<br><code>django-admin.py startproject project-name</code></p><p>创建app<br><code>python manage.py startapp app-name</code></p><p>启动测试服务器</p><ol><li><code>python manage.py runserver</code></li><li>使用其他端口<code>python manage.py runserver port</code></li><li>监听所有可用ip(内外网情况下)<code>python manage.py runserver 0.0.0.0:port</code></li></ol><h2 id="视图和网址"><a href="#视图和网址" class="headerlink" title="视图和网址"></a>视图和网址</h2><p>新建项目 “mysite”<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">django-admin.py startproject mysite</div></pre></td></tr></table></figure></p><p>创建成功后，目录样式为:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div></pre></td><td class="code"><pre><div class="line">mysite</div><div class="line">├── manage.py</div><div class="line">└── mysite</div><div class="line">    ├── __init__.py</div><div class="line">    ├── settings.py</div><div class="line">    ├── urls.py</div><div class="line">    └── wsgi.py</div></pre></td></tr></table></figure><p>在外层mysite目录下，创建app “test”<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">python manage.py startapp test</div></pre></td></tr></table></figure></p><p>test下目录结构为:<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div></pre></td><td class="code"><pre><div class="line">test/</div><div class="line">├── __init__.py</div><div class="line">├── admin.py</div><div class="line">├── models.py</div><div class="line">├── tests.py</div><div class="line">└── views.py</div></pre></td></tr></table></figure></p><p>把新建的app “test” 加入到INSTALLED_APPS中<br><code>vim mysite/mysite/settings.py</code><br>修改INSTALLED_APPS<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div></pre></td><td class="code"><pre><div class="line">INSTALLED_APPS = (</div><div class="line">    &apos;django.contrib.admin&apos;,</div><div class="line">    &apos;django.contrib.auth&apos;,</div><div class="line">    &apos;django.contrib.contenttypes&apos;,</div><div class="line">    &apos;django.contrib.sessions&apos;,</div><div class="line">    &apos;django.contrib.messages&apos;,</div><div class="line">    &apos;django.contrib.staticfiles&apos;,</div><div class="line">    &apos;test&apos;,</div><div class="line">)</div></pre></td></tr></table></figure></p><p>定义视图函数<br>修改<code>mysite/test/views.py</code><br><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div></pre></td><td class="code"><pre><div class="line"><span class="comment"># coding:utf-8</span></div><div class="line"><span class="keyword">from</span> django.http <span class="keyword">import</span> HttpResponse</div><div class="line"> </div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">index</span><span class="params">(request)</span>:</span></div><div class="line">    <span class="keyword">return</span> HttpResponse(<span class="string">u"李博伟正在学习Django"</span>)</div></pre></td></tr></table></figure></p><p>定义视图函数的URL<br>修改<code>mysite/mysite/urls.py</code><br><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">from</span> django.conf.urls <span class="keyword">import</span> url</div><div class="line"><span class="keyword">from</span> django.contrib <span class="keyword">import</span> admin</div><div class="line"><span class="keyword">from</span> learn <span class="keyword">import</span> views <span class="keyword">as</span> test_views  <span class="comment"># 修改这里</span></div><div class="line"> </div><div class="line">urlpatterns = [</div><div class="line">    url(<span class="string">r'^$'</span>, test_views.index),  <span class="comment"># 修改这里</span></div><div class="line">    url(<span class="string">r'^admin/'</span>, admin.site.urls),</div><div class="line">]</div></pre></td></tr></table></figure></p><p>在<code>url()</code>中用正则表达式定义了网址的形式，和该网址对应的视图函数</p><p>运行<code>python manage.py runserver</code>查看效果</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h2 id=&quot;安装Django&quot;&gt;&lt;a href=&quot;#安装Django&quot; class=&quot;headerlink&quot; title=&quot;安装Django&quot;&gt;&lt;/a&gt;安装Django&lt;/h2&gt;&lt;p&gt;用pip安装Django：&lt;br&gt;&lt;code&gt;pip install Django&lt;/cod
      
    
    </summary>
    
    
      <category term="python" scheme="http://libowei.net/tags/python/"/>
    
      <category term="django" scheme="http://libowei.net/tags/django/"/>
    
      <category term="服务器" scheme="http://libowei.net/tags/%E6%9C%8D%E5%8A%A1%E5%99%A8/"/>
    
  </entry>
  
  <entry>
    <title>搭建shadowsocks服务器实现ipv6免费上网</title>
    <link href="http://libowei.net/%E6%90%AD%E5%BB%BAshadowsocks%E6%9C%8D%E5%8A%A1%E5%99%A8%E5%AE%9E%E7%8E%B0ipv6%E5%85%8D%E8%B4%B9%E4%B8%8A%E7%BD%91.html"/>
    <id>http://libowei.net/搭建shadowsocks服务器实现ipv6免费上网.html</id>
    <published>2016-10-02T17:08:24.000Z</published>
    <updated>2017-11-10T04:13:24.943Z</updated>
    
    <content type="html"><![CDATA[<h2 id="背景"><a href="#背景" class="headerlink" title="背景"></a>背景</h2><p>很多高校提供一定量的免费流量，超过之后就要付费。但是ipv6的流量不需要付费。可以通过shadowsocks搭建一个走ipv6流量的代理，<br>从而实现免费上网。</p><h2 id="所需要的工具"><a href="#所需要的工具" class="headerlink" title="所需要的工具"></a>所需要的工具</h2><p>shadowsocks、支持ipv6的VPS</p><p>支持ipv6的VPS我推荐Conoha，我一直在用，百兆带宽、SSD、不限流量、基本配置900日元/月，可以支付宝付款，总之优点很多。<br>链接如下<br><a href="https://www.conoha.jp/referral/?token=WLoRtpcmNe7GzQc77Zz3qFUgHqVXJh3DstkHzd2P_o1UsydAlTM-Q47" target="_blank" rel="external">Conoha主机</a></p><p>shadowsocks的安装和一些配置可以参考<br><a href="https://github.com/shadowsocks/shadowsocks/wiki/Shadowsocks-%E4%BD%BF%E7%94%A8%E8%AF%B4%E6%98%8E" target="_blank" rel="external">Shadowsocks 使用说明</a><br><a href="http://wuchong.me/blog/2015/02/02/shadowsocks-install-and-optimize/" target="_blank" rel="external">科学上网之 Shadowsocks 安装及优化加速</a></p><h2 id="IPv6-shadowsocks配置"><a href="#IPv6-shadowsocks配置" class="headerlink" title="IPv6 shadowsocks配置"></a>IPv6 shadowsocks配置</h2><h3 id="服务端配置"><a href="#服务端配置" class="headerlink" title="服务端配置"></a>服务端配置</h3><p>若按照上面的链接中的方法安装shadowsocks，其配置文件路径为<code>/etc/shadowsocks.json</code></p><p>修改配置文件<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div></pre></td><td class="code"><pre><div class="line">&#123;</div><div class="line">    &quot;server&quot;:&quot;::&quot;,</div><div class="line">    &quot;server_port&quot;:8989,</div><div class="line">    &quot;local_address&quot;: &quot;127.0.0.1&quot;,</div><div class="line">    &quot;local_port&quot;:1080,</div><div class="line">    &quot;password&quot;:&quot;你的密码&quot;,</div><div class="line">    &quot;timeout&quot;:600,</div><div class="line">    &quot;method&quot;:&quot;aes-256-cfb&quot;</div><div class="line">&#125;</div></pre></td></tr></table></figure></p><p>与ipv4的不同之处为<code>&quot;sever&quot;:&quot;::&quot;</code>，如此修改即可启用ipv6</p><p>启动shadowsocks服务<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">ssserver -c /etc/shadowsocks.json -d start</div></pre></td></tr></table></figure></p><h3 id="客户端配置"><a href="#客户端配置" class="headerlink" title="客户端配置"></a>客户端配置</h3><p>客户端配置见下图，在服务器地址处填写服务器的ipv6地址<br><img src="http://odszv0fof.bkt.clouddn.com/shadowsocks.png" alt="客户端配置"></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h2 id=&quot;背景&quot;&gt;&lt;a href=&quot;#背景&quot; class=&quot;headerlink&quot; title=&quot;背景&quot;&gt;&lt;/a&gt;背景&lt;/h2&gt;&lt;p&gt;很多高校提供一定量的免费流量，超过之后就要付费。但是ipv6的流量不需要付费。可以通过shadowsocks搭建一个走ipv6流量的代理，&lt;
      
    
    </summary>
    
    
      <category term="shadowsocks" scheme="http://libowei.net/tags/shadowsocks/"/>
    
      <category term="ipv6" scheme="http://libowei.net/tags/ipv6/"/>
    
  </entry>
  
</feed>
