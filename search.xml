<?xml version="1.0" encoding="utf-8"?>
<search>
  
    
    <entry>
      <title><![CDATA[TensorFlow CNN 相关API]]></title>
      <url>%2FTensorFlow-CNN-%E7%9B%B8%E5%85%B3API.html</url>
      <content type="text"><![CDATA[tf.nn.conv2d123456789conv2d( input, filter, strides, padding, use_cudnn_on_gpu=None, data_format=None, name=None) 给定4维的输入和4维的过滤器Tensor，进行2卷积运算。 输入Tensor的形状为[batch,height,width,channels]，过滤器张量的形状为[filter_height, filter_width, in_channels, out_channels] 把过滤器转化为2-D，其形状为[filter_height * filter_width * in_channels, output_channels] 在图像中提取形状为[batch, out_height, out_width, filter_height * filter_width * in_channels]的Tensor 图像块向量和过滤器矩阵相乘 步长参数strides中，必须满足strides[0]=strides[3]=1。大多数情况下水平步长和垂直步长相等。 参数： input Tensor。维度的顺序由data-format确定。 filter Tensor。与input数据类型相同。形状是[filter_height, filter_width, in_channels, out_channels] strides 整数列表（长度为4的1-D Tensor）。每个维度滑动窗口的步长，维度的顺序由data-format确定。 padding 字符串：&quot;SAME&quot;或&quot;VALID&quot;。&quot;SAME&quot;是对输入的首尾补0，以满足每个滑动窗口的大小。&quot;VALID&quot;是丢弃末尾的数据。 use_cudnn_on_gpu 布尔值，默认为True。 data_format 可选&quot;NHWC&quot;或&quot;NCHW&quot;，默认为&quot;NHWC&quot; name 为这个操作取一个名字。 tf.nn.bias_add123456bias_add( value, bias, data_format=None, name=None) 这个函数的作用是将偏差项bias加到value上面。 这个操作是tf.add的一个特例。其中bias必须为1维，而value可以为任意维度。与tf.add不同的是，数据被量化的情况下，value和bias的类型可以不同。 参数： value Tensor。 bias 1维的Tensor。大小与value最后一维相同。类型也须与value相同，除非value被量化。 data_format &quot;NHWC&quot;或&quot;NCHW&quot;。 name 为这个操作取一个名字。 tf.nn.relu1234relu( features, name=None) 计算非线性激活函数max(features,0) 参数: features Tensor。 name 为这个操作取个名字 tf.nn.max_pool1234567 value, ksize, strides, padding, data_format=&apos;NHWC&apos;, name=None) 对输入数据进行最大池化 参数： value 4维Tensor，形状为[batch, height, width, channels]，类型为tf.float32 ksize 长度&gt;=4的整形列表。每个维度的窗口大小。 strides 长度&gt;=4的整形列表。每个维度的滑动步长。 padding &quot;SAME&quot;或&quot;VALID&quot;。输出形状为：output_height/output_width = (height/width - pool_size) stride + 1，&quot;SAME&quot;为向上取整，&quot;VALID&quot;为向下取整 tf.nn.lrn12345678lrn( input, depth_radius=None, bias=None, alpha=None, beta=None, name=None) 局部响应归一化 4维的inputTensor可以看做3维的1维向量（沿着最后一维），对每个向量归一化。计算公式为：12sqr_sum[a, b, c, d] = sum(input[a, b, c, d - depth_radius : d + depth_radius + 1] ** 2)output = input / (bias + alpha * sqr_sum) ** beta 参数: input Tensor depth_radius 默认为5 bias 默认为1.0 alpha 默认为1.0 beta 默认为0.5，指数项 name 为这个操作取个名字 tf.nn.sparse_softmax_cross_entropy_with_logits123456sparse_softmax_cross_entropy_with_logits( _sentinel=None, labels=None, logits=None, name=None) 计算logists和labels的稀疏softmax交叉熵。 用于度量互斥的离散分类任务（每个样本只属于一类）的概率误差，如CIFAR-10图像，每个图像只属于一类。 注意：这个操作接受未处理的输入，为了提高效率，函数内部对logits做了softmax运算。所以不要输入softmax后的数值，这会产生不正确的结果。 常见的情况，logits的形状为[batch_size, num_classes]，labels为[batch_size]。更高维度的也支持。 为了避免混淆，传递参数时需要带上参数名（如sparse_softmax_cross_entropy_with_logits(logits=logits,labels=labels) 参数： labels 形状为[d_0, d_1, ..., d_{r-1}]的Tensor（r为label的rank）。labels取值必须为[0, num_classes) logits 对数概率(?)，形状为[d_0, d_1, ..., d_{r-1}, num_classes] name 为操作取一个名字 tf.train.exponential_decay12345678exponential_decay( learning_rate, global_step, decay_steps, decay_rate, staircase=False, name=None) 对学习率应用指数衰减。 训练模型时，随着训练的进行，逐渐减小学习率是好的做法。这个函数对初始的学习率应用指数衰减方法。需要global_step作为参数计算衰减后的学习率。可以传递一个TensorFlow变量，每次训练对其加一。 函数计算方法：1ecayed_learning_rate = learning_rate * decay_rate ^ (global_step / decay_steps) 若staircase为True，global_step / decay_steps计算结果为整数，此时学习率呈阶梯状下降。 参数: learning_rate 标量，初始学习率 global_step 标量（整数），不能为负 decay_steps 标量（整数），必须为正 decay_rate 标量，衰减率 staircase 布尔。 tf.control_dependencies控制计算顺序 tf.train.ExponentialMovingAveragetf.train.ExponentialMovingAverage]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[CentOS安装TensorFlow]]></title>
      <url>%2FCentOS%E5%AE%89%E8%A3%85TensorFlow.html</url>
      <content type="text"><![CDATA[在CentOS6上安装TensorFlow1.2后，import tensorflow时出现以下问题：1ImportError: /lib64/libc.so.6: version `GLIBC_2.17&apos; not found 这个错误的原因是未安装2.17版本的glibc库。 而在CentOS上，使用yum install glibc命令，只能更新到2.12版本。需要手动下载编译安装。 glibc-2.17下载地址：https://ftp.gnu.org/gnu/glibc/glibc-2.17.tar.gz 下载glibc并解压缩12wget https://ftp.gnu.org/gnu/glibc/glibc-2.17.tar.gztar -xvf glibc-2.17.tar.gz 编译安装12345cd glibc-2.17mkdir buildcd build../configure --prefix=/usr --disable-profile --enable-add-ons --with-headers=/usr/include --with-binutils=/usr/binmake &amp;&amp; make install 查看glibc共享库：1ll /lib64/libc.so.6 现libc.so.6已经软链接到2.17版本1lrwxrwxrwx 1 root root 12 7月 21 10:11 /lib64/libc.so.6 -&gt; libc-2.17.so 可以查看系统中可使用的glibc版本1strings /lib64/libc.so.6 |grep GLIBC_ 123456789101112131415161718192021GLIBC_2.2.5GLIBC_2.2.6GLIBC_2.3GLIBC_2.3.2GLIBC_2.3.3GLIBC_2.3.4GLIBC_2.4GLIBC_2.5GLIBC_2.6GLIBC_2.7GLIBC_2.8GLIBC_2.9GLIBC_2.10GLIBC_2.11GLIBC_2.12GLIBC_2.13GLIBC_2.14GLIBC_2.15GLIBC_2.16GLIBC_2.17GLIBC_PRIVATE 现在应该就没问题了。 参考资料：Linux/CentOS 升级C基本运行库CLIBC的注意事项]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[TensorFlow学习笔记：CIFAR-10 CNN]]></title>
      <url>%2FTensorFlow%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%EF%BC%9ACNN-CIFAR-10.html</url>
      <content type="text"><![CDATA[官方文档地址：Convolutional Neural Networks 概述CIFAR-10分类是机器学习中常见的标准问题。CIFAR-10分类目标是把32*32像素的RGB图像分为10类1airplane, automobile, bird, cat, deer, dog, frog, horse, ship, and truck. 目标建立一个小型的CNN用于图像识别，做到： 规范的组织神经网络的结构、训练和评估 为构建更大、更复杂的模型提供模板 选用CIFAR-10数据的原因，一方面它足够复杂，能符合TensorFlow处理大模型的能力；另一方面它数据量较小，训练迅速，可以用来做测试和实验。 重点CIFAR-10教程示范了一些重要的结构，可以用来在TensorFlow种设计大型的复杂的模型 核心计算部分：卷积（convolution）, 修正线性激活（rectified linear activation）, 最大池化（max pooling）, 局部响应归一化（local response normalization） 神经网络训练时行为的可视化，包括图像输入，损失，神经网络行为的分布和梯度等。 计算学到的参数的moving avearage，并在评估中使用以提高预测效果 实现学习率随时间增加而减少 预取队列：将磁盘延迟和高代价的图像预处理与模型分开 提供了多GPU训练的版本，实现了： 在多个GPU卡间并行训练 在多个GPU间共享变量、更新变量值 结构该模型是由卷积层和非线性层组成的多层的结构。这些层之后连接全连接层，最后是Softmax分类器。模型结构大致和Alex Krizhevsky提出的模型一致，前面几层略有不同。 这个模型在单个GPU上训练若干小时后，就能达到非常好的效果：86%正确率。模型由1068298个可学习的参数组成，对一个图像分类需要19.5M次乘加操作。 代码组织代码在models/tutorials/image/cifar10/ cifar10_input.py 读原始的二进制CIFAR-10数据 cifar10.py 建立模型 cifar10_train.py 在CPU或GPU上训练模型 cifar10_multi_gpu_train.py 在多GPU环境中训练模型 cifar10_eval.py 评估模型 CIFAR-10模型神经网络模型代码在cifar10.py中。全部的训练图包括765个操作。建立下面的模块，编写重用性高的图结构代码： 模型输入：inputs()和disorted_inputs()，为训练和评估读入、预处理图像数据 模型预测：inference() 对提供的图片进行分类 模型训练：loss()和train()，计算损失、梯度、更新变量、结果可视化 模型输入输入模块由inputs()和distorted_inputs()构成，两个函数读入CIFAR-10二进制数据，文件由固定字节长度的记录组成，所以使用tf.FixedLengthRecordReader 图像被处理成： 裁切为24*24像素，评估裁剪中间部分，训练时随机 近似白化处理，使模型对图片动态的范围变化不敏感 从磁盘中读图片需要相当长的处理时间，为避免其使训练时间变长，使用16个独立的线程读图片来填充TenorFlow队列。 模型预测预测部分的代码在inference()中，计算预测的得分（logits），这部分的代码组织如下： conv1 卷积、修正线性激活（rectified linear activation） pool1 最大池化 norm1 局部响应归一化 conv2 卷积、修正线性激活 norm2 局部响应归一化 pool2 最大池化 local3 带有“修正线性激活的”的全连接 local4 带有“修正线性激活的”的全连接 softmax_linear 线性变换，输出结果 下图由TensorBoard生成，展示预测部分的操作 练习：inference的输出是未归一化的logits，尝试使用tf.nn.softmax修改网络结构，使其返回归一化的预测结果 练习：inference中的模型结构和cuda-convnet中的CIFAR-10模型有些许的不同。其中，在Alex的原始模型中，最上几层是局部连接而非全连接。尝试修改网络结构，在最上层形成局部连接的结构。 模型训练训练多分类网络常用的方法是多项式逻辑回归，如softmax regression。Softmax回归对结果应用非线性的softmax，计算归一化的预测结果与one-hot编码标签的交叉熵。为了正则化，使学习的变量的权重逐渐减小。模型的目标函数是交叉熵损失和，和权重衰减项的和，在loss()函数中返回。 应用tf.summary.scalar，在TensorBoard中将这一过程展示： 使用标准的梯度下降算法来训练，学习率随时间变化呈指数衰减： train()中，通过计算梯度、更新学习变量（tf.train.GradientDescentOptimizer），使目标函数最小化。train()返回一个操作，这个操作中执行一批图像的计算，以训练和更新模型。 启动和训练模型让训练跑起来：1python cifar10_train.py 结果如下：12345678Filling queue with 20000 CIFAR images before starting to train. This will take a few minutes.2015-11-04 11:45:45.927302: step 0, loss = 4.68 (2.0 examples/sec; 64.221 sec/batch)2015-11-04 11:45:49.133065: step 10, loss = 4.66 (533.8 examples/sec; 0.240 sec/batch)2015-11-04 11:45:51.397710: step 20, loss = 4.64 (597.4 examples/sec; 0.214 sec/batch)2015-11-04 11:45:54.446850: step 30, loss = 4.62 (391.0 examples/sec; 0.327 sec/batch)2015-11-04 11:45:57.152676: step 40, loss = 4.61 (430.2 examples/sec; 0.298 sec/batch)2015-11-04 11:46:00.437717: step 50, loss = 4.59 (406.4 examples/sec; 0.315 sec/batch)... 每10步显示一次损失、训练速度。下面是一些提示： 第一批(batch)数据训练很慢，因为需要预处理线程读取20000张CIFAR图像，打乱顺序加入队列。 显示的损失是最近一批数据的平均损失，这个损失是交叉熵与权重衰减项之和 在Tesla K40c上得到显示的训练速度，如果用CPU训练，速度较慢。 练习：在实验时，第一步训练耗时太长。尝试减少填入队列的图像的数目。在cifar10_input.py中查找min_fraction_of_examples_in_queue cifar10_train.py周期性的以checkpoints files保存模型参数，但不评估模型。Checkpoints会在cifar10_eval.py中使用，用来预测模型性能。 如果读完了前面的步骤，现在可以训练CIFAR-10模型了。Congratulations! cifar10_train.py返回的文字中包含少量的模型训练的信息。我们需要更多的训练时的信息，包括： 损失是真实的在减小，还是只是噪声？ 为模型提供的图片是否合适？ 梯度、激活值、权重是否合理？ 学习率是多少 TensorBoard提供了这些功能。在cifar10_train.py中，通过tf.summary.FileWriter周期性的显示这些数据。 例如，可以观察激活值的分布和特征的稀疏程度：]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[TensorFlow学习笔记：CNN MNIST]]></title>
      <url>%2FTensorFlow%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%EF%BC%9ACNN-MNIST.html</url>
      <content type="text"><![CDATA[官方文档地址：A Guide to TF Layers: Building a Convolutional Neural Network 卷积神经网络卷积神经网络是图片识别任务最先进的模型。 CNN应用一系列filters在原始像素数据上，提取学习高级特征，模型使用这些特征来分类。 CNN组成 卷积层： 指定数量的卷积核。对图像的每个区域进行一系列数学计算，得到单一的值，作为特征。再使用ReLU作为激活函数，以引入非线性特征。 池化层：对卷积得到的特征再次降维以减少处理时间。常用max_pooling，对每个2*2区域保留最大啊值，丢弃其他值。 全连接层：对卷积层提取、池化层采样的特征进行分类。每个节点都和上一层的每个节点相连接。 CNN由一堆卷积模块组成，每个模块都有卷积层，后跟一池化层。 最后一个卷积模块包含一个或多个全连接层，用作分类。 最后一个全连接层中，每个类别有一个节点（使模型能预测所有类别），用softmax作为激活函数（所有softmax值和为1），每个softmax值可以解释为图像属于该类的概率。 斯坦福CNN课程资料http://cs231n.github.io/convolutional-networks/ 使用CNN构建MNIST分类器CNN结构 卷积层-1：32个5*5filter，ReLU激活函数 池化层-1：:2*2max pooling，步长2（使采样区域不重复） 卷积层-2：64个5*5filter, ReLu激活函数 池化层-2：2*2max pooling，步长2 全连接层-1：1024神经元，dropout率0.4（训练中0.4概率给定的元素被丢弃） 全连接层-2：10神经元，每个代表一类 使用tf.layers模块构建上述各类型的层 conv2d 2维卷积层，给定卷积核数量、大小、padding、激活函数作为参数 max_pooling2d 2维池化层（max_pooling），给定filter大小、步长作为参数 dense 全连接层，参数为神经元数量、激活函数 以上方法都接收tensor作为输入，然后输出一个处理后的tensor。可以直接使用返回值作为下一层的输入。 cnn_model_fn()cnn_mnist.py接受MNIST特征数据、标签、模型类型（TRAIN、EVAL、INFER）作为输入参数，配置CNN网络，返回预测结果、损失和训练步骤。 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162def cnn_model_fn(features, labels, mode): &quot;&quot;&quot;Model function for CNN.&quot;&quot;&quot; # Input Layer input_layer = tf.reshape(features, [-1, 28, 28, 1]) # Convolutional Layer #1 conv1 = tf.layers.conv2d( inputs=input_layer, filters=32, kernel_size=[5, 5], padding=&quot;same&quot;, activation=tf.nn.relu) # Pooling Layer #1 pool1 = tf.layers.max_pooling2d(inputs=conv1, pool_size=[2, 2], strides=2) # Convolutional Layer #2 and Pooling Layer #2 conv2 = tf.layers.conv2d( inputs=pool1, filters=64, kernel_size=[5, 5], padding=&quot;same&quot;, activation=tf.nn.relu) pool2 = tf.layers.max_pooling2d(inputs=conv2, pool_size=[2, 2], strides=2) # Dense Layer pool2_flat = tf.reshape(pool2, [-1, 7 * 7 * 64]) dense = tf.layers.dense(inputs=pool2_flat, units=1024, activation=tf.nn.relu) dropout = tf.layers.dropout( inputs=dense, rate=0.4, training=mode == learn.ModeKeys.TRAIN) # Logits Layer logits = tf.layers.dense(inputs=dropout, units=10) loss = None train_op = None # Calculate Loss (for both TRAIN and EVAL modes) if mode != learn.ModeKeys.INFER: onehot_labels = tf.one_hot(indices=tf.cast(labels, tf.int32), depth=10) loss = tf.losses.softmax_cross_entropy( onehot_labels=onehot_labels, logits=logits) # Configure the Training Op (for TRAIN mode) if mode == learn.ModeKeys.TRAIN: train_op = tf.contrib.layers.optimize_loss( loss=loss, global_step=tf.contrib.framework.get_global_step(), learning_rate=0.001, optimizer=&quot;SGD&quot;) # Generate Predictions predictions = &#123; &quot;classes&quot;: tf.argmax( input=logits, axis=1), &quot;probabilities&quot;: tf.nn.softmax( logits, name=&quot;softmax_tensor&quot;) &#125; # Return a ModelFnOps object return model_fn_lib.ModelFnOps( mode=mode, predictions=predictions, loss=loss, train_op=train_op) 上述代码使用tf.layers模块构建各个层，计算损失，配置训练步骤，并生成预测。 输入层对2维的图像数据构建卷积层和池化层需要输入的tensor的形状为[batch_size,image_width,image_height, channels] batch_size 梯度下降训练时子集大小 image_width 输入图像宽度 image_height 输入图像高度 channels 彩色图片为3（红绿蓝）,黑白图片为1（黑） MNIST数据为单色28*28像素，所以输入的shape为[batch_size, 28, 28, 1] 把输入转换为该shape，执行reshape 1input_layer = tf.reshape(features, [-1, 28, 28, 1]) 上面的batch_size为-1表示该维度不确定，由输入的features动态计算。这允许我们把batch_size当做可调整的超参数。例如：batch为5时，features包含3920 (52828)个值，shape为[5,28,28,1]，batch为100时，输入的值个数为78400 卷积层-1对输入层应用32个5*5的filter，并使用ReLU作为激活函数。可以使用conv2d()函数来创建这一层 123456conv1 = tf.layers.conv2d( inputs=input_layer, filters=32, kernel_size=[5, 5], padding=&quot;same&quot;, activation=tf.nn.relu) 输入的tensor的形状必须是[batch_size, image_width,image_height,channels],这样可以与输入层相连。 filters 卷积核数量 kernel_size 卷积核维度 padding 为”valid”或”same”。padding=”same”时，输出的tensor和输入tensor维度相同（边缘处补0）。没有padding的话，2828经过55卷积，会产生24*24的tensor activation 指定激活函数，这里使用ReLU（tf.nn.relu） 输出tensor的形状为[batch_size,28,28,32]，宽度和高度和原来相同，但经过32个卷积核卷积，现在有32个channels 池化层-1现在可以把第一个pooling层和上面创建好的卷积层连接。使用max_pooling2d()构建池化层 1pool1 = tf.layers.max_pooling2d(inputs=conv1, pool_size=[2, 2], strides=2) inputs 输入tensor。形状为[batch_size, image_width, image_height, channels]。这里的输入为上一层的输出，形状是[batch_size, 28, 28, 32] pool_size max pool filter的大小（[width, height]），这里为[2,2] strides 步长的大小。2表示长和宽每2像素划分为一个子区域（2*2的filter使用strides=2，使子区域没有重叠）。也可设置其他大小，如[3,6] max_pooling2d()输入的tensor形状为[batch_size, 14, 14, 32]，2*2的filter把高度和宽度减少了50% 卷积层-2和池化层-2和前面一样，第二个卷积层和池化层使用conv2d()和max_pooling2d()。在这里，卷积层使用64个55卷积核，池化层的设置与前面一样（22的max pool，步长2） 12345678conv2 = tf.layers.conv2d( inputs=pool1, filters=64, kernel_size=[5, 5], padding=&quot;same&quot;, activation=tf.nn.relu)pool2 = tf.layers.max_pooling2d(inputs=conv2, pool_size=[2, 2], strides=2) 第二层卷积层使用第一层池化层的输出pool1作为输入，输出h_conv2tensor，h_conv2的形状为[batch_size,14,14,64]，高和宽与pool1相同，使用64个卷积核，所以channel为64 第二层池化层的形状为[batch_size, 7, 7, 64]（高和宽再次减少50%） 全连接层接下来添加一层包含1024神经元，使用ReLU的dense层，对卷积/池化采样的特征进行分类。在连接该层前，需要把特征映射（feature map）变换形状，使其变为2维 1pool2_flat = tf.reshape(pool2, [-1, 7 * 7 * 64]) -1表示batch_size维度不定，由输入的样本数量动态计算。每个样本有7764个特征（长度/宽度/channels）。现在pool2_flat的形状为[batch_size,3136] 使用dense()连接该dense层 1dense = tf.layers.dense(inputs=pool2_flat, units=1024, activation=tf.nn.relu) input 输入的tensor （展开的特征映射） units 神经元数量 activation 激活函数 为了提高模型效果，需要添加dropout regularization，使用tf.layers.dropout() 12dropout = tf.layers.dropout( inputs=dense, rate=0.4, training=mode == learn.ModeKeys.TRAIN) input输入的tensor（dense） rate dropout率：训练中40%的元素被随机丢弃 training 指定模型是否正在训练中。dropout只有在training=True时被启用。 Logits层神经网络最后一层是logits层，返回预测的原始值。该层有10个神经元（代表数字0-9）,使用线性激活函数（默认） 1logits = tf.layers.dense(inputs=dropout, units=10) 计算损失训练和评估模型时，需要定义损失函数来度量模型预测结果和真实值的差异。多分类问题（例如MNIST）中，交叉熵（cross entropy）是典型的损失度量。 12345678loss = Nonetrain_op = None# Calculate loss for both TRAIN and EVAL modesif mode != learn.ModeKeys.INFER: onehot_labels = tf.one_hot(indices=tf.cast(labels, tf.int32), depth=10) loss = tf.losses.softmax_cross_entropy( onehot_labels=onehot_labels, logits=logits) labels tensor包括一组样本的预测分类值，例如[1,9….]。为了计算交叉熵，需要将其变为one-hot编码： 123[[0, 1, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 1], ...] 使用tf.onehot()进行转换。 indices 输入，哪个位置变为one-hot值 depth 类别的数目 下面的代码生成one-hot tensor 1onehot_labels = tf.one_hot(indices=tf.cast(labels, tf.int32), depth=10) labels 中为0-9的数值（[1,9…]），将其转换为int类型。depth为10，因为有10个可能的目标类别。 使用tf.losses.softmax_cross_entropy()对logits层进行softmax，计算交叉熵，结果为标量tensor 12loss = tf.losses.softmax_cross_entropy( onehot_labels=onehot_labels, logits=logits) 设置训练步骤前面定义了CNN结构，现在设置训练步骤，以优化训练损失。使用tf.contrib.layers.optimize_loss(), 这里使用0.001学习率，并使用随机梯度下降（stochastic gradient descent）作为优化算法 1234567# Configure the Training Op (for TRAIN mode)if mode == learn.ModeKeys.TRAIN: train_op = tf.contrib.layers.optimize_loss( loss=loss, global_step=tf.contrib.framework.get_global_step(), learning_rate=0.001, optimizer=&quot;SGD&quot;) 预测logits层返回[batch_size, 10]的tensor，现将这些原始值转换为两种形式 预测的类别 各类别的概率例子中，预测的类别是返回的tensor中每行最大值对应的列，使用tf.argmax找到其索引 1tf.argmax(input=logits, axis=1) input 从该tensor中提取最大值 axis 从哪个维度提取（logits形状为[batch_size, 10]） 通过tf.nn.softmax()得到概率 1tf.nn.softmax(logits, name=&quot;softmax_tensor&quot;) 将输出整理为字典形式： 123456predictions = &#123; &quot;classes&quot;: tf.argmax( input=logits, axis=1), &quot;probabilities&quot;: tf.nn.softmax( logits, name=&quot;softmax_tensor&quot;)&#125; 现在，得到了predictions,loss,train_op，连同mode参数可以把它们返回。 123# Return a ModelFnOps objectreturn model_fn_lib.ModelFnOps( mode=mode, predictions=predictions, loss=loss, train_op=train_op) 训练和评估CNN MNIST模型加载训练集/测试集1234567def main(unused_argv): # Load training and eval data mnist = learn.datasets.load_dataset(&quot;mnist&quot;) train_data = mnist.train.images # Returns np.array train_labels = np.asarray(mnist.train.labels, dtype=np.int32) eval_data = mnist.test.images # Returns np.array eval_labels = np.asarray(mnist.test.labels, dtype=np.int32) 把训练集数据（55000图像的原始像素）和标签（0-9）以numpy arrays保存在train_data和train_lable中。相似的，保存10000数据和标签作为测试集。 创建EstimatorEstimator是TensorFlow用来进行高级模型训练、评价和预测的类。 123# Create the Estimatormnist_classifier = learn.Estimator( model_fn=cnn_model_fn, model_dir=&quot;/tmp/mnist_convnet_model&quot;) model_fn 指定训练/评估/预测的模型 model_dir 保存checkpoints的路径 Logging创建日志以跟踪训练过程。创建一个tf.train.LoggingTensorHook，会记录Softmax层计算出的概率 1234# Set up logging for predictions tensors_to_log = &#123;&quot;probabilities&quot;: &quot;softmax_tensor&quot;&#125; logging_hook = tf.train.LoggingTensorHook( tensors=tensors_to_log, every_n_iter=50) 将需要记录的tensors存入tensor_to_log字典，key作为日志中的标签，相应的值是指定tensor的名称（name属性） every_n_iter 每训练50步记录一次 训练模型1234567# Train the modelmnist_classifier.fit( x=train_data, y=train_labels, batch_size=100, steps=20000, monitors=[logging_hook]) x 训练数据 y 训练集标签 batch 每次训练样本数 steps 训练次数 monitors logging_hook 评估模型在MNIST测试集上评价训练好的模型。使用tf.contrib.learn.MetricSpec创建metrics字典来计算准确率 123456# Configure the accuracy metric for evaluationmetrics = &#123; &quot;accuracy&quot;: learn.MetricSpec( metric_fn=tf.metrics.accuracy, prediction_key=&quot;classes&quot;),&#125; metric_fn 计算metric的函数，这里使用预定义的tf.metrics.accuracy prediction_key 包含预测值的tensor的名称（前面定义过） 通过下面代码打印评估结果： 1234# Evaluate the model and print resultseval_results = mnist_classifier.evaluate( x=eval_data, y=eval_labels, metrics=metrics)print(eval_results) 运行模型结果显示如下 12345678910111213INFO:tensorflow:loss = 2.36026, step = 1INFO:tensorflow:probabilities = [[ 0.07722801 0.08618255 0.09256398, ...]]...INFO:tensorflow:loss = 2.13119, step = 101INFO:tensorflow:global_step/sec: 5.44132...INFO:tensorflow:Loss for final step: 0.553216.INFO:tensorflow:Restored model from /tmp/mnist_convnet_modelINFO:tensorflow:Eval steps [0,inf) for training step 20000.INFO:tensorflow:Input iterator is exhausted.INFO:tensorflow:Saving evaluation summary for step 20000: accuracy = 0.9733, loss = 0.0902271&#123;&apos;loss&apos;: 0.090227105, &apos;global_step&apos;: 20000, &apos;accuracy&apos;: 0.97329998&#125;]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[网络攻防实验(3)-逆向分析]]></title>
      <url>%2F%E7%BD%91%E7%BB%9C%E6%94%BB%E9%98%B2%E5%AE%9E%E9%AA%8C-3-%E9%80%86%E5%90%91%E5%88%86%E6%9E%90.html</url>
      <content type="text"><![CDATA[题目说明说明：该题目由CrackMee.exe一个文件组成。该文件是一个简单的小程序，请大家逆向该程序，找到正确的字符串，输入到程序后即为成功。 题目下载实验程序下载 使用工具 IDA pro 解题步骤 使用IDA pro F5查看反编译的程序代码。这段程序就是构造出一个字符串，输入正确的话，就输出”Key Right!”。构造过程如下：前5个字节从q中取得，第6到9字节通过x和ans获得，第10字节为’3’(ASCII码51)，第11字节为’y’(ASCII码121) 找到以上变量的值。q为DWORD型，每四字节取一个值，可得v5前5字节为”thi5_”这里可找到x的值。同理，可找到ans的值。 编写一个程序来构造目标字符串的第6-11字节，参照(1)中的代码。这里我选择用python来写运行结果为:即要输入的字符串为&quot;thi5_1s_K3y&quot; 实验结果]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[网络攻防实验(2)-缓冲区溢出]]></title>
      <url>%2F%E7%BD%91%E7%BB%9C%E6%94%BB%E9%98%B2%E5%AE%9E%E9%AA%8C-2-%E7%BC%93%E5%86%B2%E5%8C%BA%E6%BA%A2%E5%87%BA.html</url>
      <content type="text"><![CDATA[题目说明说明：该题目由memory、flag两个文件组成。memory为可执行代码，存在缓冲区溢出漏洞，请同学们分析该程序的二进制代码找到漏洞成因，并通过修改程序的执行流程导致任意代码执行漏洞将flag中的FLAG信息输出出来。单机执行：请同学们在自己的机器上运行脚本：./host，5555端口即为程序输入端口。如果编写exploit攻击成功，会出现以下提示：{FLAG:this is a flag} 题目下载实验程序下载 使用工具 IDA pro pwntools(python2.7) socat linux 解题步骤 使用IDA pro打开程序文件，按F5查看其反编译代码观察到mem_test()函数中调用了scanf()，可能造成缓冲区溢出。推测此题的解题思路为，scanf读入一定长度的字符，覆盖mem_test()的返回地址，导致任意代码执行。 寻找要执行的函数。打开函数视图，找到win_func()函数，这个函数直接调用了system(),可以传递参数为”cat flag”，从而显示flag文件中的内容。 使用IDA pro远程调试linux下的memory程序，在scanf()后添加断点。输入若干’0’后，观察栈变化。观察到栈地址0xFFB21C5C保存的正式mem_test()的返回地址，将其覆盖为win_func()的地址，即可跳转执行win_func()函数调用时，先将其返回地址入栈，再将参数入栈。0xFFB21C5C后面4字节为返回地址（可构造为任意可访问的地址）,再后面4字节为参数地址。 查看string视图，找到”cat flag”字符串，其地址为0x08048840。查看function视图，找到win_func()函数地址为0x08048610 构造payload为23字节字符+win_func地址+任意可访问地址+”cat flag”地址。使用pwntools编写exploit。 1234from pwn import *conn=remote('localhost',5555)conn.send('a'*23+p32(0x08048610)+p32(0x08048610)+p32(0x08048840))conn.interactive() 实验结果]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[网络攻防实验(1)-缓冲区溢出]]></title>
      <url>%2F%E7%BD%91%E7%BB%9C%E6%94%BB%E9%98%B2%E5%AE%9E%E9%AA%8C-1-%E7%BC%93%E5%86%B2%E5%8C%BA%E6%BA%A2%E5%87%BA.html</url>
      <content type="text"><![CDATA[题目说明说明：该题目由100、flag两个文件组成。100为可执行代码，存在缓冲区溢出漏洞。请同学们分析该程序的二进制代码找到漏洞成因，并通过修改程序的执行流程导致任意代码执行漏洞将flag中的FLAG信息输出出来。单机执行：请同学们在自己的机器上运行脚本：./host，9999端口即为程序输入端口。如果编写exploit攻击成功，会出现以下提示：{FLAG:this is a flag} 题目下载实验程序下载 使用工具 IDA pro pwntools (python2.7) socat 解题步骤 使用IDA pro打开程序，按F5可查看其反汇编代码。注意到scanf函数可能存在缓冲区溢出漏洞。推断此题的解题思路为：通过scanf函数输入一定长度的字符，覆盖栈中的v5()函数的地址。 寻找要执行函数的地址。打开函数视图(view-&gt;subview-&gt;function)，找到win()函数：执行这个函数可以读出flag文件中内容。 寻找溢出点。在scanf函数后添加断点，调试程序。在程序中输入若干个’0’后，栈信息如下：输入的’a’的ASCII码为0x30，栈中lose()函数的地址为12345674. 构造payload为 64字节字符+win()函数地址。本题中，执行host.sh脚本可将程序输入/输出重定向到9999端口,可使用pwntools编写exploit。```pythonfrom pwn import *conn=remote(&apos;localhost&apos;,9999)conn.send(&apos;a&apos;*64+p32(0x0804861B))conn.interactive() 实验结果]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[依法治国的发展及对当代大学生法治素养的探讨]]></title>
      <url>%2F%E4%BE%9D%E6%B3%95%E6%B2%BB%E5%9B%BD%E7%9A%84%E5%8F%91%E5%B1%95%E5%8F%8A%E5%AF%B9%E5%BD%93%E4%BB%A3%E5%A4%A7%E5%AD%A6%E7%94%9F%E6%B3%95%E6%B2%BB%E7%B4%A0%E5%85%BB%E7%9A%84%E6%8E%A2%E8%AE%A8.html</url>
      <content type="text"><![CDATA[摘要 本文阐述了依法治国的发展历程：从法治建设“十六字方针”到提出依法治国的基本方略；从依法治国被写入宪法，到提出全面推进依法治国。社会主义依法治国理论日益走向成熟，社会主义依法治国实践迈入更高阶段。最后，在总结依法治国的基础上，探讨了依法治国大背景下如何提高当代大学生的法制素养。 依法治国的发展历程 1954年制定了共和国第一部宪法，初步奠定了社会主义法制的基础。但在“文革”十年，社会主义法制遭到严重破坏。在党的十一届三中全会召开前的中央工作会议上，邓小平提出了“为了保障人民民主，必须加强法制，使民主制度化、法律化，做到有法可依，有法必依，执法必严，违法必究。”这段谈话，为我国依法治国基本方略的形成奠定了基本理论基础。 党的十一届三中全会确立了解放思想、实事求是的思想路线，同时提出了加强社会主义民主，健全社会主义法制的任务目标。会议公报[1]指出：“为了保障人民民主，必须加强社会主义法制，使民主制度化、法律化，使这种制度和法律具有稳定性、连续性和极大的权威，做到有法可依，有法必依，执法必严，违法必究。”这准确地描述了法治的基本精神内核，阐述了依法治国的基本内涵，为依法治国方略的最终提出奠定了思想基础。 在党的十一届三中全会精神指引下，在党的领导下我国进行了一系列重大立法工作。党的十一届三中全会以后，按照**“建设有中国特色的社会主义”和“以经济建设为中心”的方针和指导思想，立法进程被不断推进，先后制定了一系列重要的民事、经济法律，为改革开放和社会主义现代化建设提供了坚实的法律保障。 1989年，颁布行政诉讼法，是我国法治政府建设的重要开端。1993年，党的十四届三中全会通过的《决定》[2]提出：“各级政府都要依法行政，依法办事。”这是第一次在党的正式文件中提出“依法行政”，将法治政府建设作为法治建设的重点，进一步丰富了依法治国的内涵。此时，依法治国方略虽然尚未提出，但“十六字方针”和宪法及一系列重要法律的修订出台，清晰阐释了依法治国的基本精神，社会主义法制体系开始形成，这为依法治国方略的形成奠定了思想基础和制度基础。 党的十五大正式提出依法治国基本方略。十五大报告[3]中指出：“依法治国，是党领导人民治理国家的基本方略，是发展社会主义市场经济的客观需要，是社会文明进步的重要标志，是国家长治久安的重要保障。”这就正式将依法治国提升为国家治理的基本方略。依法治国方略的提出，是对我们党治国理政经验的全面总结与升华，标志着党在执政理念、领导方式上实现了一次历史性跨越，为我国此后的国家治理和社会治理指明了方向，具有里程碑意义。 1999年3月通过的宪法修正案[4]规定：“中华人民共和国实行依法治国，建设社会主义法治国家”。这正式将依法治国确立为宪法的基本原则，通过国家根本法对依法治国予以保障，使其有了宪法保障，也使“依法治国”这一基本方略有了长期性、稳定性的制度基础。 在党的十五大提出依法治国基本方略的基础上，党的十六大提出了坚持依法执政、不断提高执政能力的思想，要求不断改革和完善党的领导方式和执政方式，将民主、法治、人权建设从以往的“精神文明”范畴中独立出来，正式提出“政治文明”的概念，这就进一步丰富了依法治国的内涵，明晰了依法治国与其他治理方式的关系。党的十六大还提出“三统一”的法治原则，即“发展社会主义民主政治，最根本的是要把坚持党的领导、人民当家作主和依法治国有机统一起来”，这就确立了中国特色社会主义依法治国方略的根本原则。 十八大确立了依法治国的新任务和目标，即到2020年全面建成小康社会时，实现“依法治国基本方略全面落实，法治政府基本建成，司法公信力不断提高，人权得到切实尊重和保障。”这个战略目标与全面建成小康社会的目标同时提出，进一步凸显了依法治国的重要性。2014年10月，十八届四中全会的主题为“依法治国”，总结了依法治国的经验，研究了全面推进依法治国若干重大问题，对依法治国进行总体部署和全面规划。 在我们这样一个13亿多人口的发展中大国全面推进依法治国，是国家治理领域一场广泛而深刻的革命。全面推进依法治国，总目标是建设中国特色社会主义法治体系，建设社会主义法治国家。这就是，在中国共产党领导下，坚持中国特色社会主义制度，贯彻中国特色社会主义法治理论，形成完备的法律规范体系、高效的法治实施体系、严密的法治监督体系、有力的法治保障体系，形成完善的党内法规体系，坚持依法治国、依法执政、依法行政共同推进，坚持法治国家、法治政府、法治社会一体建设，实现科学立法、严格执法、公正司法、全民守法，促进国家治理体系和治理能力现代化。 对当代青年大学生的法治素养的探讨 党的十八届四中全会对全面推进依法治国战略做出了总部署，在《决定》[5]中提出了“把法治教育纳入国民教育体系，从青少年抓起”的要求和目标。高等学校是法治教育的重要场所，大学生是高素质青年群体的重要组成部分，是学法、守法、护法、用法的生力军，提升大学生的法制素养是实现全面依法治国方略的一个十分重要的部分。 大学生的综合素质发展不但要求有较高的文化素养、扎实的专业基础知识和较强的工作能力，还要有自觉的规则意识和法律观念。 树立宪法至上的观念，把遵守法律规范作为自己行为的首要标准，这是新时代对当代大学生素质结构的新要求，因此，提高大学生法治素养就成为了高校立德树人的重要内容。 全面建设小康社会所需要的不仅仅是高水平的科技人才，更需要思想政治素质高，具备一定法治素养的创新型复合型建设人才。只有加强大学生法治教育，提升法治素养， 才能够让他们意识到全面实施依法治国的重要性， 并成为一个懂法的社会主义建设者和接班人，才能够自觉拿起法律武器维护自身、他人以及国家的权益，才能在经济建设中、在法律框架内充分发挥自身优势，为全面建设小康社会作出贡献. 对于广大高校学生，只有他们的法治素养提升了，才能保证法治校园的建立，同样，也只有依法治校落到了实处，才能保证大学生法治教育内容与法治教育环境高度一致，从而有效提升大学生法治素养。 对于学校而言，只有各高校注重大学生法治素养的提升，形成大学生参与学校依法管理的良好氛围，使大学生能够积极参与制定、宣传、践行学校的各项规章制度，同时通过法治学习，做到知法、守法，懂得用法律武器维护自我权益，才能更好地保障高校教育的依法顺利实施。 通过建设校园法治文化可以潜移默化地提升大学生法治素养，让遵法守法意识在学校蔚然成风，让法治文化在师生心中落地生根。要充分利用校园新闻、校园报纸、学生社团以及新兴媒体等载体加大法治文化宣传，以学生喜闻乐见的形式宣传法治文化。还要将法治教育融入大学生教育中，融入学生社会实践中，在学生自我教育、自我管理、自我服务中体现和培养法治精神、法治思维等法治素养，达到内化于心、外化于行的效果。推进法治文化主题活动。搭建形式多样的法治文化主题活动，打造法治文化宣传教育活动品牌，让学生在活动中体验法治精神。 可以通过开展“国家宪法日”主题活动，让学生牢固树立公民权利与义务相统一的观念。通过开展“校园诚信教育”活动，使遵纪守法成为大学生的共同追求和自觉行动。 要探索法治文化长效机制。高校要注重将法治文化培育纳入学生成长成才的全过程，以制度形式规范法治文化建设的领导体制、运行机制、活动载体、队伍建设、考核管理、经费投入和硬件保障。要站在“四个全面”战略布局的高度，将通过法治文化熏陶提升大学生法治素养作为一项常抓不懈的工作，让法治精神在一代代大学生群体中传承和发扬。 参考文献 [1] 中国共产党第十一届中央委员会第三次全体会议公报[M]. 人民出版社, 1979. [2] 中共中央关于建立社会主义市场经济体制若干问题的决定[J]. 求实, 1993(12):1-13. [3] 江泽民. 高举邓小平理论伟大旗帜把建设有中国特色社会主义事业全面推向二十一世纪[M]. 人民出版社, 1997. [4] 中华人民共和国宪法修正案[C] 1999. [5] 中新网. 中共中央 关于全面推进依法治国若干重大问题的决定[M]. 中国法制出版社, 2014. [6] 李艳. 大学生法治教育[J]. 教育, 2015(35):210-210. [7] 李全文. 全面依法治国视域中的大学生法治教育[J]. 思想理论教育导刊, 2016(5).]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[网页设计作品]]></title>
      <url>%2F%E7%BD%91%E9%A1%B5%E8%AE%BE%E8%AE%A1%E4%BD%9C%E5%93%81.html</url>
      <content type="text"><![CDATA[女朋友的部分网页设计作品 教育年度盘点网页链接：尚未上线 数读中学专题-成都七中网页链接：数读中学系列专题—成都七中｜中国教育在线 新高三生规划网页链接：2017年高三生学习规划|高三规划|中国教育在线 2016国庆专题网页链接：已下线 高三你准备好了吗？网页链接：高三,你准备好了吗_中国教育在线 图解小升初网页链接：图解小升初：小升初过后，初中英语怎么学？]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[棋盘覆盖问题]]></title>
      <url>%2F%E6%A3%8B%E7%9B%98%E8%A6%86%E7%9B%96%E9%97%AE%E9%A2%98.html</url>
      <content type="text"><![CDATA[问题描述在一个2^k*2^k个方格组成的棋盘中，恰有一个方格与其他方格不同，称该方格为一特殊方格，如图所示，红色方格为特殊方格。 棋盘覆盖问题是指，要用下图中的四种不同形态的L型骨牌覆盖棋盘上除特殊方格以外的所有方格，且任何2个L型骨牌不能重叠覆盖。 解题思路采用分治法来求解此问题 若k=2，则直接用一个L型骨牌来覆盖非特殊方格 k&gt;3时，可把原棋盘分成四个2^(k-1)阶的子棋盘，特殊方格位于四个棋盘中的一个。其他三个子棋盘也可转化为特殊棋盘，做法是用一个L型骨牌覆盖三个子棋盘的连接处的三个方格，如图所示。此时原问题转化为4个小规模棋盘的覆盖问题，可递归求解。 算法 设置一个全局变量mark，用于记录骨牌的序号 如果边长为2，则将给定的特殊方格(x,y)用给定的序号标记，其他三个方格用mark标记。 如果边长&gt;2，则把棋盘分割为四部分，判断特殊方格所处的子棋盘。对于有特殊方格的棋盘，继续递归求解。对于其他3个子棋盘： 左上角的子棋盘：其右下角方格为“特殊方格” 右上角的子棋盘：其右下角方格为“特殊方格” 左下角的子棋盘：其右上角方格为“特殊方格” 右下角的子棋盘：其左上角方格为“特殊方格” 这三个子棋盘的假想的“特殊方格”用同一个骨牌覆盖（标记相同），继续递归处理。 Python语言实现123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081# coding=utf-8mark = 1list = []# 生成棋盘def initTable(n): global list; for i in range(n): row = [] for j in range(n): row.append(-1) list.append(row)# 检查特殊点区域def check(midx, midy, x, y): if x &lt; midx and y &lt; midy: return 0 # 左上 elif x &gt;= midx and y &gt;= midy: return 3 # 右下 elif x &gt;= midx and y &lt; midy: return 2 # 右上 else: return 1 # 左下# 覆盖x1~x2行，y1~y2列，特殊点为(x,y),用markX填充特殊点def cover(x1, x2, y1, y2, x, y, markX): global mark # 如果是2*2的格子 if x2 - x1 == 1: # 填充特殊点 list[x][y] = markX # 填充其他3个格子 for row in range(x1, x2 + 1): for col in range(y1, y2 + 1): if row != x or col != y: list[row][col] = mark mark += 1 return # 其他情况: 2*2以上的格子 # 用midx,midy把棋盘分为4部分 midx = (x1 + x2 + 1) / 2 midy = (y1 + y2 + 1) / 2 covered = &#123;0:False, 1:False, 2:False, 3:False&#125; # 检查特殊点在哪个区域 area = check(midx, midy, x, y) if area == 0: # 特殊格子在左上 cover(x1, midx - 1, y1, midy - 1, x, y, markX) if area == 1: # 特殊格子在右上 cover(x1, midx - 1, midy, y2, x, y, markX) if area == 2: # 特殊格子在左下 cover(midx, x2, y1, midy - 1, x, y, markX) if area == 3: # 特殊格子在右下 cover(midx, x2, midy, y2, x, y, markX) # 标记已经处理过的区域 covered[area] = True # 其他3个区域 取内角的三个小格子为特殊点 newMark = mark mark += 1 # 处理其他3个区域 if not covered[0]: cover(x1, midx - 1, y1, midy - 1, midx - 1, midy - 1, newMark) if not covered[1]: cover(x1, midx - 1, midy, y2, midx - 1, midy, newMark) if not covered[2]: cover(midx, x2, y1, midy - 1, midx, midy - 1, newMark) if not covered[3]: cover(midx, x2, midy, y2, midx, midy, newMark) def printTable(): global list for i in range(len(list)): row = "" for j in range(len(list)): row += (str(list[i][j]) + "\t") print(row+"\n")if __name__ == '__main__': n = 8 initTable(n) cover(0, 7, 0, 7, 2, 3, 0) printTable()]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[Django环境搭建及简单的视图与网址的配置]]></title>
      <url>%2FDjango%E7%8E%AF%E5%A2%83%E6%90%AD%E5%BB%BA%E5%8F%8A%E7%AE%80%E5%8D%95%E7%9A%84%E8%A7%86%E5%9B%BE%E4%B8%8E%E7%BD%91%E5%9D%80%E7%9A%84%E9%85%8D%E7%BD%AE.html</url>
      <content type="text"><![CDATA[安装Django用pip安装Django：pip install Django 检查是否安装成功 python命令行下123import djangodjango.VERSION#django.get_version() 若能显示出版本信息，则安装成功 Django基本命令创建一个Django projectdjango-admin.py startproject project-name 创建apppython manage.py startapp app-name 启动测试服务器 python manage.py runserver 使用其他端口python manage.py runserver port 监听所有可用ip(内外网情况下)python manage.py runserver 0.0.0.0:port 视图和网址新建项目 “mysite”1django-admin.py startproject mysite 创建成功后，目录样式为: 1234567mysite├── manage.py└── mysite ├── __init__.py ├── settings.py ├── urls.py └── wsgi.py 在外层mysite目录下，创建app “test”1python manage.py startapp test test下目录结构为:123456test/├── __init__.py├── admin.py├── models.py├── tests.py└── views.py 把新建的app “test” 加入到INSTALLED_APPS中vim mysite/mysite/settings.py修改INSTALLED_APPS123456789INSTALLED_APPS = ( &apos;django.contrib.admin&apos;, &apos;django.contrib.auth&apos;, &apos;django.contrib.contenttypes&apos;, &apos;django.contrib.sessions&apos;, &apos;django.contrib.messages&apos;, &apos;django.contrib.staticfiles&apos;, &apos;test&apos;,) 定义视图函数修改mysite/test/views.py12345# coding:utf-8from django.http import HttpResponse def index(request): return HttpResponse(u"李博伟正在学习Django") 定义视图函数的URL修改mysite/mysite/urls.py12345678from django.conf.urls import urlfrom django.contrib import adminfrom learn import views as test_views # 修改这里 urlpatterns = [ url(r'^$', test_views.index), # 修改这里 url(r'^admin/', admin.site.urls),] 在url()中用正则表达式定义了网址的形式，和该网址对应的视图函数 运行python manage.py runserver查看效果]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[搭建shadowsocks服务器实现ipv6免费上网]]></title>
      <url>%2F%E6%90%AD%E5%BB%BAshadowsocks%E6%9C%8D%E5%8A%A1%E5%99%A8%E5%AE%9E%E7%8E%B0ipv6%E5%85%8D%E8%B4%B9%E4%B8%8A%E7%BD%91.html</url>
      <content type="text"><![CDATA[背景很多高校提供一定量的免费流量，超过之后就要付费。但是ipv6的流量不需要付费。可以通过shadowsocks搭建一个走ipv6流量的代理，从而实现免费上网。 所需要的工具shadowsocks、支持ipv6的VPS 支持ipv6的VPS我推荐Conoha，我一直在用，百兆带宽、SSD、不限流量、基本配置900日元/月，可以支付宝付款，总之优点很多。链接如下Conoha主机 shadowsocks的安装和一些配置可以参考Shadowsocks 使用说明科学上网之 Shadowsocks 安装及优化加速 IPv6 shadowsocks配置服务端配置若按照上面的链接中的方法安装shadowsocks，其配置文件路径为/etc/shadowsocks.json 修改配置文件123456789&#123; &quot;server&quot;:&quot;::&quot;, &quot;server_port&quot;:8989, &quot;local_address&quot;: &quot;127.0.0.1&quot;, &quot;local_port&quot;:1080, &quot;password&quot;:&quot;你的密码&quot;, &quot;timeout&quot;:600, &quot;method&quot;:&quot;aes-256-cfb&quot;&#125; 与ipv4的不同之处为&quot;sever&quot;:&quot;::&quot;，如此修改即可启用ipv6 启动shadowsocks服务1ssserver -c /etc/shadowsocks.json -d start 客户端配置客户端配置见下图，在服务器地址处填写服务器的ipv6地址]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[国科大课件下载脚本]]></title>
      <url>%2F%E5%9B%BD%E7%A7%91%E5%A4%A7%E8%AF%BE%E4%BB%B6%E4%B8%8B%E8%BD%BD%E8%84%9A%E6%9C%AC.html</url>
      <content type="text"><![CDATA[简介终于用python写了个东西出来，之前一直在用2.7，但是在遇到十分恼火的编码问题和室友的强烈推荐后，我被迫转到了3.5。其实也差不了多少，毕竟我对python也用的不多，哈哈。 课件是个好东西，但是每次都要手动下载很烦，就想写个东西帮我完成这繁琐的事情。正好最近研究了一下requests库和beautifulsoup，可以用来练练手。 原理原理很简单了，登录，然后访问课程中心的网页，分析标签找链接，最后取得课件的下载地址。 使用requests库访问网页，调用requests.Session()，在session中请求网页能保留登录信息，不用自己再处理cookie等，很方便。 使用beautifulsoup处理html中的标签，基本上用beautifulsoup的函数就能满足需求，必要的时候再用一些正则。 下载文件也用requests。 根据文件名判断文件是否已经存在，若存在则跳过下载。 开发/运行环境Windows下的python3.5 python模块：requests,bs4 python2.7下在win控制台执行时的编码问题、中文路径问题在python3下完美解决，python3只有unicode，没有那么多字符串类型了。 脚本使用在脚本当前路径新建user.txt文件，在文件第一行输入登录名、密码，以空格分隔。例：1xxx.ucas.ac.cn 123456 在win控制台，执行命令：1python download_courseware.py 运行脚本控制台显示： 下载好的课件在各个课程名称的文件夹下 Githublibowei1213/CoursewareDownload]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[Android Studio: failed to create a child event loop]]></title>
      <url>%2FAndroid-Studio-failed-to-create-a-child-event-loop.html</url>
      <content type="text"><![CDATA[##出现的问题##Android Studio启动时出现IllegalStateException错误，导致无法使用。 123java.lang.IllegalStateException: failed to create a child event loop at ...... ##解决方法管理员权限打开Windows命令行，执行命令1netsh winsock reset 重启计算机，再次启动Android Studio，无错误。]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[国科大抢课脚本]]></title>
      <url>%2F%E5%9B%BD%E7%A7%91%E5%A4%A7%E6%8A%A2%E8%AF%BE%E8%84%9A%E6%9C%AC.html</url>
      <content type="text"><![CDATA[简介作为国科大研一新生，选课是一件头疼的事。已经开始选课有一周了，我还没有确定要保留哪些课，还有些想上的课名额已经满了。看来自己写个抢课的脚本是当下要做的一件大事。 本来想用python写的，无奈自己水平不够，只能用js写写控制台脚本，基本上也能满足需求。 选课过程分析选课实际上就是发送一些http请求，使用firebug我发现选课时浏览器 Post http://jwxk.ucas.ac.cn/courseManage/saveCourse 参数为所选课程的id，若选择课程id为123456的课，则提交的参数为“sids=123456”，若将其选为学位课，则多提交一个参数“did_123456=123456” 得到课程id在这个页面选中所有的checkbox，再点击下面的按钮，可以查看所有可选的课程列表 ajax提交请求模拟这一过程，分析返回的网页数据，可以得到课程的id、编号、课程名等信息。 使用脚本需要事先登录国科大选课系统的选择课程页面，按F12唤出控制台，手动修改脚本前面的两个数组（课程编号、是否为学位课），再执行这个js控制台脚本 代码及下载部分代码如下： 123456789101112131415$.ajax(&#123; type: 'POST', url: 'http://jwxk.ucas.ac.cn'+url, data: postData, success: function(result)&#123; $(result).find('tbody tr').each(function()&#123; var classId = $(this).children().find(':input').val(); var className = $(this).children().eq(3).text(); var classCode = $(this).children().eq(2).text(); classList.push(new course(classCode,className,classId)); &#125;); //执行抢课函数，5秒一次 window.setInterval(run,5000); &#125;&#125;); 12345678910111213141516171819202122232425262728293031var i=0;while(i&lt;classList.length)&#123; if(classList[i].classCode==favorClass[index])&#123; break; &#125; i++;&#125;//没有要选的课，返回if(i&gt;=classList.length)&#123; return;&#125;//构造Post参数var data='sids='+classList[i].classId;if(isXuewei[index])&#123; data=data+'&amp;did_'+classList[i].classId+'='+classList[i].classId;&#125;$.ajax(&#123; type: "POST", url: ajaxUrl, data: data, success: function(result)&#123; var reg='&lt;label id="loginError" class="error"&gt;(.*)&lt;/label&gt;' var group=result.match(reg); if(group)&#123; console.info(group[1]); &#125; &#125;&#125;);index=(index+1)%favorClass.length; 下载脚本 运行结果5秒Post一次，选的上选不上看运气咯。 随便试了几个课，结果如下]]></content>
    </entry>

    
  
  
</search>
